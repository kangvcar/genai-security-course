# 第 3 章：黑盒攻击技术

在上一章中，我们学习了白盒攻击技术，了解了如何在完全掌握模型信息的情况下生成对抗样本。然而，现实世界中的大多数 AI 系统并不会向用户公开其内部结构和参数。当我们面对一个"看不见内部"的模型时，还能否发起有效的攻击？本章将探讨这一更具现实意义的问题——黑盒攻击技术。通过学习本章内容，我们将理解攻击者如何在信息极度受限的情况下，依然能够找到 AI 系统的弱点。

## 章节目标

- 理解黑盒攻击与白盒攻击的本质区别，认识黑盒场景的现实意义
- 掌握迁移攻击的核心原理，能够解释对抗样本为何具有跨模型迁移性
- 了解基于查询的攻击策略，理解其工作机制和效率权衡
- 认识黑盒攻击在商业 AI 服务中的实际威胁，建立防御意识

---

## 1. 从白盒到黑盒：现实世界的攻击挑战

### 1.1 什么是黑盒攻击

在第2章中，我们学习的 FGSM 和 PGD 算法都有一个共同的前提：攻击者能够获取模型的完整信息，包括网络结构、参数权重，并且能够计算梯度。这种攻击场景被称为**白盒攻击（White-box Attack）**，因为模型对攻击者来说是完全透明的，就像一个透明的白色盒子。

然而，现实世界中的情况往往截然不同。当我们使用百度图像识别、腾讯云 AI、阿里云视觉智能等商业服务时，我们只能上传一张图片，然后收到一个识别结果。我们无法知道这些服务背后使用的是什么模型架构，更无法获取模型的参数或计算梯度。这种场景被称为**黑盒攻击（Black-box Attack）**——模型就像一个不透明的黑色盒子，我们只能观察它的输入和输出。

具体来说，在黑盒攻击场景下，攻击者面临以下限制：

- 无法获知模型的网络结构（是 ResNet、VGG 还是其他架构）
- 无法获取模型的参数权重
- 无法计算损失函数对输入的梯度
- 只能向模型发送查询请求
- 只能获得模型的输出结果（可能是类别标签，也可能包含置信度分数）

有同学可能会问：既然什么都不知道，那还怎么攻击？这正是黑盒攻击研究的核心挑战，也是本章要回答的问题。

### 1.2 黑盒场景的现实意义

理解黑盒攻击为何重要，需要先认识到一个事实：我们日常接触的绑大多数 AI 服务都是黑盒形式的。

以图像识别领域为例，主流的商业服务都采用 API 调用的方式。用户通过网络发送图片，服务器返回识别结果。用户能看到的只有输入（自己上传的图片）和输出（返回的标签和置信度），中间的模型完全是一个"黑盒"。类似的情况也出现在人脸识别门禁系统、自动驾驶的感知模块、医疗影像诊断系统等场景中。

这意味着，如果我们只研究白盒攻击，那么研究成果的实际应用价值将非常有限。真正具有现实威胁的，恰恰是那些能够在黑盒条件下发起的攻击。从防御的角度来看，理解黑盒攻击的原理和方法，对于设计更安全的 AI 系统至关重要。

### 1.3 黑盒攻击的两大策略

面对黑盒场景的挑战，研究者们发展出了两种主要的攻击策略。

第一种策略是**迁移攻击（Transfer Attack）**。这种方法的核心思想是：虽然我们不知道目标模型的细节，但我们可以在本地训练一个功能相似的"替代模型"，然后用白盒方法攻击这个替代模型。关键的发现是，在替代模型上生成的对抗样本，往往对目标模型也有效。这种现象被称为对抗样本的"迁移性"。

第二种策略是**基于查询的攻击（Query-based Attack）**。这种方法不需要替代模型，而是通过反复查询目标模型，根据返回的结果逐步调整扰动，最终找到有效的对抗样本。这就像玩"猜数字"游戏——虽然你不知道答案，但可以通过不断猜测和获取反馈来逼近目标。

这两种策略各有优劣。迁移攻击不需要查询目标模型，但成功率相对较低；查询攻击成功率较高，但需要大量的查询，可能产生较高的成本。在接下来的小节中，我们将详细探讨这两种策略的原理和实现方法。

---

## 2. 迁移攻击：借助替代模型的间接攻击

### 2.1 对抗样本迁移性的发现

迁移攻击之所以可行，源于研究者们发现的一个令人惊讶的现象：**对抗样本具有跨模型的迁移性**。

2014年，Szegedy 等人在研究对抗样本时首次注意到，针对某个特定模型生成的对抗样本，不仅能欺骗该模型，还能以一定概率欺骗其他结构完全不同的模型。这一发现最初让研究者们感到困惑——不同的模型有不同的结构和参数，为什么会被同一个对抗样本欺骗？

后续的研究逐渐揭示了其中的原因。尽管不同的深度学习模型在结构上存在差异，但它们在学习同一任务时，往往会学到相似的特征表示。例如，无论是 ResNet 还是 VGG，在进行图像分类时，浅层网络都会学习边缘、纹理等低级特征，深层网络都会学习更抽象的语义特征。这种特征学习的相似性，导致不同模型的**决策边界（Decision Boundary）**在某些方向上具有相似的脆弱性。

我们可以用一个直观的类比来理解这一现象。想象不同的模型就像不同的老师，他们各自出题考试。虽然每位老师的出题风格不同，但如果他们教的是同一门课程，那么他们关注的重点知识往往是相似的。一个学生如果找到了某种"投机取巧"的答题方式能骗过一位老师，这种方式很可能对其他老师也有一定效果，因为老师们的评判标准存在共性。

### 2.2 迁移攻击的实施流程

理解了迁移性的原理后，我们来看迁移攻击的具体实施流程。

第一步是**收集训练数据**。攻击者需要收集与目标任务相关的数据。例如，如果目标是攻击一个人脸识别系统，就需要收集人脸图像数据。这些数据不需要与目标模型的训练数据完全相同，只需要属于同一领域即可。

第二步是**训练替代模型**。使用收集到的数据，在本地训练一个或多个模型。这些模型被称为**替代模型（Surrogate Model）**或**代理模型**。替代模型的架构可以是公开的预训练模型，也可以是攻击者自己设计的模型。

第三步是**生成对抗样本**。由于攻击者完全掌握替代模型的信息，可以使用第2章学习的白盒攻击方法（如 FGSM 或 PGD）来生成对抗样本。

第四步是**测试迁移效果**。将生成的对抗样本发送给目标模型，观察攻击是否成功。如果目标模型也被欺骗，说明对抗样本成功迁移。

这个流程的关键在于第二步和第三步。替代模型与目标模型越相似，生成的对抗样本迁移成功的概率就越高。

### 2.3 提高迁移成功率的方法

在实践中，单一替代模型生成的对抗样本迁移成功率往往不够理想。研究者们提出了多种方法来提高迁移性。

**方法一：集成多个替代模型**

与其依赖单一的替代模型，不如同时使用多个不同架构的模型。在生成对抗样本时，要求扰动能够同时欺骗所有这些替代模型。直觉上，如果一个扰动能够欺骗多个不同的模型，那么它更可能捕捉到了深度学习模型的某种共性弱点，从而更可能对未知的目标模型也有效。

具体实现时，可以将多个模型的损失函数加权求和，然后对这个综合损失计算梯度来生成扰动。

**方法二：输入变换增强**

在生成对抗样本的过程中，对输入图像施加各种随机变换，如随机缩放、随机裁剪、随机旋转等。这样生成的扰动需要在各种变换下都保持有效，因此更加"鲁棒"，也更容易迁移到其他模型。

**方法三：攻击中间层特征**

除了攻击模型的最终输出，还可以攻击模型的中间层特征表示。研究表明，不同模型的中间层特征往往比最终输出更加相似，因此针对中间层的攻击可能具有更好的迁移性。

这些方法可以组合使用，以进一步提高迁移攻击的成功率。在实际研究中，结合多种技术的迁移攻击成功率可以达到50%以上，这对于商业 AI 系统来说是一个不可忽视的威胁。

---

## 3. 基于查询的攻击：通过试错逼近目标

### 3.1 查询攻击的基本思想

迁移攻击虽然不需要查询目标模型，但成功率受限于替代模型与目标模型的相似程度。如果攻击者能够反复查询目标模型，是否能找到更可靠的攻击方法？基于查询的攻击正是基于这一思路发展起来的。

查询攻击的核心思想可以用"猜数字"游戏来类比。假设有人心里想了一个1到100之间的数字，你需要猜出这个数字。每次猜测后，对方会告诉你"大了"或"小了"。虽然你不知道答案是什么，但通过不断猜测和获取反馈，你可以逐步缩小范围，最终找到正确答案。

在对抗攻击的场景中，"猜测"对应于尝试不同的扰动，"反馈"对应于模型的输出结果。通过分析模型对不同扰动的响应，攻击者可以逐步优化扰动，最终找到能够欺骗模型的对抗样本。

### 3.2 基于评分的查询攻击

根据目标模型返回信息的详细程度，查询攻击可以分为两类。第一类是**基于评分的攻击（Score-based Attack）**，适用于模型返回置信度分数的情况。

许多商业 API 不仅返回预测的类别标签，还会返回每个类别的置信度分数。例如，一个图像分类 API 可能返回"猫: 0.85, 狗: 0.10, 兔子: 0.05"这样的结果。这些置信度分数为攻击者提供了丰富的信息。

在基于评分的攻击中，攻击者的目标是找到一个扰动，使得目标类别的置信度下降（无目标攻击）或使得指定错误类别的置信度上升（有目标攻击）。由于无法直接计算梯度，攻击者需要使用**梯度估计（Gradient Estimation）**技术。

一种常用的梯度估计方法是**有限差分法**。其基本思想是：在当前点的某个方向上施加微小的扰动，观察输出的变化，从而估计该方向上的梯度。通过在多个随机方向上重复这一过程，可以估计出完整的梯度向量，然后使用类似 FGSM 的方法生成对抗样本。

另一种方法是使用**进化算法（Evolutionary Algorithm）**，如自然进化策略（NES）。这类方法维护一个扰动的"种群"，通过模拟自然选择的过程，不断淘汰效果差的扰动，保留和变异效果好的扰动，最终进化出有效的对抗样本。

### 3.3 基于决策的查询攻击

第二类是**基于决策的攻击（Decision-based Attack）**，适用于模型只返回最终类别标签的情况。这是更具挑战性的场景，因为攻击者获得的信息更少。

在这种场景下，一个著名的算法是 **HopSkipJump**。这个算法的思想非常巧妙：首先找到一个已经能够欺骗模型的起点（例如一张随机噪声图片，模型会将其错误分类），然后逐步向原始图片靠近，同时保持"攻击成功"的状态。最终，算法会找到决策边界上距离原始图片最近的点，这个点就是扰动最小的对抗样本。

这个过程可以想象为：你站在一个房间里，房间被一堵看不见的墙分成两半。你知道自己在墙的哪一侧，但不知道墙的具体位置。通过不断尝试向前走并检查是否越过了墙，你可以逐步找到墙的位置。

### 3.4 查询攻击的效率与成本

查询攻击的一个关键问题是效率。生成一个有效的对抗样本可能需要成百上千次查询，这带来了实际的成本问题。

从查询次数来看，基于评分的攻击通常需要1000到5000次查询，而基于决策的攻击可能需要5000到20000次查询。对于按查询次数收费的商业 API，这意味着每生成一个对抗样本可能需要花费几十甚至上百元人民币。

此外，大量的查询还可能触发服务商的安全机制。许多商业 API 会限制单位时间内的查询次数，或者检测异常的查询模式。如果攻击者在短时间内发送大量相似的图片，很可能会被识别为攻击行为并被封禁。

这些限制使得查询攻击在实际应用中面临诸多挑战，但也为防御方提供了检测和阻止攻击的机会。

---

## 4. 真实案例：商业 AI 服务的黑盒攻击

### 4.1 案例背景

2020年，来自清华大学和加州大学伯克利分校的研究团队发表了一项重要研究，展示了对多个商业人脸识别 API 的黑盒攻击。这项研究选择了包括 Amazon Rekognition、Microsoft Azure Face API 以及国内某知名云服务商在内的多个商业服务作为攻击目标。

研究的目标是验证：在完全不知道这些商业服务内部模型细节的情况下，能否生成有效的对抗样本来欺骗它们？

### 4.2 攻击过程

研究团队采用了迁移攻击的策略。他们首先在本地部署了多个公开的人脸识别模型作为替代模型，包括 FaceNet、ArcFace 等业界常用的模型。

接下来，研究团队使用集成攻击的方法，同时针对多个替代模型生成对抗样本。他们还采用了输入变换增强技术，在生成过程中对图像施加随机变换，以提高对抗样本的迁移性。

生成对抗样本后，研究团队将这些样本上传到各个商业 API 进行测试。他们测试了两种攻击场景：一是让系统无法识别出原本能够识别的人脸（拒绝服务攻击），二是让系统将一个人误识别为另一个人（身份冒充攻击）。

### 4.3 攻击结果与影响

实验结果令人警醒。在拒绝服务攻击场景下，对抗样本的迁移成功率达到了47%到67%，这意味着接近一半到三分之二的对抗样本能够成功欺骗商业 API。在身份冒充攻击场景下，成功率虽然较低，但仍然达到了12%到22%。

这些数字的含义是：如果一个攻击者想要绕过基于人脸识别的身份验证系统，他只需要生成几个对抗样本进行尝试，就有相当大的概率成功。考虑到人脸识别技术已经广泛应用于支付验证、门禁系统、身份认证等安全敏感场景，这一发现具有重要的安全警示意义。

### 4.4 案例启示

这个案例给我们带来了几点重要启示。

首先，商业 AI 服务并非坚不可摧。尽管这些服务由技术实力雄厚的公司提供，使用了先进的模型和大量的训练数据，但它们仍然容易受到对抗攻击的威胁。

其次，对抗样本的迁移性是一个真实存在的安全风险。攻击者不需要知道目标系统的任何内部信息，仅凭公开可用的模型就能发起有效攻击。

最后，这也提醒 AI 服务提供商，在部署模型时需要考虑对抗鲁棒性，不能仅仅关注正常情况下的准确率。

---

## 5. 本章小结

本章探讨了在无法获取模型内部信息的黑盒场景下，如何对 AI 系统发起对抗攻击。

我们首先认识到，黑盒攻击比白盒攻击更具现实意义，因为大多数商业 AI 服务都以黑盒形式提供。在黑盒场景下，攻击者面临无法获取模型结构、参数和梯度的限制，只能通过输入输出来进行攻击。

针对这一挑战，研究者发展出了两种主要策略。迁移攻击利用对抗样本的跨模型迁移性，在本地替代模型上生成对抗样本，然后测试其对目标模型的效果。通过集成多个替代模型、使用输入变换增强等技术，可以显著提高迁移成功率。基于查询的攻击则通过反复查询目标模型，利用梯度估计或进化算法等方法逐步优化扰动，最终找到有效的对抗样本。

通过商业人脸识别 API 被攻击的真实案例，我们看到黑盒攻击并非理论上的可能性，而是实际存在的安全威胁。这提醒我们，在设计和部署 AI 系统时，必须将对抗鲁棒性纳入考量。

---

## 教学资源

**图表与示意图**：
- 图3-1：白盒攻击与黑盒攻击的对比示意图（建议配图）
- 图3-2：迁移攻击的完整流程图（建议配图）
- 图3-3：基于查询攻击的迭代优化过程（建议配图）

**配套实验**：
- 本章内容对应实验 3.3：黑盒迁移攻击
- 实验中将训练简单的替代模型，生成对抗样本并测试其在不同模型间的迁移效果

**延伸阅读**：
- Papernot 等人的论文《Practical Black-Box Attacks against Machine Learning》系统介绍了黑盒攻击的方法
- Chen 等人提出的 HopSkipJump 算法是基于决策攻击的代表性工作

---

## 课后思考题

1. **理解性问题**：对抗样本为什么会具有跨模型的迁移性？请从深度学习模型学习特征的角度进行解释。

2. **分析性问题**：某商业图像识别 API 为了增加攻击难度，决定只返回置信度最高的类别标签，而不返回具体的置信度分数。这种做法能否有效防御查询攻击？攻击者还有什么应对策略？

3. **应用性问题**：假设你是一家提供人脸识别服务的公司的安全工程师，基于本章学习的黑盒攻击知识，你会建议采取哪些措施来提高系统的安全性？

---

## 术语对照表

| 中文术语 | 英文术语 | 简要解释 |
|---------|---------|---------|
| 黑盒攻击 | Black-box Attack | 攻击者不知道模型内部信息，只能通过输入输出进行攻击 |
| 白盒攻击 | White-box Attack | 攻击者完全掌握模型的结构、参数等信息 |
| 迁移攻击 | Transfer Attack | 利用对抗样本的迁移性，在替代模型上生成样本攻击目标模型 |
| 替代模型 | Surrogate Model | 攻击者在本地训练的、用于生成对抗样本的模型 |
| 决策边界 | Decision Boundary | 模型区分不同类别的分界线 |
| 梯度估计 | Gradient Estimation | 在无法直接计算梯度时，通过查询来近似估计梯度的方法 |

---

**法律与伦理提醒**：

对商业 AI 服务进行未经授权的对抗攻击测试可能违反服务条款和相关法律法规。根据《中华人民共和国网络安全法》和《中华人民共和国刑法》的相关规定，未经授权入侵计算机信息系统或破坏计算机信息系统功能的行为可能构成犯罪。本章内容仅用于安全教育和防御研究，读者在进行任何安全测试前，必须获得系统所有者的明确授权。
