{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验 5.1：标签翻转攻击\n",
    "\n",
    "## 实验目标\n",
    "- 理解标签翻转攻击的原理\n",
    "- 观察不同投毒比例对模型性能的影响\n",
    "- 体验数据投毒攻击的持久性危害\n",
    "\n",
    "## 实验背景\n",
    "标签翻转攻击是最简单的数据投毒方式：故意给数据标注错误的标签。\n",
    "虽然简单，但即使只污染1-5%的数据，也能显著影响模型性能。\n",
    "\n",
    "## 预计时间：20分钟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一步：环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置中文显示\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 设置随机种子，确保结果可复现\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"环境准备完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二步：创建模拟数据集\n",
    "\n",
    "我们创建一个简单的二分类数据集来演示标签翻转攻击。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(n_samples=1000):\n",
    "    \"\"\"\n",
    "    创建一个简单的二分类数据集\n",
    "    类别0：中心在(-2, -2)的点\n",
    "    类别1：中心在(2, 2)的点\n",
    "    \"\"\"\n",
    "    # 类别0的数据点\n",
    "    X0 = np.random.randn(n_samples // 2, 2) + np.array([-2, -2])\n",
    "    y0 = np.zeros(n_samples // 2)\n",
    "    \n",
    "    # 类别1的数据点\n",
    "    X1 = np.random.randn(n_samples // 2, 2) + np.array([2, 2])\n",
    "    y1 = np.ones(n_samples // 2)\n",
    "    \n",
    "    # 合并数据\n",
    "    X = np.vstack([X0, X1])\n",
    "    y = np.hstack([y0, y1])\n",
    "    \n",
    "    # 打乱顺序\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    return X[indices], y[indices]\n",
    "\n",
    "# 创建训练集和测试集\n",
    "X_train, y_train = create_dataset(800)\n",
    "X_test, y_test = create_dataset(200)\n",
    "\n",
    "print(f\"训练集大小: {len(X_train)}\")\n",
    "print(f\"测试集大小: {len(X_test)}\")\n",
    "\n",
    "# 可视化原始数据\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], c='blue', label='类别0', alpha=0.5)\n",
    "plt.scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], c='red', label='类别1', alpha=0.5)\n",
    "plt.xlabel('特征1')\n",
    "plt.ylabel('特征2')\n",
    "plt.title('原始训练数据（干净）')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三步：实现标签翻转攻击\n",
    "\n",
    "标签翻转攻击的核心：随机选择一部分样本，将其标签翻转（0→1 或 1→0）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poison_labels(y, poison_ratio):\n",
    "    \"\"\"\n",
    "    对标签进行投毒（翻转）\n",
    "    \n",
    "    参数:\n",
    "        y: 原始标签数组\n",
    "        poison_ratio: 投毒比例（0.0-1.0）\n",
    "    \n",
    "    返回:\n",
    "        投毒后的标签数组\n",
    "    \"\"\"\n",
    "    y_poisoned = y.copy()\n",
    "    n_samples = len(y)\n",
    "    \n",
    "    # 【填空1】计算需要翻转的样本数量\n",
    "    # 提示：用总样本数乘以投毒比例，然后取整\n",
    "    # 参考答案：n_poison = int(n_samples * poison_ratio)\n",
    "    n_poison = ___________________\n",
    "    \n",
    "    # 随机选择要翻转的样本索引\n",
    "    poison_indices = np.random.choice(n_samples, n_poison, replace=False)\n",
    "    \n",
    "    # 【填空2】翻转选中样本的标签（0变1，1变0）\n",
    "    # 提示：用 1 减去原标签即可实现翻转\n",
    "    # 参考答案：y_poisoned[poison_indices] = 1 - y_poisoned[poison_indices]\n",
    "    y_poisoned[poison_indices] = ___________________\n",
    "    \n",
    "    return y_poisoned, poison_indices\n",
    "\n",
    "# 测试：对10%的数据进行投毒\n",
    "y_train_poisoned, poison_idx = poison_labels(y_train, 0.1)\n",
    "print(f\"投毒样本数量: {len(poison_idx)}\")\n",
    "print(f\"投毒比例: {len(poison_idx)/len(y_train)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四步：定义简单分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    \"\"\"简单的二分类神经网络\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 两层全连接网络\n",
    "        self.fc1 = nn.Linear(2, 16)   # 输入2维，隐藏层16个神经元\n",
    "        self.fc2 = nn.Linear(16, 1)   # 输出1维（二分类）\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "def train_model(X_train, y_train, epochs=100):\n",
    "    \"\"\"训练分类模型\"\"\"\n",
    "    # 转换为PyTorch张量\n",
    "    X = torch.FloatTensor(X_train)\n",
    "    y = torch.FloatTensor(y_train).reshape(-1, 1)\n",
    "    \n",
    "    # 创建模型\n",
    "    model = SimpleClassifier()\n",
    "    criterion = nn.BCELoss()  # 二分类交叉熵损失\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # 训练\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"评估模型准确率\"\"\"\n",
    "    X = torch.FloatTensor(X_test)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X).numpy().flatten()\n",
    "    \n",
    "    # 【填空3】将概率转换为类别预测（>0.5为类别1）\n",
    "    # 提示：使用比较运算符，结果转为整数\n",
    "    # 参考答案：predicted_labels = (predictions > 0.5).astype(int)\n",
    "    predicted_labels = ___________________\n",
    "    \n",
    "    # 计算准确率\n",
    "    accuracy = np.mean(predicted_labels == y_test)\n",
    "    return accuracy\n",
    "\n",
    "print(\"模型定义完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第五步：对比实验 - 不同投毒比例的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试不同的投毒比例\n",
    "poison_ratios = [0.0, 0.05, 0.10, 0.15, 0.20, 0.30]\n",
    "accuracies = []\n",
    "\n",
    "print(\"开始对比实验...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for ratio in poison_ratios:\n",
    "    # 对训练数据进行投毒\n",
    "    y_poisoned, _ = poison_labels(y_train, ratio)\n",
    "    \n",
    "    # 训练模型\n",
    "    model = train_model(X_train, y_poisoned, epochs=100)\n",
    "    \n",
    "    # 在干净的测试集上评估\n",
    "    acc = evaluate_model(model, X_test, y_test)\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "    print(f\"投毒比例: {ratio*100:5.1f}% | 测试准确率: {acc*100:.2f}%\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"实验完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第六步：可视化投毒效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制投毒比例与准确率的关系\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# 左图：准确率变化\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot([r*100 for r in poison_ratios], [a*100 for a in accuracies], \n",
    "         'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('投毒比例 (%)')\n",
    "plt.ylabel('测试准确率 (%)')\n",
    "plt.title('标签翻转攻击效果')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([40, 100])\n",
    "\n",
    "# 右图：准确率下降幅度\n",
    "plt.subplot(1, 2, 2)\n",
    "baseline = accuracies[0]\n",
    "drops = [(baseline - a) * 100 for a in accuracies]\n",
    "colors = ['green' if d < 5 else 'orange' if d < 15 else 'red' for d in drops]\n",
    "plt.bar([f\"{r*100:.0f}%\" for r in poison_ratios], drops, color=colors)\n",
    "plt.xlabel('投毒比例')\n",
    "plt.ylabel('准确率下降 (%)')\n",
    "plt.title('准确率下降幅度')\n",
    "plt.axhline(y=5, color='orange', linestyle='--', label='轻微影响线')\n",
    "plt.axhline(y=15, color='red', linestyle='--', label='严重影响线')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第七步：可视化投毒数据分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对20%的数据投毒，可视化投毒样本的位置\n",
    "y_poisoned_20, poison_idx_20 = poison_labels(y_train, 0.20)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 左图：原始数据\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], \n",
    "            c='blue', label='类别0', alpha=0.5)\n",
    "plt.scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], \n",
    "            c='red', label='类别1', alpha=0.5)\n",
    "plt.xlabel('特征1')\n",
    "plt.ylabel('特征2')\n",
    "plt.title('原始数据（干净标签）')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 右图：投毒后数据（标记被翻转的样本）\n",
    "plt.subplot(1, 2, 2)\n",
    "# 正常样本\n",
    "normal_idx = np.setdiff1d(np.arange(len(y_train)), poison_idx_20)\n",
    "plt.scatter(X_train[normal_idx, 0], X_train[normal_idx, 1], \n",
    "            c=['blue' if y==0 else 'red' for y in y_poisoned_20[normal_idx]], \n",
    "            alpha=0.3, label='正常样本')\n",
    "# 投毒样本（用特殊标记）\n",
    "plt.scatter(X_train[poison_idx_20, 0], X_train[poison_idx_20, 1], \n",
    "            c='yellow', edgecolors='black', s=100, marker='X', \n",
    "            label='投毒样本（标签被翻转）')\n",
    "plt.xlabel('特征1')\n",
    "plt.ylabel('特征2')\n",
    "plt.title('投毒后数据（20%标签被翻转）')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n观察：黄色X标记的点是被投毒的样本\")\n",
    "print(\"这些点的标签被翻转，会导致模型学习到错误的决策边界\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验总结\n",
    "\n",
    "### 关键发现\n",
    "\n",
    "1. **投毒效果显著**：即使只翻转5-10%的标签，模型准确率就会明显下降\n",
    "\n",
    "2. **攻击简单有效**：标签翻转是最简单的投毒方式，不需要了解模型结构\n",
    "\n",
    "3. **危害持久**：一旦模型被投毒数据训练，错误会持续存在\n",
    "\n",
    "### 思考问题\n",
    "\n",
    "1. 为什么即使只有少量投毒样本，也能显著影响模型性能？\n",
    "\n",
    "2. 如果你是数据收集方，如何防止众包标注中的恶意标签？\n",
    "\n",
    "3. 标签翻转攻击有什么明显的缺点？（提示：考虑检测难度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实验完成检查\n",
    "print(\"=\"*50)\n",
    "print(\"实验 5.1 完成！\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\n请回答以下问题：\")\n",
    "print(\"1. 投毒比例从0%增加到20%，准确率下降了多少？\")\n",
    "print(\"2. 观察可视化图，投毒样本主要分布在什么位置？\")\n",
    "print(\"3. 为什么标签翻转攻击容易被检测？\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
