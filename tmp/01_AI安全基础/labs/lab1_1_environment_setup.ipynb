{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验 1.1：环境搭建与模型调用\n",
    "\n",
    "## 实验目标\n",
    "- 熟悉 Cloud Studio 实验环境\n",
    "- 学会加载和调用大语言模型\n",
    "- 理解模型参数（Temperature）对输出的影响\n",
    "- 为后续安全实验打下基础\n",
    "\n",
    "## 实验环境\n",
    "- 平台：腾讯 Cloud Studio（https://cloudstudio.net/）\n",
    "- GPU：NVIDIA Tesla T4（16GB 显存）\n",
    "- 模型：Qwen2-1.5B-Instruct（阿里通义千问）\n",
    "\n",
    "## 预计时间：20分钟\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一部分：环境验证\n",
    "\n",
    "首先确认 GPU 环境是否正确配置。Cloud Studio 提供免费的 T4 GPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 环境验证脚本 ======\n",
    "print(\"=\" * 50)\n",
    "print(\"AI 安全实验环境检查\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 检查 Python 版本\n",
    "import sys\n",
    "print(f\"\\n[1] Python 版本: {sys.version.split()[0]}\")\n",
    "\n",
    "# 检查 PyTorch 和 CUDA\n",
    "import torch\n",
    "print(f\"[2] PyTorch 版本: {torch.__version__}\")\n",
    "print(f\"    CUDA 可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"    GPU 型号: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"    GPU 显存: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# 检查 Transformers\n",
    "import transformers\n",
    "print(f\"[3] Transformers 版本: {transformers.__version__}\")\n",
    "\n",
    "# 检查其他依赖\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "print(f\"[4] NumPy 版本: {np.__version__}\")\n",
    "print(f\"[5] Matplotlib 版本: {matplotlib.__version__}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"✓ 环境检查通过！GPU 已就绪\")\n",
    "else:\n",
    "    print(\"⚠ 警告：未检测到 GPU，请确认已选择 GPU 环境\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第二部分：加载中文大语言模型\n",
    "\n",
    "我们使用阿里的 **Qwen2-1.5B-Instruct** 模型，这是目前最优秀的中文开源模型之一。\n",
    "\n",
    "**为什么选择 Qwen2？**\n",
    "- 中文能力顶级，专门针对中文优化\n",
    "- 1.5B 参数，T4 GPU 轻松运行（约 4GB 显存）\n",
    "- 有安全护栏，适合后续安全实验\n",
    "- Instruct 版本经过指令微调，对话体验好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 加载 Qwen2-1.5B 模型 ======\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "\n",
    "print(f\"正在加载模型: {model_name}\")\n",
    "print(\"首次加载需要下载模型文件（约 3GB），请耐心等待...\")\n",
    "\n",
    "# 【填空 1】加载分词器\n",
    "# 提示：使用 AutoTokenizer.from_pretrained() 方法，参数是 model_name\n",
    "# 参考答案：tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer = ___________________\n",
    "\n",
    "# 加载模型到 GPU（使用半精度节省显存）\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # 半精度，显存减半\n",
    "    device_map=\"auto\"           # 自动选择设备（GPU）\n",
    ")\n",
    "\n",
    "print(\"\\n✓ 模型加载成功！\")\n",
    "print(f\"  模型参数量: {model.num_parameters() / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第三部分：与模型对话\n",
    "\n",
    "现在让我们定义一个对话函数，与模型进行交互。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 定义对话函数 ======\n",
    "\n",
    "def chat(user_message, system_prompt=\"你是一个有帮助的AI助手。\"):\n",
    "    \"\"\"\n",
    "    与模型进行对话\n",
    "    \n",
    "    参数:\n",
    "        user_message: 用户输入的消息\n",
    "        system_prompt: 系统提示词（定义AI的角色和行为）\n",
    "    \n",
    "    返回:\n",
    "        模型的回复文本\n",
    "    \"\"\"\n",
    "    # 构建对话格式（Qwen 使用 messages 格式）\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    # 【填空 2】将对话转换为模型输入格式\n",
    "    # 提示：使用 tokenizer.apply_chat_template() 方法\n",
    "    # 参考答案：text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    text = ___________________\n",
    "    \n",
    "    # 编码输入并移到 GPU\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # 生成回复\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # 解码输出（只取新生成的部分）\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# 测试对话\n",
    "print(\"测试对话功能...\")\n",
    "response = chat(\"你好，请用一句话介绍你自己。\")\n",
    "print(f\"\\n用户: 你好，请用一句话介绍你自己。\")\n",
    "print(f\"AI: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第四部分：探索模型能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 测试模型的不同能力 ======\n",
    "\n",
    "test_questions = [\n",
    "    \"什么是人工智能？请用简单的话解释。\",\n",
    "    \"用 Python 写一个计算 1 到 100 求和的代码。\",\n",
    "    \"帮我写一首关于春天的五言绝句。\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"模型能力测试\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n【问题 {i}】{question}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 【填空 3】调用对话函数获取回复\n",
    "    # 提示：调用前面定义的 chat() 函数，参数是 question\n",
    "    # 参考答案：response = chat(question)\n",
    "    response = ___________________\n",
    "    \n",
    "    print(f\"【回复】\\n{response}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第五部分：Temperature 参数实验\n",
    "\n",
    "Temperature 控制输出的随机性：\n",
    "- **低温（如 0.1）**：输出更确定、更保守、更一致\n",
    "- **高温（如 1.2）**：输出更随机、更有创意、更多样\n",
    "\n",
    "这个参数在安全测试中很重要：高温设置可能导致模型产生意外输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Temperature 对比实验 ======\n",
    "\n",
    "def chat_with_temp(message, temperature):\n",
    "    \"\"\"使用指定 temperature 进行对话\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一个有帮助的AI助手。\"},\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "prompt = \"给我讲一个关于机器人的故事的开头。\"\n",
    "\n",
    "print(f\"测试问题: {prompt}\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"低温生成 (Temperature = 0.1) - 结果更一致\")\n",
    "print(\"=\" * 50)\n",
    "for i in range(3):\n",
    "    result = chat_with_temp(prompt, temperature=0.1)\n",
    "    print(f\"第 {i+1} 次: {result[:80]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"高温生成 (Temperature = 1.2) - 结果更多样\")\n",
    "print(\"=\" * 50)\n",
    "for i in range(3):\n",
    "    result = chat_with_temp(prompt, temperature=1.2)\n",
    "    print(f\"第 {i+1} 次: {result[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 观察问题\n",
    "\n",
    "1. 低温和高温的输出有什么区别？\n",
    "2. 如果你在开发一个银行客服 AI，应该用高温还是低温？为什么？\n",
    "3. 高温设置可能带来什么安全风险？\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第六部分：观察系统提示的作用\n",
    "\n",
    "**系统提示（System Prompt）** 定义了 AI 的角色和行为规范。\n",
    "\n",
    "这在安全测试中非常重要——很多攻击就是试图绕过系统提示的限制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 系统提示对比实验 ======\n",
    "\n",
    "system_prompts = {\n",
    "    \"默认助手\": \"你是一个有帮助的AI助手。\",\n",
    "    \"安全专家\": \"你是一位网络安全专家，专门回答安全相关的问题。对于非安全问题，礼貌地引导用户回到安全话题。\",\n",
    "    \"严格模式\": \"你是一个严格的AI助手。你只能回答编程和技术问题，对于其他问题你必须拒绝回答。\",\n",
    "}\n",
    "\n",
    "test_message = \"今天天气怎么样？\"\n",
    "\n",
    "print(f\"测试问题: {test_message}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, prompt in system_prompts.items():\n",
    "    print(f\"\\n【{name}】\")\n",
    "    print(f\"系统提示: {prompt[:50]}...\")\n",
    "    print(\"-\" * 40)\n",
    "    response = chat(test_message, system_prompt=prompt)\n",
    "    print(f\"回复: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 观察问题\n",
    "\n",
    "1. 不同系统提示下，模型对同一问题的回答有什么不同？\n",
    "2. \"严格模式\"下模型是否成功拒绝了非技术问题？\n",
    "3. 如果有人想让模型忽略系统提示，可能会怎么做？（这就是提示词注入攻击的核心）\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验总结\n",
    "\n",
    "通过本实验，你应该：\n",
    "\n",
    "✅ 熟悉了 Cloud Studio 的 GPU 环境\n",
    "\n",
    "✅ 学会了加载和调用 Qwen2 中文大语言模型\n",
    "\n",
    "✅ 理解了 Temperature 参数对输出的影响\n",
    "\n",
    "✅ 观察了系统提示如何控制模型行为\n",
    "\n",
    "### 关键收获\n",
    "\n",
    "1. **Temperature 影响安全性**：高温可能导致意外输出\n",
    "2. **系统提示是第一道防线**：但它可能被攻击者绕过\n",
    "3. **中文模型选择很重要**：Qwen2 是目前最佳的中文开源选择\n",
    "\n",
    "---\n",
    "\n",
    "## 下一步\n",
    "\n",
    "继续完成 **实验 1.2：AI 漏洞侦察**，学习如何探测模型的安全边界。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考答案\n",
    "\n",
    "**填空 1**：`tokenizer = AutoTokenizer.from_pretrained(model_name)`\n",
    "\n",
    "**填空 2**：`text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)`\n",
    "\n",
    "**填空 3**：`response = chat(question)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理显存（实验结束后运行）\n",
    "# del model, tokenizer\n",
    "# torch.cuda.empty_cache()\n",
    "# print(\"显存已清理\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
