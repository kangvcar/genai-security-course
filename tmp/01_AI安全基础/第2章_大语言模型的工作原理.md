# 第2章：大语言模型的工作原理

当我们与 ChatGPT 对话时，它能够理解我们的问题，生成流畅的回答，甚至展现出一定的"创造力"。这背后的技术原理是什么？为什么大语言模型有时会"一本正经地胡说八道"？为什么调整一个参数就能让模型的回答风格发生巨大变化？本章将揭开大语言模型的神秘面纱，帮助大家理解这些强大 AI 系统的工作机制。更重要的是，理解这些原理将帮助我们认识到大语言模型的能力边界和潜在的安全风险。

## 章节目标

学完本章后，你将能够：

1. **理解大语言模型的核心机制**：掌握 Transformer 架构的基本思想和文本分词（Tokenization）的原理
2. **认识关键参数的作用**：理解 Temperature、Top-p、Max Tokens 等参数如何影响模型的输出行为
3. **把握模型的能力与局限**：了解大语言模型擅长什么、不擅长什么，以及为什么会出现"幻觉"等问题
4. **建立安全视角**：从工作原理的角度理解大语言模型的安全风险来源

## 1. 从"自动补全"说起：理解语言模型的本质

### 1.1 一个熟悉的场景

当你在手机上打字时，输入法会自动推荐下一个可能的词语。例如，当你输入"今天天气"，输入法可能会建议"真好"、"不错"、"很热"等选项。这个功能看似简单，但它体现了语言模型的核心思想：根据已有的文本，预测接下来最可能出现的内容。

大语言模型本质上就是一个超级强大的"自动补全系统"。不同的是，它不仅能预测下一个词，还能预测整个句子、段落，甚至完整的文章。它通过学习海量的文本数据，掌握了语言的统计规律和语义关系。

让我们通过一个类比来理解这个过程：

想象一个人读过了图书馆里所有的书。当你给他一个句子的开头，他能够根据记忆中类似的句子，推测出最合理的续写。他不是在"思考"或"理解"，而是在进行高度复杂的模式匹配和统计推断。大语言模型的工作方式与此类似，只不过它处理的数据量远超人类的记忆容量。

### 1.2 语言模型的训练过程

大语言模型是如何学会"补全"文本的？这个过程称为"训练"。

训练的基本思路是：给模型展示大量的文本，让它学习预测"下一个词"。具体来说：

1. **准备训练数据**：收集海量文本，可能包括网页、书籍、新闻、对话等各种来源的内容。例如，GPT-3 的训练数据包含了约 45TB 的文本。

2. **学习预测**：模型会看到一个句子的前半部分，然后尝试预测下一个词。如果预测错误，模型会调整内部参数，使得下次遇到类似情况时能做出更好的预测。

3. **反复迭代**：这个过程会重复数百万次，模型逐渐学会了语言的各种模式：语法规则、常见搭配、语义关系，甚至一些常识知识。

有同学可能会问：模型真的"理解"了语言吗？这是一个深刻的问题。从技术角度看，模型并没有像人类那样理解语言的含义，它只是学会了文本的统计规律。但这种统计学习的效果如此之好，以至于在很多任务上，模型的表现接近甚至超过人类。

这种"不理解但能做得很好"的特性，正是大语言模型安全问题的根源之一。模型可能生成看似合理但实际错误的内容，因为它只是在进行模式匹配，而不是真正理解事实。

## 2. Transformer：大语言模型的核心架构

### 2.1 为什么需要 Transformer

在 Transformer 出现之前，处理文本的主流方法是循环神经网络（RNN）。RNN 的工作方式类似于逐字阅读：从左到右依次处理每个词，每次处理都会参考之前看到的内容。

这种方法有一个明显的缺点：处理长文本时效率很低，而且难以捕捉距离较远的词之间的关系。例如，在句子"我昨天去了北京，那里的天气很好"中，"那里"指的是"北京"，但这两个词之间隔了好几个字。RNN 需要"记住"前面的内容，才能理解这种关联。

Transformer 架构在 2017 年被提出，它采用了一种全新的思路：不再逐字处理，而是同时关注整个句子中所有词之间的关系。这就像从"逐字阅读"升级到"一眼看全文"。

### 2.2 注意力机制：Transformer 的核心

Transformer 的关键创新是"注意力机制"（Attention Mechanism）。这个机制让模型能够判断句子中哪些词之间的关系更重要。

让我们用一个例子来理解：

句子："银行的利率上涨了"

当模型处理"利率"这个词时，注意力机制会让它重点关注"银行"这个词，因为这两个词在语义上密切相关。而"的"、"了"这些虚词的重要性就相对较低。

这个过程可以类比为：当你阅读一篇文章时，你的注意力会自然地聚焦在关键词上，而不是平均分配给每个字。Transformer 通过数学计算实现了类似的效果。

注意力机制的优势在于：

1. **并行处理**：可以同时计算所有词之间的关系，而不需要逐个处理，大大提高了训练和推理的速度。

2. **长距离依赖**：能够轻松捕捉距离很远的词之间的关系，不受文本长度的限制。

3. **可解释性**：我们可以可视化注意力权重，看到模型在处理某个词时关注了哪些其他词，这为理解模型行为提供了一定的线索。

### 2.3 Transformer 的层次结构

一个完整的 Transformer 模型通常包含多个层（Layer），每一层都会对文本进行一次处理和转换。以 GPT-3 为例，它有 96 层，每一层都在前一层的基础上提取更高级的语义特征。

这种层次结构可以类比为：

- **第一层**：识别基本的词汇和语法结构
- **中间层**：理解句子的语义和逻辑关系
- **最后几层**：整合全局信息，生成最终的输出

层数越多，模型的表达能力越强，但计算成本也越高。这就是为什么大型语言模型需要强大的计算资源。

有同学可能会问：既然 Transformer 这么强大，为什么还会出现错误？原因在于，Transformer 本质上仍然是一个统计模型。它学习的是训练数据中的模式，而不是真正的知识和逻辑。如果训练数据中存在偏见、错误或恶意内容，模型也可能学到这些问题。这正是我们在安全防护中需要特别关注的。

## 3. 文本分词：将语言转化为数字

### 3.1 计算机如何"看懂"文字

计算机只能处理数字，不能直接理解文字。因此，在将文本输入模型之前，需要先将其转换为数字形式。这个过程称为"分词"（Tokenization）。

分词的基本思路是：将文本切分成更小的单元（称为 Token），然后为每个 Token 分配一个唯一的数字编号。模型实际处理的就是这些数字序列。

### 3.2 分词的不同策略

分词有多种策略，不同的策略会影响模型的性能和行为：

**1. 按字符分词**

最简单的方法是将每个字符作为一个 Token。例如，"你好"会被分为"你"和"好"两个 Token。

优点：词表很小，可以处理任何文本。
缺点：序列会很长，模型需要学习如何将字符组合成词，增加了学习难度。

**2. 按词分词**

将每个完整的词作为一个 Token。例如，"今天天气很好"会被分为"今天"、"天气"、"很"、"好"四个 Token。

优点：序列较短，语义单元更清晰。
缺点：词表会非常大，而且无法处理未见过的新词。

**3. 子词分词（Subword Tokenization）**

这是目前主流的方法，介于字符和词之间。它将常见的词保持完整，将罕见的词拆分成更小的单元。

例如，"unhappiness"可能被分为"un"、"happiness"两个 Token。这样既保持了词表的合理大小，又能处理新词。

GPT 系列模型使用的是一种叫做 BPE（Byte Pair Encoding）的子词分词方法。这种方法通过统计训练数据中的字符组合频率，自动学习最优的分词策略。

### 3.3 分词对安全的影响

分词方式会影响模型的安全性。让我们看一个例子：

假设我们想让模型拒绝回答关于"炸弹"的问题。如果"炸弹"被分为一个 Token，模型可以学会识别并拒绝。但如果攻击者使用"炸 弹"（中间加空格）或"zhàdàn"（拼音），分词结果就会不同，可能绕过过滤机制。

这说明，分词是模型的第一道处理环节，也是潜在的攻击点。攻击者可以通过精心设计的输入，利用分词的特性来绕过安全机制。

此外，不同语言的分词难度不同。中文没有明显的词边界，分词更加复杂，这也可能带来额外的安全挑战。

## 4. 关键参数：控制模型的行为

当我们使用大语言模型时，通常可以调整一些参数来控制其输出。理解这些参数的作用，对于安全使用模型至关重要。

### 4.1 Temperature：控制"创造力"

Temperature（温度）参数控制模型输出的随机性。它的取值范围通常是 0 到 2。

**Temperature = 0**：模型总是选择概率最高的词，输出是确定的、保守的。
**Temperature = 1**：按照模型计算的概率分布采样，输出较为平衡。
**Temperature > 1**：增加低概率词被选中的机会，输出更加多样化、"创造性"，但也可能更加不可预测。

让我们通过一个例子理解：

输入："今天天气"

- Temperature = 0：模型可能总是输出"很好"（最高概率）
- Temperature = 1：可能输出"很好"、"不错"、"晴朗"等多种选项
- Temperature = 2：可能输出"糟糕"、"多变"甚至一些不太常见的词

类比：Temperature 就像一个人的"冒险精神"。低温度的人总是选择最安全的选项，高温度的人则更愿意尝试新鲜事物，但也可能做出不靠谱的选择。

**安全影响**：

- 过低的 Temperature 可能让模型输出过于刻板，缺乏灵活性
- 过高的 Temperature 可能让模型输出不可控的内容，包括不当或错误的信息
- 在安全关键的应用中，通常使用较低的 Temperature 以保证输出的稳定性

### 4.2 Top-p（Nucleus Sampling）：控制候选范围

Top-p 是另一种控制输出随机性的方法。它的工作原理是：只从累积概率达到 p 的最高概率词中采样。

例如，Top-p = 0.9 意味着：

1. 将所有可能的下一个词按概率从高到低排序
2. 累加概率，直到总和达到 0.9
3. 只从这些词中随机选择

这种方法的优势是：它会根据概率分布动态调整候选词的数量。如果某个词的概率特别高（如 0.95），那么候选范围就很小；如果概率分布比较平均，候选范围就会更大。

**安全影响**：

- Top-p 可以防止模型选择概率极低的"奇怪"词汇
- 合理设置 Top-p（如 0.9）可以在保持多样性的同时避免极端输出
- 在需要高可靠性的场景中，可以使用较小的 Top-p 值（如 0.5）

### 4.3 Max Tokens：控制输出长度

Max Tokens 参数限制模型生成的最大 Token 数量。这个参数看似简单，但对安全和成本控制都很重要。

**安全影响**：

1. **防止资源滥用**：如果不限制输出长度，恶意用户可能让模型生成极长的文本，消耗大量计算资源。

2. **控制信息泄露**：在某些攻击场景中，攻击者可能试图让模型输出大量信息以提取训练数据。限制输出长度可以降低这种风险。

3. **避免失控输出**：有时模型可能陷入重复或无意义的生成循环。Max Tokens 可以作为一个"安全阀"，防止这种情况持续太久。

### 4.4 其他重要参数

**Frequency Penalty 和 Presence Penalty**：

这两个参数用于减少重复。Frequency Penalty 会降低已经出现多次的词再次出现的概率，Presence Penalty 会降低已经出现过的词再次出现的概率（不考虑出现次数）。

适当使用这些参数可以让输出更加多样化，避免模型陷入重复循环。

**Stop Sequences**：

指定一些特殊的字符串，当模型生成这些字符串时立即停止。这可以用于控制输出的格式，也可以作为一种安全机制，防止模型生成特定的敏感内容。

有同学可能会问：既然有这么多参数可以调整，是不是意味着我们可以完全控制模型的行为？答案是否定的。这些参数只能在一定程度上影响模型的输出，但无法完全消除模型的不确定性和潜在风险。这就是为什么我们需要多层次的安全防护措施。

## 5. 大语言模型的能力与局限

理解大语言模型能做什么、不能做什么，对于安全使用这些系统至关重要。

### 5.1 大语言模型擅长的任务

**1. 文本生成和续写**

这是模型的核心能力。无论是写文章、编故事，还是生成代码，模型都能根据提示生成连贯的文本。

**2. 文本理解和分析**

模型可以回答问题、总结文本、提取关键信息。虽然它不是真正"理解"文本，但统计学习的效果足以应对很多实际任务。

**3. 格式转换**

翻译、改写、风格转换等任务，模型都能胜任。例如，将正式文本改写为口语化表达，或将中文翻译为英文。

**4. 简单推理**

对于常见的逻辑推理和常识问题，模型通常能给出正确答案。这是因为训练数据中包含了大量类似的例子。

### 5.2 大语言模型的局限性

**1. 缺乏真实世界的理解**

模型只是学习了文本的统计规律，并不真正理解现实世界。例如，它可能"知道"水会往低处流，但这只是因为训练数据中经常出现这样的表述，而不是因为它理解了重力。

**2. 知识截止日期**

模型的知识来自训练数据，而训练数据有一个截止日期。对于截止日期之后发生的事情，模型一无所知。例如，GPT-3.5 的知识截止于 2021 年 9 月，它不知道之后发生的新闻事件。

**3. 容易产生"幻觉"**

这是大语言模型最严重的问题之一。模型有时会生成看似合理但完全错误的信息，这种现象称为"幻觉"（Hallucination）。

例如，你问模型："请列举三篇关于量子计算的经典论文"，模型可能会编造出听起来很专业的论文标题和作者，但这些论文实际上并不存在。

为什么会出现幻觉？因为模型的目标是生成"看起来合理"的文本，而不是"事实正确"的文本。当模型不确定答案时，它不会说"我不知道"，而是会根据训练数据中的模式生成一个"听起来像答案"的内容。

**4. 缺乏持续学习能力**

模型在训练完成后，参数就固定了。它不会从与用户的对话中学习新知识。每次对话都是独立的，模型不会"记住"之前的交互（除非在同一个会话中）。

**5. 容易受到提示词的影响**

模型的输出高度依赖于输入的提示词。稍微改变提问方式，可能得到完全不同的答案。这种敏感性既是灵活性的体现，也是安全风险的来源。

### 5.3 从局限性看安全风险

这些局限性直接导致了安全问题：

- **幻觉问题**可能导致错误信息传播，在医疗、法律等关键领域尤其危险
- **对提示词的敏感性**使得提示词注入攻击成为可能
- **缺乏真实理解**意味着模型可能被诱导生成有害内容，因为它不理解这些内容的实际危害
- **知识截止日期**可能导致模型给出过时的建议，在快速变化的领域（如网络安全）尤其成问题

理解这些局限性，能够帮助我们在使用大语言模型时保持警惕，不盲目信任其输出。

## 6. 提示工程：与模型有效交互的艺术

既然大语言模型的行为高度依赖于输入，那么如何设计好的提示词就成为一门重要的技能，这被称为"提示工程"（Prompt Engineering）。

### 6.1 提示词的基本结构

一个有效的提示词通常包含以下几个部分：

**1. 角色设定**：告诉模型它应该扮演什么角色
- 例如："你是一个专业的 Python 程序员"

**2. 任务描述**：清楚地说明你希望模型做什么
- 例如："请帮我写一个函数，计算列表中所有偶数的和"

**3. 输入数据**：提供必要的信息或上下文
- 例如："列表是 [1, 2, 3, 4, 5, 6]"

**4. 输出格式**：指定期望的输出形式
- 例如："请用 Python 代码回答，并添加注释"

**5. 约束条件**：说明需要遵守的限制
- 例如："不要使用第三方库"

### 6.2 提示工程的技巧

**1. 提供示例（Few-shot Learning）**

在提示词中包含几个示例，可以显著提高模型的表现。

例如：
```
将以下句子翻译成英文：

中文：今天天气很好
英文：The weather is nice today

中文：我喜欢编程
英文：I like programming

中文：人工智能很有趣
英文：
```

**2. 链式思考（Chain-of-Thought）**

对于复杂问题，要求模型"一步步思考"可以提高准确率。

例如：
```
问题：小明有 15 个苹果，他给了小红 3 个，又买了 7 个，现在有多少个？
请一步步计算：
```

**3. 明确的指令**

使用清晰、具体的语言，避免歧义。

不好的提示："告诉我关于 Python 的事情"
好的提示："请列举 Python 语言的三个主要特点，并简要解释每个特点"

### 6.3 提示工程与安全的关系

提示工程是一把双刃剑：

**正面作用**：
- 通过精心设计的系统提示词，可以引导模型遵守安全规则
- 可以使用提示词来检测和过滤不当输入

**负面风险**：
- 攻击者也可以使用提示工程技巧来绕过安全机制
- 复杂的提示词可能产生意想不到的副作用

这就是为什么在后续章节中，我们会专门学习提示词注入攻击和防御技术。

## 本章小结

本章我们深入了解了大语言模型的工作原理。让我们回顾关键要点：

1. **语言模型的本质**：大语言模型本质上是一个超级强大的"自动补全系统"，通过学习海量文本数据掌握语言的统计规律。

2. **Transformer 架构**：注意力机制让模型能够同时关注文本中所有词之间的关系，这是大语言模型强大能力的基础。

3. **文本分词**：将文字转换为数字的过程，分词策略会影响模型的性能和安全性。

4. **关键参数**：Temperature、Top-p、Max Tokens 等参数控制模型的输出行为，合理设置这些参数对安全使用模型至关重要。

5. **能力与局限**：模型在文本生成、理解等任务上表现出色，但也存在幻觉、缺乏真实理解等严重局限。

6. **提示工程**：有效的提示词设计可以显著提升模型表现，但也可能被攻击者利用。

理解这些原理，我们就能更好地认识大语言模型的安全风险来源。在下一章中，我们将学习如何以"红队"的视角思考 AI 安全问题。

## 教学资源

**配套实验**：
- 实验 1.1：环境搭建与模型调用
  通过实际操作体验大语言模型的基本使用和参数调整

**推荐阅读**：
- "Attention Is All You Need" - Transformer 原始论文（可选阅读，了解技术细节）
- OpenAI 的 GPT 系列技术博客

**延伸思考**：
- 尝试使用不同的 Temperature 值与 ChatGPT 对话，观察输出的变化
- 思考：如果你要设计一个安全的 AI 客服系统，应该如何设置这些参数？

## 课后思考题

1. **理解性问题**：请用自己的话解释，为什么说大语言模型是"超级自动补全系统"？这种工作方式会带来哪些安全隐患？

2. **分析性问题**：Temperature 参数和 Top-p 参数都能控制输出的随机性，但它们的工作原理不同。请分析：在什么场景下应该使用较低的 Temperature？在什么场景下应该使用较小的 Top-p？

3. **应用性问题**：假设你正在开发一个 AI 写作助手，用户输入一个主题，系统自动生成文章。根据本章学到的知识，你会如何设置模型参数（Temperature、Top-p、Max Tokens）？请说明理由，并考虑可能的安全风险。
