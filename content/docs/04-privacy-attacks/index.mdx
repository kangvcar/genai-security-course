---
title: 隐私攻击总览
description: 学习 AI 系统的隐私泄露风险和防护技术
---

import { Cards, Card } from 'fumadocs-ui/components/card';
import { Callout } from 'fumadocs-ui/components/callout';
import { Accordion, Accordions } from 'fumadocs-ui/components/accordion';

本模块将深入探讨 AI 系统面临的隐私安全威胁。通过本模块的学习，你将了解 AI 模型如何"记住"敏感信息，以及攻击者如何利用这些特性窃取隐私数据。

## 学习目标

<Callout title="完成本模块后，你将能够：" type="success">
- 理解 AI 模型的"记忆泄露"问题
- 掌握成员推理攻击的原理和实施方法
- 了解模型逆向攻击的技术细节
- 学习差分隐私等隐私保护技术
</Callout>

## 章节概览

<Cards>
  <Card title="第1章：AI 的记忆泄露问题" href="/docs/04-privacy-attacks/memory-leakage">
    理解 AI 模型如何"记住"训练数据中的敏感信息
  </Card>
  <Card title="第2章：成员推理攻击" href="/docs/04-privacy-attacks/membership-inference">
    学习如何判断特定数据是否被用于模型训练
  </Card>
  <Card title="第3章：模型逆向攻击" href="/docs/04-privacy-attacks/model-inversion">
    探索如何从模型输出中重建训练数据特征
  </Card>
  <Card title="第4章：差分隐私基础" href="/docs/04-privacy-attacks/differential-privacy">
    掌握差分隐私等隐私保护技术的基本原理
  </Card>
</Cards>

## 配套实验

<Cards>
  <Card title="实验 4.1：数据提取" href="/docs/04-privacy-attacks/labs/data-extraction">
    实践从模型中提取训练数据的技术
  </Card>
  <Card title="实验 4.2：成员推理" href="/docs/04-privacy-attacks/labs/membership-inference">
    实现成员推理攻击并分析结果
  </Card>
  <Card title="实验 4.3：差分隐私" href="/docs/04-privacy-attacks/labs/differential-privacy">
    应用差分隐私技术保护模型隐私
  </Card>
</Cards>

<Callout title="预计学习时间" type="info">
阅读约 2-3 小时 | 实验约 2 小时
</Callout>

## 常见问题

<Accordions>
  <Accordion title="为什么 AI 模型会泄露隐私？">
    AI 模型在训练过程中会"记住"训练数据中的某些内容，特别是重复出现或具有独特模式的数据。攻击者可以通过特定的查询方式提取这些"记忆"，从而获取敏感信息。
  </Accordion>
  <Accordion title="成员推理攻击有什么实际危害？">
    成员推理攻击可以判断某人的数据是否被用于训练。例如，如果攻击者能确认某人的医疗记录被用于训练某个疾病预测模型，就能推断该人患有相关疾病。
  </Accordion>
  <Accordion title="差分隐私能完全保护隐私吗？">
    差分隐私提供的是数学上可证明的隐私保证，但需要在隐私保护和模型效用之间做权衡。当隐私预算 ε 设置很小时，隐私保护很强，但模型性能会下降。
  </Accordion>
  <Accordion title="如何防范隐私攻击？">
    主要防范措施包括：1) 训练数据脱敏处理；2) 使用差分隐私训练；3) 限制模型输出的信息量；4) 模型输出建议；5) 定期进行隐私审计。
  </Accordion>
</Accordions>
