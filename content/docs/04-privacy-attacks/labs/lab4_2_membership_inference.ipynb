{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# å®éªŒ 4.2ï¼šæˆå‘˜æ¨ç†æ”»å‡»\n\n## ğŸ¯ å­¦ä¹ ç›®æ ‡\n\nå®Œæˆæœ¬å®éªŒåï¼Œä½ å°†èƒ½å¤Ÿï¼š\n- âœ… è§£é‡Šæˆå‘˜æ¨ç†æ”»å‡»çš„åŸºæœ¬åŸç†\n- âœ… ç†è§£æ¨¡å‹ç½®ä¿¡åº¦ä¸è®­ç»ƒé›†æˆå‘˜èº«ä»½çš„å…³ç³»\n- âœ… å®ç°åŸºäºç½®ä¿¡åº¦é˜ˆå€¼çš„æˆå‘˜æ¨ç†\n- âœ… åˆ†æè¿‡æ‹Ÿåˆå¯¹æ”»å‡»æˆåŠŸç‡çš„å½±å“\n\n## ğŸ“š å‰ç½®çŸ¥è¯†\n\n- ç¥ç»ç½‘ç»œåˆ†ç±»çš„åŸºæœ¬æ¦‚å¿µ\n- æ¨¡å‹è®­ç»ƒä¸è¿‡æ‹Ÿåˆ\n- Softmax æ¦‚ç‡è¾“å‡ºçš„å«ä¹‰\n\n## ğŸ–¥ï¸ å®éªŒç¯å¢ƒ\n\n- å¹³å°ï¼šè…¾è®¯ Cloud Studio\n- GPUï¼šNVIDIA Tesla T4ï¼ˆ16GBï¼‰\n- æ¨¡å‹ï¼šè‡ªå®šä¹‰ PyTorch åˆ†ç±»ç½‘ç»œ\n\n## ğŸ“ å¡«ç©ºè¯´æ˜\n\næœ¬å®éªŒå…± **4 ä¸ªå¡«ç©º**ï¼Œéš¾åº¦ï¼šâ­â­â­â˜†â˜†\n\nâš ï¸ **å®‰å…¨æé†’**ï¼šæœ¬å®éªŒä»…ç”¨äºæ•™è‚²ç›®çš„ï¼Œæ¼”ç¤ºå¦‚ä½•é˜²æŠ¤æ¨¡å‹éšç§æ³„éœ²ã€‚è¯·å‹¿å°†æŠ€æœ¯ç”¨äºæœªæˆæƒç³»ç»Ÿã€‚"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸€éƒ¨åˆ†ï¼šæ ¸å¿ƒæ¦‚å¿µä¸ç¯å¢ƒå‡†å¤‡\n",
    "\n",
    "### ä»€ä¹ˆæ˜¯æˆå‘˜æ¨ç†æ”»å‡»ï¼Ÿ\n",
    "\n",
    "**æ ¸å¿ƒæ€æƒ³**ï¼šåˆ¤æ–­ä¸€ä¸ªæ ·æœ¬æ˜¯å¦è¢«ç”¨äºè®­ç»ƒæ¨¡å‹\n",
    "\n",
    "**ç±»æ¯”ç†è§£**ï¼š\n",
    "- ä½ çš„æ¨¡å‹åœ¨è€ƒè¯•é¢˜ä¸Šå‡†ç¡®ç‡é«˜ï¼Œåœ¨è€ƒè¯•é¢˜å¤–çš„é¢˜ä¸Šå‡†ç¡®ç‡ä½\n",
    "- æ”»å‡»è€…å¯ä»¥æ¨æ–­ï¼š\"è¿™é“é¢˜å‡†ç¡®ç‡è¿™ä¹ˆé«˜ï¼Œä½ è‚¯å®šåœ¨è®­ç»ƒæ—¶è§è¿‡å®ƒ\"\n",
    "\n",
    "**éšç§é£é™©**ï¼š\n",
    "- æ”»å‡»è€…å¯ä»¥æ¨æ–­æ•°æ®æ˜¯å¦åœ¨è®­ç»ƒé›†ä¸­\n",
    "- å¯¹äºæ•æ„Ÿæ•°æ®é›†ï¼ˆåŒ»ç–—ã€è´¢åŠ¡ï¼‰ï¼Œè¿™æ˜¯ä¸¥é‡çš„éšç§æ³„éœ²\n",
    "\n",
    "### å…³é”®è§‚å¯Ÿ\n",
    "\n",
    "åœ¨è¿‡æ‹Ÿåˆçš„æ¨¡å‹ä¸­ï¼š\n",
    "- **è®­ç»ƒæ ·æœ¬ï¼ˆæˆå‘˜ï¼‰**ï¼šæ¨¡å‹é«˜ç½®ä¿¡åº¦ã€å‡†ç¡®ç‡é«˜\n",
    "- **æµ‹è¯•æ ·æœ¬ï¼ˆéæˆå‘˜ï¼‰**ï¼šæ¨¡å‹ä½ç½®ä¿¡åº¦ã€å‡†ç¡®ç‡ä½\n",
    "\n",
    "è¿™ä¸ªå·®å¼‚å¯è¢«ç”¨äºæˆå‘˜æ¨ç†æ”»å‡»ï¼"
   ]
  },
  {
   "cell_type": "code",
   "id": "oaq7ucdv47",
   "source": "# æœ¬å®éªŒæ‰€éœ€ä¾èµ–\n%pip install torch numpy matplotlib -q\n\n# éªŒè¯å®‰è£…\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"NumPy: {np.__version__}\")\nprint(\"âœ“ ä¾èµ–å®‰è£…æˆåŠŸï¼\")\n\n# è®¾ç½®éšæœºç§å­ï¼ˆä¿è¯ç»“æœå¯å¤ç°ï¼‰\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º\nplt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']\nplt.rcParams['axes.unicode_minus'] = False\n\nprint(\"ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "fk8wn111mka",
   "source": "import torch.nn as nn\nimport torch.optim as optim\n\n# åˆ›å»ºä¸€ä¸ªç®€å•çš„åˆ†ç±»æ•°æ®é›†\ndef create_dataset(n_samples=1000, n_features=20, n_classes=5):\n    \"\"\"\n    åˆ›å»ºä¸€ä¸ªåˆæˆåˆ†ç±»æ•°æ®é›†\n    \n    å‚æ•°:\n        n_samples: æ ·æœ¬æ•°é‡\n        n_features: ç‰¹å¾ç»´åº¦\n        n_classes: ç±»åˆ«æ•°\n    \n    è¿”å›:\n        X: ç‰¹å¾çŸ©é˜µ (n_samples, n_features)\n        y: æ ‡ç­¾å‘é‡ (n_samples,)\n    \"\"\"\n    X = np.random.randn(n_samples, n_features).astype(np.float32)\n    \n    # ä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºä¸­å¿ƒç‚¹ï¼ˆä½¿æ•°æ®å¯åˆ†ç¦»ï¼‰\n    centers = np.random.randn(n_classes, n_features) * 2\n    y = np.random.randint(0, n_classes, n_samples)\n    \n    # å°†æ ·æœ¬å‘å¯¹åº”ç±»åˆ«ä¸­å¿ƒåç§»\n    for i in range(n_samples):\n        X[i] += centers[y[i]]\n    \n    return torch.tensor(X), torch.tensor(y)\n\n# åˆ›å»ºæ•°æ®é›†\nX, y = create_dataset(n_samples=1000, n_features=20, n_classes=5)\n\n# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\ntrain_size = 500\nX_train, y_train = X[:train_size], y[:train_size]\nX_test, y_test = X[train_size:], y[train_size:]\n\nprint(\"æ•°æ®é›†åˆ›å»ºå®Œæˆï¼š\")\nprint(f\"  æ€»æ ·æœ¬æ•°: {len(X)}\")\nprint(f\"  ç‰¹å¾ç»´åº¦: {X.shape[1]}\")\nprint(f\"  ç±»åˆ«æ•°: {len(torch.unique(y))}\")\nprint(f\"\\n  è®­ç»ƒé›†ï¼ˆæˆå‘˜ï¼‰: {len(X_train)} æ ·æœ¬\")\nprint(f\"  æµ‹è¯•é›†ï¼ˆéæˆå‘˜ï¼‰: {len(X_test)} æ ·æœ¬\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "92szgx2cjde",
   "source": "# å®šä¹‰ç¥ç»ç½‘ç»œåˆ†ç±»å™¨\nclass SimpleClassifier(nn.Module):\n    \"\"\"\n    ç®€å•çš„å‰é¦ˆç¥ç»ç½‘ç»œåˆ†ç±»å™¨\n    \"\"\"\n    def __init__(self, n_features=20, n_classes=5, hidden_size=64):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(n_features, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, n_classes)\n        )\n    \n    def forward(self, x):\n        return self.network(x)\n\nprint(\"æ¨¡å‹æ¶æ„ï¼š\")\nmodel_demo = SimpleClassifier()\nprint(model_demo)\nprint(\"\\nâœ“ æ¨¡å‹å®šä¹‰å®Œæˆï¼\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c6qgzg74b6a",
   "source": "## ç¬¬äºŒéƒ¨åˆ†ï¼šè®­ç»ƒç›®æ ‡æ¨¡å‹\n\nğŸ’¡ **å®éªŒè®¾è®¡**\n\næˆ‘ä»¬æ•…æ„è®©æ¨¡å‹è¿‡æ‹Ÿåˆï¼Œè¿™æ ·å¯ä»¥æ¸…æ¥šåœ°æ¼”ç¤ºæ”»å‡»æ•ˆæœï¼š\n- è®­ç»ƒé›†å‡†ç¡®ç‡é«˜ï¼ˆæ¨¡å‹\"è®°ä½\"äº†æ•°æ®ï¼‰\n- æµ‹è¯•é›†å‡†ç¡®ç‡ä½ï¼ˆæ³›åŒ–èƒ½åŠ›å·®ï¼‰\n- è¿™ä¸ªå·®å¼‚å¯è¢«ç”¨äºæˆå‘˜æ¨ç†",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "1u9cto6f2au",
   "source": "def train_model(model, X_train, y_train, epochs=50, lr=0.01):\n    \"\"\"\n    è®­ç»ƒåˆ†ç±»æ¨¡å‹\n    \n    å‚æ•°:\n        model: PyTorch æ¨¡å‹\n        X_train: è®­ç»ƒç‰¹å¾\n        y_train: è®­ç»ƒæ ‡ç­¾\n        epochs: è®­ç»ƒè½®æ•°\n        lr: å­¦ä¹ ç‡\n    \n    è¿”å›:\n        æŸå¤±å†å²\n    \"\"\"\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n    \n    losses = []\n    for epoch in range(epochs):\n        model.train()\n        optimizer.zero_grad()\n        \n        outputs = model(X_train)\n        loss = criterion(outputs, y_train)\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n        \n        if (epoch + 1) % 10 == 0:\n            with torch.no_grad():\n                acc = (outputs.argmax(dim=1) == y_train).float().mean()\n            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Acc: {acc:.2%}\")\n    \n    return losses\n\n# è®­ç»ƒä¸€ä¸ªè¿‡æ‹Ÿåˆçš„æ¨¡å‹\nprint(\"è®­ç»ƒç›®æ ‡æ¨¡å‹...\")\nprint(\"(æ•…æ„è¿‡æ‹Ÿåˆä»¥æ¼”ç¤ºæ”»å‡»æ•ˆæœ)\\n\")\nprint(\"=\" * 50)\n\nmodel = SimpleClassifier()\nlosses = train_model(model, X_train, y_train, epochs=100, lr=0.01)\n\nprint(\"=\" * 50)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "uzjb2gbc7j",
   "source": "# è¯„ä¼°æ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„è¡¨ç°\nmodel.eval()\n\nwith torch.no_grad():\n    train_outputs = model(X_train)\n    train_acc = (train_outputs.argmax(dim=1) == y_train).float().mean()\n    \n    test_outputs = model(X_test)\n    test_acc = (test_outputs.argmax(dim=1) == y_test).float().mean()\n\noverfit_gap = train_acc - test_acc\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"æ¨¡å‹æ€§èƒ½è¯„ä¼°\")\nprint(\"=\" * 50)\nprint(f\"è®­ç»ƒé›†å‡†ç¡®ç‡: {train_acc:.2%}\")\nprint(f\"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.2%}\")\nprint(f\"è¿‡æ‹Ÿåˆç¨‹åº¦: {overfit_gap:.2%}\")\n\nif overfit_gap > 0.1:\n    print(f\"\\nâš ï¸ æ¨¡å‹å­˜åœ¨æ˜æ˜¾è¿‡æ‹Ÿåˆï¼\")\n    print(f\"è¿™ä½¿å¾—æˆå‘˜æ¨ç†æ”»å‡»æ›´å®¹æ˜“æˆåŠŸã€‚\")\nelse:\n    print(f\"\\nâœ“ æ¨¡å‹æ³›åŒ–æ€§èƒ½è‰¯å¥½ï¼Œæ”»å‡»éš¾åº¦ä¼šå¢åŠ \")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "lynbhki2av",
   "source": "## ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®ç°æˆå‘˜æ¨ç†æ”»å‡»\n\nğŸ’¡ **æ”»å‡»åŸç†**\n\n1. è·å–æ¨¡å‹å¯¹æ¯ä¸ªæ ·æœ¬çš„ç½®ä¿¡åº¦ï¼ˆå¯¹çœŸå®ç±»åˆ«çš„æ¦‚ç‡ï¼‰\n2. é€‰æ‹©ä¸€ä¸ªé˜ˆå€¼ï¼ˆé€šå¸¸ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ç½®ä¿¡åº¦çš„ä¸­ä½æ•°ï¼‰\n3. ç½®ä¿¡åº¦é«˜ â†’ æ¨æ–­ä¸ºæˆå‘˜ï¼›ç½®ä¿¡åº¦ä½ â†’ æ¨æ–­ä¸ºéæˆå‘˜\n4. è®¡ç®—æ”»å‡»å‡†ç¡®ç‡\n\n**å…³é”®æ´å¯Ÿ**ï¼šè¿‡æ‹Ÿåˆå¯¼è‡´æˆå‘˜å’Œéæˆå‘˜çš„ç½®ä¿¡åº¦åˆ†å¸ƒå·®å¼‚å¤§ï¼Œä½¿æ”»å‡»æ›´å®¹æ˜“æˆåŠŸ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "# ========== å¡«ç©º 1ï¼šå®ç°ç½®ä¿¡åº¦è®¡ç®— ==========\n#\n# ğŸ¯ ä»»åŠ¡ï¼šä»æ¨¡å‹è¾“å‡ºä¸­æå–å¯¹çœŸå®ç±»åˆ«çš„ç½®ä¿¡åº¦\n#\n# ğŸ’¡ æç¤ºï¼š\n#   - softmax å°†è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ\n#   - gather(1, indices) æŒ‰ç´¢å¼•æå–ç‰¹å®šä½ç½®çš„å€¼\n#   - éœ€è¦ç”¨ y.unsqueeze(1) å°†æ ‡ç­¾å˜æˆåˆ—å‘é‡\n#\n# è¯·å°† ___________ æ›¿æ¢ä¸ºæ­£ç¡®çš„ä»£ç \n#\n# éš¾åº¦ï¼šâ­â­â˜†â˜†â˜†\n\ndef get_confidence(model, X, y):\n    \"\"\"\n    è·å–æ¨¡å‹å¯¹æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç½®ä¿¡åº¦\n    \n    å‚æ•°:\n        model: è®­ç»ƒå¥½çš„æ¨¡å‹\n        X: è¾“å…¥ç‰¹å¾\n        y: çœŸå®æ ‡ç­¾\n    \n    è¿”å›:\n        confidences: æ¯ä¸ªæ ·æœ¬çš„ç½®ä¿¡åº¦ (numpy array)\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        outputs = model(X)\n        \n        # è®¡ç®— softmax æ¦‚ç‡åˆ†å¸ƒ\n        probs = torch.softmax(outputs, dim=1)\n        \n        # æå–æ¯ä¸ªæ ·æœ¬åœ¨çœŸå®ç±»åˆ«ä¸Šçš„ç½®ä¿¡åº¦\n        confidences = ___________  # æœŸæœ›ï¼šprobs.gather(1, y.unsqueeze(1)).squeeze()\n    \n    return confidences.numpy()\n\n# è·å–è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç½®ä¿¡åº¦\ntrain_confidences = get_confidence(model, X_train, y_train)\ntest_confidences = get_confidence(model, X_test, y_test)\n\nprint(\"ç½®ä¿¡åº¦ç»Ÿè®¡ï¼š\")\nprint(\"=\" * 50)\nprint(f\"è®­ç»ƒé›†ï¼ˆæˆå‘˜ï¼‰ï¼š\")\nprint(f\"  å¹³å‡ç½®ä¿¡åº¦: {np.mean(train_confidences):.4f}\")\nprint(f\"  ä¸­ä½æ•°: {np.median(train_confidences):.4f}\")\nprint(f\"  æ ‡å‡†å·®: {np.std(train_confidences):.4f}\")\n\nprint(f\"\\næµ‹è¯•é›†ï¼ˆéæˆå‘˜ï¼‰ï¼š\")\nprint(f\"  å¹³å‡ç½®ä¿¡åº¦: {np.mean(test_confidences):.4f}\")\nprint(f\"  ä¸­ä½æ•°: {np.median(test_confidences):.4f}\")\nprint(f\"  æ ‡å‡†å·®: {np.std(test_confidences):.4f}\")\n\nprint(f\"\\nç½®ä¿¡åº¦å·®å¼‚: {np.mean(train_confidences) - np.mean(test_confidences):.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–ç½®ä¿¡åº¦åˆ†å¸ƒ\n",
    "plt.figure(figsize=(11, 5))\n",
    "\n",
    "plt.hist(train_confidences, bins=30, alpha=0.6, label='è®­ç»ƒé›†ï¼ˆæˆå‘˜ï¼‰', color='#FF6B6B', edgecolor='black')\n",
    "plt.hist(test_confidences, bins=30, alpha=0.6, label='æµ‹è¯•é›†ï¼ˆéæˆå‘˜ï¼‰', color='#4ECDC4', edgecolor='black')\n",
    "\n",
    "plt.xlabel('ç½®ä¿¡åº¦', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('æ ·æœ¬æ•°é‡', fontsize=11, fontweight='bold')\n",
    "plt.title('æˆå‘˜æ¨ç†ï¼šç½®ä¿¡åº¦åˆ†å¸ƒå¯¹æ¯”\\nï¼ˆå·¦ä¾§=ä½ç½®ä¿¡åº¦ï¼Œå³ä¾§=é«˜ç½®ä¿¡åº¦ï¼‰', fontsize=13, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ” å…³é”®è§‚å¯Ÿï¼š\")\n",
    "print(\"  æˆå‘˜æ ·æœ¬çš„ç½®ä¿¡åº¦åˆ†å¸ƒåå‘å³ä¾§ï¼ˆé«˜ç½®ä¿¡åº¦ï¼‰\")\n",
    "print(\"  éæˆå‘˜æ ·æœ¬çš„ç½®ä¿¡åº¦åˆ†å¸ƒåå‘å·¦ä¾§ï¼ˆä½ç½®ä¿¡åº¦ï¼‰\")\n",
    "print(\"  è¿™ä¸ªåˆ†å¸ƒå·®å¼‚æ˜¯æ”»å‡»çš„åŸºç¡€ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# ========== å¡«ç©º 2ï¼šå®ç°æˆå‘˜æ¨ç†å‡½æ•° ==========\n#\n# ğŸ¯ ä»»åŠ¡ï¼šæ ¹æ®é˜ˆå€¼åˆ¤æ–­æ ·æœ¬æ˜¯å¦ä¸ºæˆå‘˜\n#\n# ğŸ’¡ æç¤ºï¼š\n#   - ç½®ä¿¡åº¦ > é˜ˆå€¼ â†’ é¢„æµ‹ä¸ºæˆå‘˜ï¼ˆè¿”å› Trueï¼‰\n#   - ç½®ä¿¡åº¦ <= é˜ˆå€¼ â†’ é¢„æµ‹ä¸ºéæˆå‘˜ï¼ˆè¿”å› Falseï¼‰\n#\n# è¯·å°† ___________ æ›¿æ¢ä¸ºæ­£ç¡®çš„ä»£ç \n#\n# éš¾åº¦ï¼šâ­â˜†â˜†â˜†â˜†\n\ndef membership_inference_attack(confidences, threshold):\n    \"\"\"\n    æ‰§è¡Œæˆå‘˜æ¨ç†æ”»å‡»\n    \n    å‚æ•°:\n        confidences: æ¨¡å‹å¯¹æ ·æœ¬çš„ç½®ä¿¡åº¦\n        threshold: æˆå‘˜/éæˆå‘˜çš„åˆ†ç•Œé˜ˆå€¼\n    \n    è¿”å›:\n        predictions: å¸ƒå°”æ•°ç»„ï¼ŒTrue=æˆå‘˜ï¼ŒFalse=éæˆå‘˜\n    \"\"\"\n    # æ ¹æ®é˜ˆå€¼åˆ¤æ–­æˆå‘˜èº«ä»½\n    predictions = ___________  # æœŸæœ›ï¼šconfidences > threshold\n    \n    return predictions\n\n# é€‰æ‹©é˜ˆå€¼ï¼ˆä½¿ç”¨ä¸¤ä¸ªåˆ†å¸ƒä¸­ä½æ•°çš„å‡å€¼ï¼‰\nthreshold = (np.median(train_confidences) + np.median(test_confidences)) / 2\n\nprint(f\"é€‰æ‹©çš„æ”»å‡»é˜ˆå€¼: {threshold:.4f}\")\nprint(f\"  è®­ç»ƒé›†ä¸­ä½æ•°: {np.median(train_confidences):.4f}\")\nprint(f\"  æµ‹è¯•é›†ä¸­ä½æ•°: {np.median(test_confidences):.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# ========== å¡«ç©º 3ï¼šè®¡ç®—æ”»å‡»å‡†ç¡®ç‡ ==========\n#\n# ğŸ¯ ä»»åŠ¡ï¼šè®¡ç®—æˆå‘˜çš„è¯†åˆ«å‡†ç¡®ç‡\n#\n# ğŸ’¡ æç¤ºï¼š\n#   - æˆå‘˜åº”è¯¥è¢«é¢„æµ‹ä¸º Trueï¼ˆé«˜ç½®ä¿¡åº¦ï¼‰\n#   - ä½¿ç”¨ np.mean() è®¡ç®—æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹\n#   - train_pred == True è¿”å›å¸ƒå°”æ•°ç»„\n#\n# è¯·å°† ___________ æ›¿æ¢ä¸ºæ­£ç¡®çš„ä»£ç \n#\n# éš¾åº¦ï¼šâ­â­â˜†â˜†â˜†\n\n# å¯¹è®­ç»ƒé›†ï¼ˆæˆå‘˜ï¼‰å’Œæµ‹è¯•é›†ï¼ˆéæˆå‘˜ï¼‰æ‰§è¡Œæ”»å‡»\ntrain_pred = membership_inference_attack(train_confidences, threshold)\ntest_pred = membership_inference_attack(test_confidences, threshold)\n\n# è®¡ç®—æ”»å‡»å‡†ç¡®ç‡\n# å¯¹æˆå‘˜ï¼šé¢„æµ‹ä¸º True çš„å‡†ç¡®ç‡\n# å¯¹éæˆå‘˜ï¼šé¢„æµ‹ä¸º False çš„å‡†ç¡®ç‡\nmember_acc = ___________    # æœŸæœ›ï¼šnp.mean(train_pred == True)\nnon_member_acc = np.mean(test_pred == False)\n\noverall_acc = (member_acc + non_member_acc) / 2\n\nprint(\"\\næˆå‘˜æ¨ç†æ”»å‡»ç»“æœï¼š\")\nprint(\"=\" * 50)\nprint(f\"æˆå‘˜è¯†åˆ«ç‡ï¼ˆçœŸé˜³ç‡ï¼‰: {member_acc:.2%}\")\nprint(f\"éæˆå‘˜è¯†åˆ«ç‡ï¼ˆçœŸé˜´ç‡ï¼‰: {non_member_acc:.2%}\")\nprint(f\"æ€»ä½“å‡†ç¡®ç‡: {overall_acc:.2%}\")\n\nif overall_acc > 0.6:\n    print(f\"\\nâš ï¸ æ”»å‡»æˆåŠŸï¼å‡†ç¡®ç‡æ˜¾è‘—é«˜äºéšæœºçŒœæµ‹(50%)\")\n    print(f\"å·®å€¼: +{overall_acc - 0.5:.1%}\")\nelse:\n    print(f\"\\nâœ“ æ”»å‡»æ•ˆæœæœ‰é™\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## ç¬¬å››éƒ¨åˆ†ï¼šé˜ˆå€¼å¯¹æ”»å‡»æ•ˆæœçš„å½±å“\n",
    "\n",
    "### é—®é¢˜ï¼šæœ€ä¼˜é˜ˆå€¼æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "ä¸åŒé˜ˆå€¼ä¼šå½±å“ï¼š\n",
    "- **çœŸé˜³ç‡ï¼ˆTPRï¼‰**ï¼šæˆå‘˜è¢«æ­£ç¡®è¯†åˆ«çš„æ¯”ä¾‹\n",
    "- **å‡é˜³ç‡ï¼ˆFPRï¼‰**ï¼šéæˆå‘˜è¢«é”™è¯¯è¯†åˆ«ä¸ºæˆå‘˜çš„æ¯”ä¾‹\n",
    "- **æ€»ä½“å‡†ç¡®ç‡**ï¼šå¹³è¡¡çš„åˆ†ç±»æ€§èƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ä¸åŒé˜ˆå€¼çš„æ”»å‡»æ•ˆæœ\n",
    "thresholds = np.linspace(0.1, 0.95, 20)\n",
    "results = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    train_pred = membership_inference_attack(train_confidences, thresh)\n",
    "    test_pred = membership_inference_attack(test_confidences, thresh)\n",
    "    \n",
    "    tpr = np.mean(train_pred == True)      # çœŸé˜³ç‡ï¼šæˆå‘˜è¯†åˆ«ç‡\n",
    "    fpr = np.mean(test_pred == True)       # å‡é˜³ç‡ï¼šéæˆå‘˜è¯¯åˆ¤ç‡\n",
    "    acc = (tpr + (1 - fpr)) / 2            # å¹³è¡¡å‡†ç¡®ç‡\n",
    "    \n",
    "    results.append((thresh, tpr, fpr, acc))\n",
    "\n",
    "# æå–æ•°æ®ç”¨äºå¯è§†åŒ–\n",
    "thresholds_list = [r[0] for r in results]\n",
    "tpr_list = [r[1] for r in results]\n",
    "fpr_list = [r[2] for r in results]\n",
    "acc_list = [r[3] for r in results]\n",
    "\n",
    "# æ‰¾åˆ°æœ€ä½³é˜ˆå€¼\n",
    "best_idx = np.argmax(acc_list)\n",
    "best_threshold = thresholds_list[best_idx]\n",
    "best_acc = acc_list[best_idx]\n",
    "\n",
    "print(f\"æœ€ä½³é˜ˆå€¼åˆ†æï¼š\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"æœ€ä½³é˜ˆå€¼: {best_threshold:.3f}\")\n",
    "print(f\"æœ€é«˜å‡†ç¡®ç‡: {best_acc:.2%}\")\n",
    "print(f\"å¯¹åº”çš„çœŸé˜³ç‡: {tpr_list[best_idx]:.2%}\")\n",
    "print(f\"å¯¹åº”çš„å‡é˜³ç‡: {fpr_list[best_idx]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–é˜ˆå€¼å½±å“\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# å·¦å›¾ï¼šTPR vs FPR\n",
    "ax1.plot(thresholds_list, tpr_list, 'o-', color='#FF6B6B', linewidth=2, markersize=5, label='çœŸé˜³ç‡ï¼ˆæˆå‘˜è¯†åˆ«ï¼‰')\n",
    "ax1.plot(thresholds_list, fpr_list, 's-', color='#4ECDC4', linewidth=2, markersize=5, label='å‡é˜³ç‡ï¼ˆéæˆå‘˜è¯¯åˆ¤ï¼‰')\n",
    "ax1.axvline(x=best_threshold, color='gray', linestyle='--', alpha=0.5, label='æœ€ä½³é˜ˆå€¼')\n",
    "ax1.set_xlabel('é˜ˆå€¼', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('æ¯”ç‡', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('é˜ˆå€¼ vs çœŸé˜³ç‡/å‡é˜³ç‡', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# å³å›¾ï¼šå‡†ç¡®ç‡\n",
    "ax2.plot(thresholds_list, acc_list, 'D-', color='#95E1D3', linewidth=2.5, markersize=6)\n",
    "ax2.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='éšæœºçŒœæµ‹(50%)')\n",
    "ax2.axvline(x=best_threshold, color='#FF6B6B', linestyle='--', alpha=0.7, label=f'æœ€ä½³é˜ˆå€¼({best_threshold:.2f})')\n",
    "ax2.scatter([best_threshold], [best_acc], color='#FF6B6B', s=200, zorder=5, marker='*')\n",
    "ax2.set_xlabel('é˜ˆå€¼', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('å‡†ç¡®ç‡', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('é˜ˆå€¼ vs æ”»å‡»å‡†ç¡®ç‡', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([0.4, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ’¡ è§‚å¯Ÿï¼š\")\n",
    "print(f\"  é˜ˆå€¼å¤ªä½ â†’ å¤§å¤šæ•°è¢«é¢„æµ‹ä¸ºæˆå‘˜ â†’ é«˜å‡é˜³ç‡\")\n",
    "print(f\"  é˜ˆå€¼å¤ªé«˜ â†’ å¤§å¤šæ•°è¢«é¢„æµ‹ä¸ºéæˆå‘˜ â†’ ä½çœŸé˜³ç‡\")\n",
    "print(f\"  æœ€ä½³é˜ˆå€¼å¹³è¡¡ä¸¤è€…ï¼Œå®ç°æœ€é«˜å‡†ç¡®ç‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## ç¬¬äº”éƒ¨åˆ†ï¼šè¿‡æ‹Ÿåˆç¨‹åº¦å¯¹æ”»å‡»çš„å½±å“\n",
    "\n",
    "### æ ¸å¿ƒé—®é¢˜ï¼šè¿‡æ‹Ÿåˆä¸ºä»€ä¹ˆä¼šå¯¼è‡´éšç§æ³„éœ²ï¼Ÿ\n",
    "\n",
    "**ç›´è§‰ç†è§£**ï¼š\n",
    "- æ¨¡å‹\"è®°ä½\"äº†è®­ç»ƒæ•°æ® = è¿‡æ‹Ÿåˆ\n",
    "- å¯¹è®­ç»ƒæ•°æ®çš„ç½®ä¿¡åº¦é«˜\n",
    "- å¯¹æµ‹è¯•æ•°æ®çš„ç½®ä¿¡åº¦ä½\n",
    "- è¿™ä¸ªå·®å¼‚èƒ½è¢«ç”¨äºæ¨æ–­æˆå‘˜èº«ä»½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# ========== å¡«ç©º 4ï¼šåˆ†æè¿‡æ‹Ÿåˆå¯¹æ”»å‡»çš„å½±å“ ==========\n#\n# ğŸ¯ ä»»åŠ¡ï¼šè§‚å¯Ÿå¹¶åˆ†æ\"è¿‡æ‹Ÿåˆç¨‹åº¦è¶Šé«˜ = æ”»å‡»æˆåŠŸç‡è¶Šé«˜\"çš„è§„å¾‹\n#\n# ğŸ’¡ æç¤ºï¼š\n#   - è®­ç»ƒä¸åŒè½®æ•°çš„æ¨¡å‹ä¼šæœ‰ä¸åŒçš„è¿‡æ‹Ÿåˆç¨‹åº¦\n#   - å¯¹æ¯”è¿‡æ‹Ÿåˆç¨‹åº¦ï¼ˆtrain_acc - test_accï¼‰å’Œæ”»å‡»å‡†ç¡®ç‡\n#   - è¿™æ˜¯ä¸€ä¸ªå®éªŒè§‚å¯Ÿä»»åŠ¡ï¼Œè¿è¡Œä»£ç å¹¶åˆ†æç»“æœ\n#\n# éš¾åº¦ï¼šâ­â­â˜†â˜†â˜†ï¼ˆè§‚å¯Ÿåˆ†æä¸ºä¸»ï¼‰\n\n# æµ‹è¯•ä¸åŒè®­ç»ƒç¨‹åº¦å¯¹æ”»å‡»çš„å½±å“\nepoch_values = [10, 30, 50, 100, 150]\nattack_results = []\n\nprint(\"æµ‹è¯•ä¸åŒè®­ç»ƒç¨‹åº¦å¯¹æ”»å‡»æˆåŠŸç‡çš„å½±å“...\\n\")\nprint(\"=\" * 60)\nprint(f\"{'è½®æ•°':<6} {'è®­ç»ƒå‡†ç¡®ç‡':<10} {'æµ‹è¯•å‡†ç¡®ç‡':<10} {'è¿‡æ‹Ÿåˆ':<8} {'æ”»å‡»å‡†ç¡®ç‡':<10}\")\nprint(\"=\" * 60)\n\nfor epochs in epoch_values:\n    # è®­ç»ƒæ–°æ¨¡å‹\n    temp_model = SimpleClassifier()\n    train_model(temp_model, X_train, y_train, epochs=epochs, lr=0.01)\n    \n    # è¯„ä¼°è¿‡æ‹Ÿåˆç¨‹åº¦\n    temp_model.eval()\n    with torch.no_grad():\n        train_acc = (temp_model(X_train).argmax(dim=1) == y_train).float().mean().item()\n        test_acc = (temp_model(X_test).argmax(dim=1) == y_test).float().mean().item()\n    \n    # æ‰§è¡Œæ”»å‡»\n    train_conf = get_confidence(temp_model, X_train, y_train)\n    test_conf = get_confidence(temp_model, X_test, y_test)\n    best_thresh = (np.median(train_conf) + np.median(test_conf)) / 2\n    \n    train_pred = membership_inference_attack(train_conf, best_thresh)\n    test_pred = membership_inference_attack(test_conf, best_thresh)\n    attack_acc = (np.mean(train_pred == True) + np.mean(test_pred == False)) / 2\n    \n    overfit = train_acc - test_acc\n    attack_results.append({\n        'epochs': epochs,\n        'train_acc': train_acc,\n        'test_acc': test_acc,\n        'overfit': overfit,\n        'attack_acc': attack_acc\n    })\n    \n    print(f\"{epochs:<6} {train_acc:<10.1%} {test_acc:<10.1%} {overfit:<8.1%} {attack_acc:<10.1%}\")\n\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–è¿‡æ‹Ÿåˆä¸æ”»å‡»çš„å…³ç³»\n",
    "overfits = [r['overfit'] for r in attack_results]\n",
    "attack_accs = [r['attack_acc'] for r in attack_results]\n",
    "epochs_list = [r['epochs'] for r in attack_results]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# æ•£ç‚¹å›¾\n",
    "scatter = ax.scatter(overfits, attack_accs, s=200, c=epochs_list, cmap='viridis', \n",
    "                     edgecolors='black', alpha=0.8, linewidth=2)\n",
    "\n",
    "# æ ‡æ³¨\n",
    "for i, r in enumerate(attack_results):\n",
    "    ax.annotate(f\"{r['epochs']}e\", \n",
    "               (overfits[i], attack_accs[i]),\n",
    "               textcoords=\"offset points\",\n",
    "               xytext=(8, 8),\n",
    "               fontsize=10,\n",
    "               fontweight='bold')\n",
    "\n",
    "# å‚è€ƒçº¿\n",
    "ax.axhline(y=0.5, color='red', linestyle='--', linewidth=2, alpha=0.5, label='éšæœºçŒœæµ‹(50%)')\n",
    "\n",
    "# æ ‡ç­¾å’Œæ ‡é¢˜\n",
    "ax.set_xlabel('è¿‡æ‹Ÿåˆç¨‹åº¦ï¼ˆè®­ç»ƒå‡†ç¡®ç‡ - æµ‹è¯•å‡†ç¡®ç‡ï¼‰', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('æˆå‘˜æ¨ç†æ”»å‡»å‡†ç¡®ç‡', fontsize=12, fontweight='bold')\n",
    "ax.set_title('è¿‡æ‹Ÿåˆç¨‹åº¦ vs æˆå‘˜æ¨ç†æ”»å‡»æˆåŠŸç‡\\nï¼ˆæ›´å¤šè¿‡æ‹Ÿåˆ = æ”»å‡»æ›´å®¹æ˜“æˆåŠŸï¼‰', \n",
    "            fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# é¢œè‰²æ¡\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('è®­ç»ƒè½®æ•°', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š å…³é”®å‘ç°ï¼š\")\n",
    "print(f\"  è¿‡æ‹Ÿåˆç¨‹åº¦æœ€å°: {min(overfits):.1%} â†’ æ”»å‡»å‡†ç¡®ç‡: {attack_accs[overfits.index(min(overfits))]:.1%}\")\n",
    "print(f\"  è¿‡æ‹Ÿåˆç¨‹åº¦æœ€å¤§: {max(overfits):.1%} â†’ æ”»å‡»å‡†ç¡®ç‡: {attack_accs[overfits.index(max(overfits))]:.1%}\")\n",
    "print(f\"  ç›¸å…³æ€§: æ­£ç›¸å…³ âœ“\")\n",
    "print(f\"\\nğŸ’¡ ç»“è®ºï¼š\")\n",
    "print(f\"  è¿‡æ‹Ÿåˆæ˜¯éšç§æ³„éœ²çš„æ ¹æœ¬åŸå› ï¼\")\n",
    "print(f\"  é˜²æ­¢è¿‡æ‹Ÿåˆå¯ä»¥æœ‰æ•ˆé™ä½æˆå‘˜æ¨ç†æ”»å‡»æˆåŠŸç‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "### ğŸ¤” æ€è€ƒä¸è®¨è®º\n",
    "\n",
    "1. **è§‚å¯Ÿ**ï¼šæˆå‘˜å’Œéæˆå‘˜çš„ç½®ä¿¡åº¦åˆ†å¸ƒæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿä¸ºä»€ä¹ˆä¼šæœ‰è¿™ç§åŒºåˆ«ï¼Ÿ\n",
    "\n",
    "2. **åˆ†æ**ï¼šè¿‡æ‹Ÿåˆå¦‚ä½•å½±å“æ”»å‡»æˆåŠŸç‡ï¼Ÿä»æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®çš„\"è®°å¿†\"è§’åº¦è§£é‡Šã€‚\n",
    "\n",
    "3. **åº”ç”¨**ï¼šå¦‚ä½•é˜²å¾¡æˆå‘˜æ¨ç†æ”»å‡»ï¼ŸåŸºäºå®éªŒè§‚å¯Ÿï¼Œæå‡ºå¯èƒ½çš„é˜²å¾¡æ€è·¯ã€‚\n",
    "   - å‡å°‘è¿‡æ‹Ÿåˆï¼ˆæ›´å¼ºçš„æ­£åˆ™åŒ–ï¼‰\n",
    "   - æ·»åŠ å™ªå£°ï¼ˆå·®åˆ†éšç§ï¼‰\n",
    "   - é™åˆ¶è¾“å‡ºä¿¡æ¯ï¼ˆéšè—ç½®ä¿¡åº¦ï¼‰\n",
    "\n",
    "4. **æ‰©å±•**ï¼šé™¤äº†ç½®ä¿¡åº¦ï¼Œè¿˜æœ‰å…¶ä»–æŒ‡æ ‡å¯ä»¥ç”¨äºæˆå‘˜æ¨ç†å—ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": "## ğŸ“‹ å®éªŒå°ç»“\n\n### æ ¸å¿ƒæ”¶è·\n\n1. **æ¦‚å¿µç†è§£**ï¼š\n   - æˆå‘˜æ¨ç†æ”»å‡»å¯ä»¥åˆ¤æ–­æ ·æœ¬æ˜¯å¦åœ¨è®­ç»ƒé›†ä¸­\n   - è¿‡æ‹Ÿåˆå¯¼è‡´æˆå‘˜å’Œéæˆå‘˜çš„ç½®ä¿¡åº¦å·®å¼‚æ˜æ˜¾\n   - è¿™ä¸ªå·®å¼‚å¯è¢«åˆ©ç”¨è¿›è¡Œéšç§æ”»å‡»\n\n2. **æŠ€èƒ½æŒæ¡**ï¼š\n   - èƒ½ä»æ¨¡å‹è¾“å‡ºä¸­æå–ç½®ä¿¡åº¦\n   - èƒ½é€‰æ‹©åˆé€‚çš„é˜ˆå€¼è¿›è¡Œåˆ†ç±»\n   - èƒ½è¯„ä¼°æ”»å‡»çš„æˆåŠŸç‡\n\n3. **å®‰å…¨è®¤è¯†**ï¼š\n   - æ¨¡å‹éšç§æ³„éœ²æ˜¯çœŸå®çš„å®‰å…¨å¨èƒ\n   - è¿‡æ‹Ÿåˆæ—¢é™ä½æ³›åŒ–æ€§èƒ½ï¼Œåˆå¢åŠ éšç§é£é™©\n   - éœ€è¦ä»æ¨¡å‹è®¾è®¡å’Œè®­ç»ƒæ–¹æ³•å¤šæ–¹é¢é˜²æŠ¤\n\n### å…³é”®ä»£ç å›é¡¾\n\n```python\n# æå–ç½®ä¿¡åº¦ï¼ˆéšç§æ³„éœ²æŒ‡æ ‡ï¼‰\nconfidences = probs.gather(1, y.unsqueeze(1)).squeeze()\n\n# æ ¹æ®é˜ˆå€¼æ¨ç†æˆå‘˜èº«ä»½\npredictions = confidences > threshold\n\n# è®¡ç®—æ”»å‡»æˆåŠŸç‡\nattack_acc = (member_acc + non_member_acc) / 2\n```\n\n### é˜²å¾¡æªæ–½æ€»ç»“\n\n| é˜²å¾¡æ–¹æ³• | åŸç† | ä¼˜ç‚¹ | ç¼ºç‚¹ |\n|---------|------|------|------|\n| æ­£åˆ™åŒ– | å‡å°‘è¿‡æ‹Ÿåˆ | ç®€å•æ˜“è¡Œ | å¯èƒ½å½±å“æ¨¡å‹å‡†ç¡®ç‡ |\n| å·®åˆ†éšç§ | è®­ç»ƒæ—¶æ·»åŠ å™ªå£° | æä¾›éšç§ä¿è¯ | æ€§èƒ½ä¸‹é™ï¼Œå¤æ‚åº¦é«˜ |\n| è¾“å‡ºè¿‡æ»¤ | éšè—/ä¿®æ”¹ç½®ä¿¡åº¦ | ä¸å½±å“æ¨¡å‹ | å®¹æ˜“è¢«ç»•è¿‡ |\n| æˆå‘˜é¢„æµ‹æ£€æµ‹ | è¯†åˆ«æ”»å‡»è¡Œä¸º | å®æ—¶é˜²æŠ¤ | æ£€æµ‹ç‡ä¸ä¸€å®šé«˜ |\n\n### å»¶ä¼¸é˜…è¯»\n\n- [è®ºæ–‡] Shokri et al. \"Membership Inference Attacks Against Machine Learning Models\" (S&P 2017)\n- [é˜²å¾¡æ–¹æ³•] å®éªŒ 4.3ï¼šå·®åˆ†éšç§è®­ç»ƒ\n- [è¿›é˜¶è¯é¢˜] æ¨¡å‹åæ¼”æ”»å‡»ã€å±æ€§æ¨ç†æ”»å‡»"
  },
  {
   "cell_type": "markdown",
   "id": "jdrrlg8xsbj",
   "source": "## ğŸ“– å‚è€ƒç­”æ¡ˆ\n\n<details>\n<summary>ç‚¹å‡»æŸ¥çœ‹å‚è€ƒç­”æ¡ˆ</summary>\n\n**å¡«ç©º 1**ï¼šæå–ç½®ä¿¡åº¦\n```python\nconfidences = probs.gather(1, y.unsqueeze(1)).squeeze()\n```\n\n**å¡«ç©º 2**ï¼šæ ¹æ®é˜ˆå€¼æ¨ç†\n```python\npredictions = confidences > threshold\n```\n\n**å¡«ç©º 3**ï¼šè®¡ç®—å‡†ç¡®ç‡\n```python\nmember_acc = np.mean(train_pred == True)\n```\n\n**å¡«ç©º 4**ï¼šåˆ†æè¿‡æ‹Ÿåˆå½±å“ï¼ˆè§‚å¯Ÿä»»åŠ¡ï¼Œæ— éœ€å¡«ç©ºï¼‰\n- è¿è¡Œå¹¶è§‚å¯Ÿè¿‡æ‹Ÿåˆç¨‹åº¦ä¸æ”»å‡»å‡†ç¡®ç‡çš„å…³ç³»\n- é¢„æœŸï¼šæ­£ç›¸å…³å…³ç³»\n\n</details>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "axfc9zzt595",
   "source": "âš ï¸ **é‡è¦æé†’**\n\nå®éªŒå®Œæˆåï¼Œè¯·è®°å¾—ï¼š\n\n1. **é‡Šæ”¾ GPU èµ„æº**ï¼šå¦‚æœä½ ä½¿ç”¨çš„æ˜¯ Tencent CloudStudioï¼Œè¯·ç‚¹å‡»å³ä¸Šè§’çš„\"åœæ­¢\"æŒ‰é’®ï¼Œåœæ­¢è¿è¡Œã€‚\n2. **ä¿å­˜å®éªŒç»“æœ**ï¼šå¦‚éœ€ä¿å­˜ï¼Œè¯·ä¸‹è½½ notebook æ–‡ä»¶\n\nè¿™æ ·å¯ä»¥é¿å…ä¸å¿…è¦çš„èµ„æºå ç”¨å’Œè´¹ç”¨äº§ç”Ÿã€‚**å¦åˆ™ä¼šä¸€ç›´æ¶ˆè€—ä½ çš„å…è´¹èµ„æºé¢åº¦ã€‚**",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}