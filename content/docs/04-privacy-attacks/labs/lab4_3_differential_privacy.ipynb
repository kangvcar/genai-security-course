{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# å®éªŒ 4.3ï¼šå·®åˆ†éšç§é˜²å¾¡å®æˆ˜\n\n## å­¦ä¹ ç›®æ ‡\nå®Œæˆæœ¬å®éªŒåï¼Œä½ å°†èƒ½å¤Ÿï¼š\n- è§£é‡Šå·®åˆ†éšç§ï¼ˆDPï¼‰çš„æ ¸å¿ƒæœºåˆ¶ï¼šæ¢¯åº¦è£å‰ªä¸å™ªå£°æ·»åŠ \n- å®ç°ç®€åŒ–ç‰ˆçš„å·®åˆ†éšç§ SGDï¼ˆDP-SGDï¼‰è®­ç»ƒæµç¨‹\n- å¯¹æ¯”æ™®é€šè®­ç»ƒä¸ DP è®­ç»ƒåœ¨æ¨¡å‹æ€§èƒ½ä¸Šçš„å·®å¼‚\n- è¯„ä¼°å·®åˆ†éšç§å¯¹æˆå‘˜æ¨ç†æ”»å‡»çš„é˜²å¾¡æ•ˆæœ\n- åˆ†æéšç§é¢„ç®—ï¼ˆÎµï¼‰ä¸æ¨¡å‹æ•ˆç”¨ä¹‹é—´çš„æƒè¡¡å…³ç³»\n\n## å‰ç½®çŸ¥è¯†\n- ç†è§£æ¢¯åº¦ä¸‹é™å’Œåå‘ä¼ æ’­çš„åŸºæœ¬åŸç†\n- äº†è§£æˆå‘˜æ¨ç†æ”»å‡»çš„å¨èƒæ¨¡å‹ï¼ˆå®Œæˆå®éªŒ 4.2ï¼‰\n- ç†Ÿæ‚‰ PyTorch åŸºç¡€æ“ä½œï¼ˆå¼ é‡ã€æ¨¡å‹è®­ç»ƒï¼‰\n\n## å®éªŒç¯å¢ƒ\n- å¹³å°ï¼šæœ¬åœ°ç¯å¢ƒæˆ–è…¾è®¯ Cloud Studio\n- GPUï¼šå¯é€‰ï¼ˆCPU å³å¯å®Œæˆï¼‰\n- æ¨¡å‹ï¼šè‡ªå®šä¹‰ PyTorch åˆ†ç±»å™¨\n- æ•°æ®é›†ï¼šåˆæˆå¤šåˆ†ç±»æ•°æ®é›†\n\n## å¡«ç©ºè¯´æ˜\næœ¬å®éªŒå…± **5 ä¸ªå¡«ç©º**ï¼Œéš¾åº¦ï¼šâ­â­â­â˜†â˜†\n- å¡«ç©º 1-2ï¼šå®ç°å·®åˆ†éšç§æ ¸å¿ƒæœºåˆ¶ï¼ˆæ¢¯åº¦è£å‰ªã€å™ªå£°æ·»åŠ ï¼‰\n- å¡«ç©º 3-4ï¼šæ¨¡å‹è¯„ä¼°ä¸é˜²å¾¡æ•ˆæœåˆ†æ\n- å¡«ç©º 5ï¼šéšç§-æ•ˆç”¨æƒè¡¡å®éªŒ\n\nâš ï¸ **é‡è¦æç¤º**ï¼šå·®åˆ†éšç§æ˜¯ç†è®ºä¸¥è°¨çš„éšç§ä¿æŠ¤æŠ€æœ¯ï¼Œæœ¬å®éªŒä¸ºç®€åŒ–æ•™å­¦ç‰ˆæœ¬ï¼Œå®é™…åº”ç”¨è¯·ä½¿ç”¨ä¸“ä¸šåº“ï¼ˆå¦‚ Opacusï¼‰ã€‚"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ç¬¬ä¸€éƒ¨åˆ†ï¼šç¯å¢ƒå‡†å¤‡\n\n### 1.1 å®‰è£…ä¾èµ–"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…å¿…è¦çš„åº“\n",
    "!pip install torch numpy matplotlib scikit-learn -q\n",
    "\n",
    "print(\"âœ“ ä¾èµ–å®‰è£…å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”§ ç¯å¢ƒé…ç½®\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"  è®¾å¤‡: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(\"  âœ“ ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.2 åˆ›å»ºå®éªŒæ•°æ®é›†"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆåˆæˆåˆ†ç±»æ•°æ®é›†\n",
    "def create_dataset(n_samples=1000, n_features=20, n_classes=5, test_size=0.5, random_state=42):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆç”¨äºè®­ç»ƒå’Œæµ‹è¯•çš„åˆ†ç±»æ•°æ®é›†\n",
    "    \n",
    "    å‚æ•°:\n",
    "        n_samples: æ€»æ ·æœ¬æ•°\n",
    "        n_features: ç‰¹å¾ç»´åº¦\n",
    "        n_classes: ç±»åˆ«æ•°\n",
    "        test_size: æµ‹è¯•é›†æ¯”ä¾‹\n",
    "    \n",
    "    è¿”å›:\n",
    "        X_train, y_train, X_test, y_test (å¼ é‡æ ¼å¼)\n",
    "    \"\"\"\n",
    "    # ç”Ÿæˆå¤šåˆ†ç±»æ•°æ®\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_informative=15,\n",
    "        n_redundant=5,\n",
    "        n_classes=n_classes,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # è½¬æ¢ä¸º PyTorch å¼ é‡\n",
    "    return (\n",
    "        torch.FloatTensor(X_train),\n",
    "        torch.LongTensor(y_train),\n",
    "        torch.FloatTensor(X_test),\n",
    "        torch.LongTensor(y_test)\n",
    "    )\n",
    "\n",
    "# åˆ›å»ºæ•°æ®é›†\n",
    "X_train, y_train, X_test, y_test = create_dataset()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š æ•°æ®é›†ä¿¡æ¯\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬\")\n",
    "print(f\"  æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬\")\n",
    "print(f\"  ç‰¹å¾ç»´åº¦: {X_train.shape[1]}\")\n",
    "print(f\"  ç±»åˆ«æ•°: {len(torch.unique(y_train))}\")\n",
    "print(\"  âœ“ æ•°æ®é›†åˆ›å»ºå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.3 å®šä¹‰æ¨¡å‹æ¶æ„"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰ç®€å•çš„å‰é¦ˆç¥ç»ç½‘ç»œåˆ†ç±»å™¨\n",
    "class SimpleClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    ç®€å•çš„ä¸‰å±‚å…¨è¿æ¥ç¥ç»ç½‘ç»œ\n",
    "    \n",
    "    ç»“æ„: è¾“å…¥å±‚ -> éšè—å±‚1(64) -> éšè—å±‚2(64) -> è¾“å‡ºå±‚\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features=20, n_classes=5, hidden_size=64):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_features, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size, n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# æµ‹è¯•æ¨¡å‹\n",
    "test_model = SimpleClassifier()\n",
    "test_input = torch.randn(4, 20)\n",
    "test_output = test_model(test_input)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ§  æ¨¡å‹æ¶æ„\")\n",
    "print(\"=\"*60)\n",
    "print(test_model)\n",
    "print(f\"\\n  æµ‹è¯•è¾“å…¥å½¢çŠ¶: {test_input.shape}\")\n",
    "print(f\"  æµ‹è¯•è¾“å‡ºå½¢çŠ¶: {test_output.shape}\")\n",
    "print(\"  âœ“ æ¨¡å‹å®šä¹‰å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "è¿è¡Œåˆ°è¿™é‡Œï¼Œä½ åº”è¯¥çœ‹åˆ°ï¼š\n- ç¯å¢ƒé…ç½®æˆåŠŸï¼Œæ˜¾ç¤º PyTorch ç‰ˆæœ¬\n- æ•°æ®é›†åŒ…å« 500 ä¸ªè®­ç»ƒæ ·æœ¬å’Œ 500 ä¸ªæµ‹è¯•æ ·æœ¬\n- æ¨¡å‹æ¶æ„æ¸…æ™°æ˜¾ç¤ºï¼ŒåŒ…å« 3 å±‚å…¨è¿æ¥ç½‘ç»œ\n\nå¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ˜¯å¦æˆåŠŸå®‰è£…äº† PyTorch å’Œç›¸å…³ä¾èµ–ã€‚"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ç¬¬äºŒéƒ¨åˆ†ï¼šå·®åˆ†éšç§æ ¸å¿ƒæœºåˆ¶\n\nğŸ’¡ **ä»€ä¹ˆæ˜¯å·®åˆ†éšç§ï¼Ÿ**\n\n**å·®åˆ†éšç§ï¼ˆDifferential Privacy, DPï¼‰** æ˜¯ä¸€ç§ä¸¥æ ¼çš„éšç§ä¿æŠ¤æ ‡å‡†ï¼Œå®ƒç¡®ä¿ï¼šå³ä½¿æ”»å‡»è€…æ‹¥æœ‰é™¤ä¸€æ¡è®°å½•å¤–çš„æ‰€æœ‰æ•°æ®ï¼Œä¹Ÿæ— æ³•ç¡®å®šè¯¥è®°å½•æ˜¯å¦åœ¨è®­ç»ƒé›†ä¸­ã€‚\n\n### æ ¸å¿ƒæ€æƒ³\n\né€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ·»åŠ **ç²¾å¿ƒè®¾è®¡çš„å™ªå£°**ï¼Œä½¿å¾—å•ä¸ªæ ·æœ¬çš„å­˜åœ¨å¯¹æ¨¡å‹è¾“å‡ºçš„å½±å“å¯ä»¥è¢«å¿½ç•¥ã€‚\n\n### DP-SGD çš„ä¸¤ä¸ªå…³é”®æ­¥éª¤\n\n```\næ™®é€šè®­ç»ƒ:  æ ·æœ¬ â†’ è®¡ç®—æ¢¯åº¦ â†’ ç´¯åŠ  â†’ æ›´æ–°å‚æ•°\n                      â†“\nDPè®­ç»ƒ:    æ ·æœ¬ â†’ è®¡ç®—æ¢¯åº¦ â†’ è£å‰ª â†’ æ·»åŠ å™ªå£° â†’ æ›´æ–°å‚æ•°\n                         [æ§åˆ¶å½±å“] [ä¿æŠ¤éšç§]\n```\n\n1. **æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰**ï¼šé™åˆ¶æ¯ä¸ªæ ·æœ¬çš„æ¢¯åº¦èŒƒæ•°ä¸è¶…è¿‡é˜ˆå€¼ C\n   - ä½œç”¨ï¼šé˜²æ­¢æŸä¸ªæ ·æœ¬çš„æ¢¯åº¦è¿‡å¤§ï¼Œä¸»å¯¼æ¨¡å‹æ›´æ–°\n   - å…¬å¼ï¼š`g' = g * min(1, C / ||g||)`\n\n2. **å™ªå£°æ·»åŠ ï¼ˆNoise Additionï¼‰**ï¼šåœ¨èšåˆæ¢¯åº¦ä¸Šæ·»åŠ é«˜æ–¯å™ªå£°\n   - ä½œç”¨ï¼šéšè—å•ä¸ªæ ·æœ¬å¯¹æ¢¯åº¦çš„å…·ä½“è´¡çŒ®\n   - å…¬å¼ï¼š`g_final = g_avg + N(0, ÏƒÂ²CÂ²)`\n\n### éšç§é¢„ç®— Îµï¼ˆEpsilonï¼‰\n\n| Îµ å€¼ | éšç§ä¿æŠ¤ | å™ªå£°å¤§å° | æ¨¡å‹æ€§èƒ½ |\n|------|----------|----------|----------|\n| å°ï¼ˆ0.1-1ï¼‰ | å¼º | å¤§ | è¾ƒå·® |\n| ä¸­ï¼ˆ1-3ï¼‰ | ä¸­ç­‰ | ä¸­ç­‰ | å¹³è¡¡ |\n| å¤§ï¼ˆ3-10ï¼‰ | å¼± | å° | è¾ƒå¥½ |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.1 å®ç°å·®åˆ†éšç§çš„æ ¸å¿ƒå‡½æ•°"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== å¡«ç©º 1ï¼šå®ç°æ¢¯åº¦è£å‰ªå‡½æ•° ==========\n# \n# ğŸ¯ ä»»åŠ¡ï¼šå®ç°æ¢¯åº¦è£å‰ªï¼Œé™åˆ¶æ¢¯åº¦çš„ L2 èŒƒæ•°ä¸è¶…è¿‡ max_norm\n# \n# ğŸ’¡ æç¤ºï¼š\n#   - å…ˆè®¡ç®—æ¢¯åº¦çš„ L2 èŒƒæ•°ï¼štorch.norm(gradient)\n#   - å¦‚æœèŒƒæ•° > max_normï¼Œåˆ™æŒ‰æ¯”ä¾‹ç¼©æ”¾ï¼šgradient * (max_norm / å½“å‰èŒƒæ•°)\n#   - å¦‚æœèŒƒæ•° â‰¤ max_normï¼Œåˆ™ä¸å˜\n#\n# éš¾åº¦ï¼šâ­â­â˜†â˜†â˜†\n\ndef clip_gradient(gradient, max_norm):\n    \"\"\"\n    è£å‰ªæ¢¯åº¦ï¼Œç¡®ä¿å…¶ L2 èŒƒæ•°ä¸è¶…è¿‡ max_norm\n    \n    å‚æ•°:\n        gradient: æ¢¯åº¦å¼ é‡\n        max_norm: æœ€å¤§èŒƒæ•°é˜ˆå€¼ï¼ˆè£å‰ªé˜ˆå€¼ Cï¼‰\n    \n    è¿”å›:\n        è£å‰ªåçš„æ¢¯åº¦\n    \"\"\"\n    # æ­¥éª¤1ï¼šè®¡ç®—æ¢¯åº¦çš„ L2 èŒƒæ•°\n    grad_norm = torch.norm(gradient)\n    \n    # æ­¥éª¤2ï¼šå¦‚æœèŒƒæ•°è¶…è¿‡é˜ˆå€¼ï¼Œè¿›è¡Œè£å‰ª\n    # è¯·å°† ___________ æ›¿æ¢ä¸ºæ­£ç¡®çš„è£å‰ªå…¬å¼\n    if grad_norm > max_norm:\n        gradient = ___________  # æœŸæœ›ï¼šå°†æ¢¯åº¦èŒƒæ•°ç¼©æ”¾åˆ° max_norm\n    \n    return gradient\n\n# æµ‹è¯•æ¢¯åº¦è£å‰ª\nprint(\"=\"*60)\nprint(\"ğŸ§ª æµ‹è¯•æ¢¯åº¦è£å‰ªåŠŸèƒ½\")\nprint(\"=\"*60)\n\ntest_grad = torch.randn(10) * 5  # ç”Ÿæˆä¸€ä¸ªå¤§æ¢¯åº¦\noriginal_norm = torch.norm(test_grad).item()\nclipped_grad = clip_gradient(test_grad.clone(), max_norm=1.0)\nclipped_norm = torch.norm(clipped_grad).item()\n\nprint(f\"  åŸå§‹æ¢¯åº¦èŒƒæ•°: {original_norm:.4f}\")\nprint(f\"  è£å‰ªåèŒƒæ•°: {clipped_norm:.4f}\")\nprint(f\"  è£å‰ªé˜ˆå€¼: 1.0\")\n\nif clipped_norm <= 1.001:  # å…è®¸å¾®å°çš„æ•°å€¼è¯¯å·®\n    print(\"  âœ“ æ¢¯åº¦è£å‰ªåŠŸèƒ½æ­£å¸¸ï¼\")\nelse:\n    print(\"  âœ— æ¢¯åº¦è£å‰ªå¯èƒ½æœ‰è¯¯ï¼Œè¯·æ£€æŸ¥å¡«ç©º 1\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== å¡«ç©º 2ï¼šå®ç°å™ªå£°æ·»åŠ æœºåˆ¶ ==========\n# \n# ğŸ¯ ä»»åŠ¡ï¼šåœ¨å¹³å‡æ¢¯åº¦ä¸Šæ·»åŠ é«˜æ–¯å™ªå£°ï¼Œå®ç°å·®åˆ†éšç§ä¿æŠ¤\n# \n# ğŸ’¡ æç¤ºï¼š\n#   - å™ªå£°æ ‡å‡†å·®è®¡ç®—å…¬å¼ï¼šnoise_std = (max_grad_norm * noise_multiplier) / n_samples\n#   - ä½¿ç”¨ torch.randn_like(tensor) ç”Ÿæˆä¸ tensor åŒå½¢çŠ¶çš„æ ‡å‡†æ­£æ€åˆ†å¸ƒéšæœºæ•°\n#   - å°†éšæœºæ•°ä¹˜ä»¥ noise_std å¾—åˆ°æœ€ç»ˆå™ªå£°\n#\n# éš¾åº¦ï¼šâ­â­â­â˜†â˜†\n\ndef add_noise_to_gradient(avg_gradient, max_grad_norm, noise_multiplier, n_samples):\n    \"\"\"\n    åœ¨å¹³å‡æ¢¯åº¦ä¸Šæ·»åŠ é«˜æ–¯å™ªå£°\n    \n    å‚æ•°:\n        avg_gradient: å¹³å‡æ¢¯åº¦\n        max_grad_norm: æ¢¯åº¦è£å‰ªé˜ˆå€¼ C\n        noise_multiplier: å™ªå£°ä¹˜æ•° Ïƒï¼ˆæ§åˆ¶éšç§å¼ºåº¦ï¼‰\n        n_samples: æ ·æœ¬æ•°é‡\n    \n    è¿”å›:\n        æ·»åŠ å™ªå£°åçš„æ¢¯åº¦\n    \"\"\"\n    # æ­¥éª¤1ï¼šè®¡ç®—å™ªå£°æ ‡å‡†å·®\n    noise_std = (max_grad_norm * noise_multiplier) / n_samples\n    \n    # æ­¥éª¤2ï¼šç”Ÿæˆé«˜æ–¯å™ªå£°\n    # è¯·å°† ___________ æ›¿æ¢ä¸ºç”Ÿæˆå™ªå£°çš„ä»£ç \n    noise = ___________  # æœŸæœ›ï¼šç”Ÿæˆ N(0, noise_stdÂ²) çš„å™ªå£°\n    \n    # æ­¥éª¤3ï¼šå°†å™ªå£°æ·»åŠ åˆ°æ¢¯åº¦ä¸Š\n    noisy_gradient = avg_gradient + noise\n    \n    return noisy_gradient\n\n# æµ‹è¯•å™ªå£°æ·»åŠ \nprint(\"=\"*60)\nprint(\"ğŸ§ª æµ‹è¯•å™ªå£°æ·»åŠ åŠŸèƒ½\")\nprint(\"=\"*60)\n\ntest_avg_grad = torch.ones(10) * 0.1\nnoisy_grad = add_noise_to_gradient(test_avg_grad.clone(), max_grad_norm=1.0, \n                                   noise_multiplier=1.0, n_samples=100)\n\nnoise_magnitude = torch.norm(noisy_grad - test_avg_grad).item()\nprint(f\"  åŸå§‹æ¢¯åº¦å‡å€¼: {test_avg_grad.mean().item():.4f}\")\nprint(f\"  æ·»åŠ å™ªå£°åå‡å€¼: {noisy_grad.mean().item():.4f}\")\nprint(f\"  å™ªå£°å¹…åº¦: {noise_magnitude:.4f}\")\n\nif noise_magnitude > 0.001:\n    print(\"  âœ“ å™ªå£°æ·»åŠ åŠŸèƒ½æ­£å¸¸ï¼\")\nelse:\n    print(\"  âœ— å™ªå£°å¯èƒ½æœªæ­£ç¡®æ·»åŠ ï¼Œè¯·æ£€æŸ¥å¡«ç©º 2\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ğŸ¤” æ€è€ƒä¸€ä¸‹\n\n1. **ä¸ºä»€ä¹ˆéœ€è¦å…ˆè£å‰ªæ¢¯åº¦å†æ·»åŠ å™ªå£°ï¼Ÿ** å¦‚æœä¸è£å‰ªç›´æ¥æ·»åŠ å™ªå£°ä¼šæœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ\n\n2. **å™ªå£°æ ‡å‡†å·®ä¸ºä»€ä¹ˆé™¤ä»¥æ ·æœ¬æ•° n_samplesï¼Ÿ** è¿™ä¸éšç§ä¿æŠ¤æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®ç°å·®åˆ†éšç§è®­ç»ƒ\n\n### 3.1 å®ç°å®Œæ•´çš„ DP-SGD è®­ç»ƒæµç¨‹"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_dp(model, X_train, y_train, epochs=50, lr=0.01, \n",
    "                  max_grad_norm=1.0, noise_multiplier=1.0, verbose=True):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨å·®åˆ†éšç§è®­ç»ƒæ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆ DP-SGDï¼‰\n",
    "    \n",
    "    å‚æ•°:\n",
    "        model: PyTorch æ¨¡å‹\n",
    "        X_train, y_train: è®­ç»ƒæ•°æ®\n",
    "        epochs: è®­ç»ƒè½®æ•°\n",
    "        lr: å­¦ä¹ ç‡\n",
    "        max_grad_norm: æ¢¯åº¦è£å‰ªé˜ˆå€¼ C\n",
    "        noise_multiplier: å™ªå£°ä¹˜æ•° Ïƒ\n",
    "        verbose: æ˜¯å¦æ‰“å°è®­ç»ƒè¿‡ç¨‹\n",
    "    \n",
    "    è¿”å›:\n",
    "        losses: æ¯ä¸ª epoch çš„æŸå¤±å€¼åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    n_samples = len(X_train)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nğŸ” å¼€å§‹å·®åˆ†éšç§è®­ç»ƒï¼ˆå™ªå£°å¼ºåº¦={noise_multiplier}ï¼‰\")\n",
    "        print(f\"  æ¢¯åº¦è£å‰ªé˜ˆå€¼: {max_grad_norm}\")\n",
    "        print(f\"  è®­ç»ƒæ ·æœ¬æ•°: {n_samples}\")\n",
    "        print(f\"  å­¦ä¹ ç‡: {lr}\\n\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # åˆå§‹åŒ–ç´¯ç§¯æ¢¯åº¦å­—å…¸\n",
    "        accumulated_grads = {name: torch.zeros_like(param) \n",
    "                            for name, param in model.named_parameters()}\n",
    "        \n",
    "        # å¯¹æ¯ä¸ªæ ·æœ¬å•ç‹¬è®¡ç®—æ¢¯åº¦ï¼ˆper-sample gradientï¼‰\n",
    "        for i in range(n_samples):\n",
    "            # å‰å‘ä¼ æ’­\n",
    "            output = model(X_train[i:i+1])\n",
    "            loss = criterion(output, y_train[i:i+1])\n",
    "            \n",
    "            # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # å¯¹æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦è¿›è¡Œè£å‰ªå¹¶ç´¯åŠ \n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    # è£å‰ªå•ä¸ªæ ·æœ¬çš„æ¢¯åº¦\n",
    "                    clipped_grad = clip_gradient(param.grad.clone(), max_grad_norm)\n",
    "                    # ç´¯åŠ åˆ°æ€»æ¢¯åº¦\n",
    "                    accumulated_grads[name] += clipped_grad\n",
    "        \n",
    "        # è®¡ç®—å¹³å‡æ¢¯åº¦å¹¶æ·»åŠ å™ªå£°\n",
    "        for name, param in model.named_parameters():\n",
    "            # è®¡ç®—å¹³å‡æ¢¯åº¦\n",
    "            avg_grad = accumulated_grads[name] / n_samples\n",
    "            \n",
    "            # æ·»åŠ é«˜æ–¯å™ªå£°\n",
    "            noisy_grad = add_noise_to_gradient(\n",
    "                avg_grad, max_grad_norm, noise_multiplier, n_samples\n",
    "            )\n",
    "            \n",
    "            # ä½¿ç”¨å¸¦å™ªå£°çš„æ¢¯åº¦æ›´æ–°å‚æ•°\n",
    "            param.data -= lr * noisy_grad\n",
    "        \n",
    "        # è®¡ç®—å½“å‰ epoch çš„æŸå¤±å’Œå‡†ç¡®ç‡\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_train)\n",
    "            epoch_loss = criterion(outputs, y_train).item()\n",
    "            epoch_acc = (outputs.argmax(dim=1) == y_train).float().mean().item()\n",
    "            losses.append(epoch_loss)\n",
    "            accuracies.append(epoch_acc)\n",
    "        \n",
    "        # æ‰“å°è®­ç»ƒè¿›åº¦\n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            print(f\"  Epoch [{epoch+1:2d}/{epochs}] - Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.2%}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"  âœ“ è®­ç»ƒå®Œæˆï¼\\n\")\n",
    "    \n",
    "    return losses, accuracies\n",
    "\n",
    "print(\"âœ“ å·®åˆ†éšç§è®­ç»ƒå‡½æ•°å®šä¹‰å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.2 å®ç°æ™®é€šè®­ç»ƒå‡½æ•°ï¼ˆç”¨äºå¯¹æ¯”ï¼‰"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_normal(model, X_train, y_train, epochs=50, lr=0.01, verbose=True):\n",
    "    \"\"\"\n",
    "    æ™®é€šè®­ç»ƒï¼ˆæ— éšç§ä¿æŠ¤ï¼‰\n",
    "    \n",
    "    å‚æ•°:\n",
    "        model: PyTorch æ¨¡å‹\n",
    "        X_train, y_train: è®­ç»ƒæ•°æ®\n",
    "        epochs: è®­ç»ƒè½®æ•°\n",
    "        lr: å­¦ä¹ ç‡\n",
    "        verbose: æ˜¯å¦æ‰“å°è®­ç»ƒè¿‡ç¨‹\n",
    "    \n",
    "    è¿”å›:\n",
    "        losses: æ¯ä¸ª epoch çš„æŸå¤±å€¼åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nğŸ”“ å¼€å§‹æ™®é€šè®­ç»ƒï¼ˆæ— éšç§ä¿æŠ¤ï¼‰\")\n",
    "        print(f\"  å­¦ä¹ ç‡: {lr}\\n\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # æ ‡å‡†çš„æ‰¹é‡æ¢¯åº¦ä¸‹é™\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # è®°å½•æŸå¤±å’Œå‡†ç¡®ç‡\n",
    "        losses.append(loss.item())\n",
    "        acc = (outputs.argmax(dim=1) == y_train).float().mean().item()\n",
    "        accuracies.append(acc)\n",
    "        \n",
    "        # æ‰“å°è®­ç»ƒè¿›åº¦\n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            print(f\"  Epoch [{epoch+1:2d}/{epochs}] - Loss: {loss.item():.4f} | Acc: {acc:.2%}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"  âœ“ è®­ç»ƒå®Œæˆï¼\\n\")\n",
    "    \n",
    "    return losses, accuracies\n",
    "\n",
    "print(\"âœ“ æ™®é€šè®­ç»ƒå‡½æ•°å®šä¹‰å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.3 è®­ç»ƒä¸¤ä¸ªæ¨¡å‹è¿›è¡Œå¯¹æ¯”"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# è®­ç»ƒæ™®é€šæ¨¡å‹\n",
    "normal_model = SimpleClassifier()\n",
    "normal_losses, normal_accs = train_normal(normal_model, X_train, y_train, epochs=50, lr=0.01)\n",
    "\n",
    "# è®­ç»ƒå·®åˆ†éšç§æ¨¡å‹\n",
    "dp_model = SimpleClassifier()\n",
    "dp_losses, dp_accs = train_with_dp(dp_model, X_train, y_train, epochs=50, lr=0.01,\n",
    "                                    max_grad_norm=1.0, noise_multiplier=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.4 å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»˜åˆ¶è®­ç»ƒæŸå¤±å’Œå‡†ç¡®ç‡å¯¹æ¯”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# æŸå¤±æ›²çº¿\n",
    "axes[0].plot(normal_losses, label='æ™®é€šè®­ç»ƒ', color='steelblue', linewidth=2)\n",
    "axes[0].plot(dp_losses, label='å·®åˆ†éšç§è®­ç»ƒ (Ïƒ=1.0)', color='coral', linewidth=2)\n",
    "axes[0].set_xlabel('è®­ç»ƒè½®æ•° (Epoch)', fontsize=12)\n",
    "axes[0].set_ylabel('æŸå¤±å€¼ (Loss)', fontsize=12)\n",
    "axes[0].set_title('ğŸ“‰ è®­ç»ƒæŸå¤±å¯¹æ¯”', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# å‡†ç¡®ç‡æ›²çº¿\n",
    "axes[1].plot(normal_accs, label='æ™®é€šè®­ç»ƒ', color='steelblue', linewidth=2)\n",
    "axes[1].plot(dp_accs, label='å·®åˆ†éšç§è®­ç»ƒ (Ïƒ=1.0)', color='coral', linewidth=2)\n",
    "axes[1].set_xlabel('è®­ç»ƒè½®æ•° (Epoch)', fontsize=12)\n",
    "axes[1].set_ylabel('å‡†ç¡®ç‡ (Accuracy)', fontsize=12)\n",
    "axes[1].set_title('ğŸ“ˆ è®­ç»ƒå‡†ç¡®ç‡å¯¹æ¯”', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š è§‚å¯Ÿï¼š\")\n",
    "print(\"  - æ™®é€šæ¨¡å‹æ”¶æ•›æ›´å¿«ï¼ŒæŸå¤±ä¸‹é™æ›´æ˜æ˜¾\")\n",
    "print(\"  - DP æ¨¡å‹ç”±äºå™ªå£°å½±å“ï¼Œè®­ç»ƒæ›²çº¿æ›´åŠ æ³¢åŠ¨\")\n",
    "print(\"  - è¿™æ˜¯éšç§ä¿æŠ¤çš„ä»£ä»·ï¼šå™ªå£°å¹²æ‰°äº†æ­£å¸¸çš„æ¢¯åº¦ä¸‹é™\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "è¿è¡Œåˆ°è¿™é‡Œï¼Œä½ åº”è¯¥çœ‹åˆ°ï¼š\n- ä¸¤ä¸ªæ¨¡å‹éƒ½æˆåŠŸå®Œæˆ 50 è½®è®­ç»ƒ\n- æ™®é€šæ¨¡å‹çš„è®­ç»ƒå‡†ç¡®ç‡é€šå¸¸åœ¨ 85% ä»¥ä¸Š\n- DP æ¨¡å‹çš„è®­ç»ƒå‡†ç¡®ç‡ç•¥ä½äºæ™®é€šæ¨¡å‹ï¼ˆçº¦ 75-85%ï¼‰\n- DP æ¨¡å‹çš„æŸå¤±æ›²çº¿æ›´åŠ æ³¢åŠ¨ï¼ˆå™ªå£°å¯¼è‡´ï¼‰\n\nå¦‚æœ DP æ¨¡å‹å‡†ç¡®ç‡è¿‡ä½ï¼ˆ<60%ï¼‰ï¼Œå¯ä»¥å°è¯•å‡å° noise_multiplier æˆ–å¢åŠ è®­ç»ƒè½®æ•°ã€‚"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ç¬¬å››éƒ¨åˆ†ï¼šè¯„ä¼°æ¨¡å‹æ€§èƒ½ä¸éšç§ä¿æŠ¤\n\n### 4.1 è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ³›åŒ–æ€§èƒ½"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== å¡«ç©º 3ï¼šå®ç°æ¨¡å‹è¯„ä¼°å‡½æ•° ==========\n# \n# ğŸ¯ ä»»åŠ¡ï¼šè®¡ç®—æ¨¡å‹åœ¨ç»™å®šæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡\n# \n# ğŸ’¡ æç¤ºï¼š\n#   - ä½¿ç”¨ model.eval() åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼\n#   - ä½¿ç”¨ torch.no_grad() ç¦ç”¨æ¢¯åº¦è®¡ç®—\n#   - outputs.argmax(dim=1) è·å–é¢„æµ‹ç±»åˆ«\n#   - (pred == y).float().mean() è®¡ç®—å‡†ç¡®ç‡\n#\n# éš¾åº¦ï¼šâ­â­â˜†â˜†â˜†\n\ndef evaluate_model(model, X, y):\n    \"\"\"\n    è¯„ä¼°æ¨¡å‹åœ¨ç»™å®šæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡\n    \n    å‚æ•°:\n        model: PyTorch æ¨¡å‹\n        X, y: è¯„ä¼°æ•°æ®\n    \n    è¿”å›:\n        accuracy: å‡†ç¡®ç‡ï¼ˆ0-1 ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼‰\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        outputs = model(X)\n        predictions = outputs.argmax(dim=1)\n        \n        # è¯·å°† ___________ æ›¿æ¢ä¸ºè®¡ç®—å‡†ç¡®ç‡çš„ä»£ç \n        accuracy = ___________  # æœŸæœ›ï¼šè¿”å› 0-1 ä¹‹é—´çš„å‡†ç¡®ç‡\n    \n    return accuracy\n\n# è¯„ä¼°ä¸¤ä¸ªæ¨¡å‹\nprint(\"=\"*60)\nprint(\"ğŸ“Š æ¨¡å‹æ€§èƒ½è¯„ä¼°\")\nprint(\"=\"*60)\n\n# æ™®é€šæ¨¡å‹\nnormal_train_acc = evaluate_model(normal_model, X_train, y_train)\nnormal_test_acc = evaluate_model(normal_model, X_test, y_test)\n\n# DP æ¨¡å‹\ndp_train_acc = evaluate_model(dp_model, X_train, y_train)\ndp_test_acc = evaluate_model(dp_model, X_test, y_test)\n\nprint(f\"\\n  {'æ¨¡å‹ç±»å‹':<15} {'è®­ç»ƒå‡†ç¡®ç‡':<15} {'æµ‹è¯•å‡†ç¡®ç‡':<15}\")\nprint(\"  \" + \"-\"*45)\nprint(f\"  {'æ™®é€šæ¨¡å‹':<15} {normal_train_acc:>13.2%} {normal_test_acc:>13.2%}\")\nprint(f\"  {'DPæ¨¡å‹ (Ïƒ=1.0)':<15} {dp_train_acc:>13.2%} {dp_test_acc:>13.2%}\")\nprint(\"\\n  ğŸ“‰ æ€§èƒ½ä¸‹é™ï¼š\")\nprint(f\"     è®­ç»ƒé›†: {(normal_train_acc - dp_train_acc):.2%}\")\nprint(f\"     æµ‹è¯•é›†: {(normal_test_acc - dp_test_acc):.2%}\")\n\nif (normal_test_acc - dp_test_acc) < 0.15:\n    print(\"\\n  âœ“ æ€§èƒ½ä¸‹é™åœ¨å¯æ¥å—èŒƒå›´å†…ï¼ˆ<15%ï¼‰\")\nelse:\n    print(\"\\n  âš  æ€§èƒ½ä¸‹é™è¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´éšç§å‚æ•°\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.2 æµ‹è¯•å¯¹æˆå‘˜æ¨ç†æ”»å‡»çš„é˜²å¾¡æ•ˆæœ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== å¡«ç©º 4ï¼šå®ç°æˆå‘˜æ¨ç†æ”»å‡» ==========\n# \n# ğŸ¯ ä»»åŠ¡ï¼šåŸºäºæ¨¡å‹ç½®ä¿¡åº¦æ‰§è¡Œæˆå‘˜æ¨ç†æ”»å‡»ï¼Œè¯„ä¼°éšç§æ³„éœ²é£é™©\n# \n# ğŸ’¡ æç¤ºï¼š\n#   - æˆå‘˜æ ·æœ¬é€šå¸¸æœ‰æ›´é«˜çš„é¢„æµ‹ç½®ä¿¡åº¦\n#   - ä½¿ç”¨ torch.softmax() å°† logits è½¬æ¢ä¸ºæ¦‚ç‡\n#   - gather() å‡½æ•°å¯ä»¥æå–æ­£ç¡®ç±»åˆ«çš„æ¦‚ç‡\n#   - ä½¿ç”¨é˜ˆå€¼åˆ¤æ–­ï¼šç½®ä¿¡åº¦ > é˜ˆå€¼ â†’ åˆ¤å®šä¸ºæˆå‘˜\n#\n# éš¾åº¦ï¼šâ­â­â­â˜†â˜†\n\ndef membership_inference_attack(model, X_train, y_train, X_test, y_test):\n    \"\"\"\n    æ‰§è¡ŒåŸºäºç½®ä¿¡åº¦çš„æˆå‘˜æ¨ç†æ”»å‡»\n    \n    å‚æ•°:\n        model: ç›®æ ‡æ¨¡å‹\n        X_train, y_train: è®­ç»ƒé›†ï¼ˆæˆå‘˜æ ·æœ¬ï¼‰\n        X_test, y_test: æµ‹è¯•é›†ï¼ˆéæˆå‘˜æ ·æœ¬ï¼‰\n    \n    è¿”å›:\n        attack_acc: æ”»å‡»å‡†ç¡®ç‡\n        train_conf: è®­ç»ƒæ ·æœ¬çš„ç½®ä¿¡åº¦æ•°ç»„\n        test_conf: æµ‹è¯•æ ·æœ¬çš„ç½®ä¿¡åº¦æ•°ç»„\n    \"\"\"\n    model.eval()\n    \n    with torch.no_grad():\n        # æ­¥éª¤1ï¼šè·å–æ¨¡å‹åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„è¾“å‡ºæ¦‚ç‡\n        # è¯·å°† ___________ æ›¿æ¢ä¸ºè®¡ç®— softmax æ¦‚ç‡çš„ä»£ç \n        train_probs = ___________  # æœŸæœ›ï¼šå½¢çŠ¶ [n_train, n_classes]\n        test_probs = ___________   # æœŸæœ›ï¼šå½¢çŠ¶ [n_test, n_classes]\n        \n        # æ­¥éª¤2ï¼šæå–æ­£ç¡®ç±»åˆ«çš„ç½®ä¿¡åº¦ï¼ˆçœŸå®æ ‡ç­¾å¯¹åº”çš„æ¦‚ç‡ï¼‰\n        train_conf = train_probs.gather(1, y_train.unsqueeze(1)).squeeze().cpu().numpy()\n        test_conf = test_probs.gather(1, y_test.unsqueeze(1)).squeeze().cpu().numpy()\n    \n    # æ­¥éª¤3ï¼šè®¾ç½®æ”»å‡»é˜ˆå€¼ï¼ˆä½¿ç”¨ä¸¤ç»„æ•°æ®çš„ä¸­ä½æ•°ç½®ä¿¡åº¦çš„å¹³å‡å€¼ï¼‰\n    threshold = (np.median(train_conf) + np.median(test_conf)) / 2\n    \n    # æ­¥éª¤4ï¼šæ‰§è¡Œæ”»å‡»\n    # è§„åˆ™ï¼šå¦‚æœç½®ä¿¡åº¦ > é˜ˆå€¼ï¼Œåˆ¤å®šä¸ºæˆå‘˜ï¼›å¦åˆ™åˆ¤å®šä¸ºéæˆå‘˜\n    member_correct = np.mean(train_conf > threshold)      # æˆå‘˜æ ·æœ¬è¢«æ­£ç¡®è¯†åˆ«çš„æ¯”ä¾‹\n    non_member_correct = np.mean(test_conf <= threshold)  # éæˆå‘˜æ ·æœ¬è¢«æ­£ç¡®è¯†åˆ«çš„æ¯”ä¾‹\n    \n    # æ”»å‡»æ€»å‡†ç¡®ç‡\n    attack_acc = (member_correct + non_member_correct) / 2\n    \n    return attack_acc, train_conf, test_conf, threshold\n\n# å¯¹ä¸¤ä¸ªæ¨¡å‹æ‰§è¡Œæˆå‘˜æ¨ç†æ”»å‡»\nprint(\"=\"*60)\nprint(\"ğŸ¯ æˆå‘˜æ¨ç†æ”»å‡»è¯„ä¼°\")\nprint(\"=\"*60)\n\nnormal_attack_acc, normal_train_conf, normal_test_conf, normal_threshold = \\\n    membership_inference_attack(normal_model, X_train, y_train, X_test, y_test)\n\ndp_attack_acc, dp_train_conf, dp_test_conf, dp_threshold = \\\n    membership_inference_attack(dp_model, X_train, y_train, X_test, y_test)\n\nprint(f\"\\n  {'æ¨¡å‹ç±»å‹':<15} {'æ”»å‡»å‡†ç¡®ç‡':<15} {'åˆ¤æ–­é˜ˆå€¼':<15}\")\nprint(\"  \" + \"-\"*45)\nprint(f\"  {'æ™®é€šæ¨¡å‹':<15} {normal_attack_acc:>13.2%} {normal_threshold:>13.4f}\")\nprint(f\"  {'DPæ¨¡å‹ (Ïƒ=1.0)':<15} {dp_attack_acc:>13.2%} {dp_threshold:>13.4f}\")\n\ndefense_improvement = normal_attack_acc - dp_attack_acc\nprint(f\"\\n  ğŸ›¡ï¸ é˜²å¾¡æ•ˆæœ: æ”»å‡»å‡†ç¡®ç‡é™ä½ {defense_improvement:.2%}\")\n\nif dp_attack_acc < 0.55:\n    print(\"  âœ“ å·®åˆ†éšç§æ˜¾è‘—é™ä½äº†éšç§æ³„éœ²é£é™©ï¼\")\nelif dp_attack_acc < 0.65:\n    print(\"  âš  å·®åˆ†éšç§æä¾›äº†ä¸€å®šä¿æŠ¤ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´\")\nelse:\n    print(\"  âœ— å½“å‰éšç§å‚æ•°å¯èƒ½ä¸è¶³ï¼Œå»ºè®®å¢åŠ å™ªå£°å¼ºåº¦\")\n\nprint(f\"\\n  ğŸ’¡ å‚è€ƒï¼šéšæœºçŒœæµ‹çš„æ”»å‡»å‡†ç¡®ç‡ä¸º 50%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.3 å¯è§†åŒ–æˆå‘˜ä¸éæˆå‘˜çš„ç½®ä¿¡åº¦åˆ†å¸ƒ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»˜åˆ¶ç½®ä¿¡åº¦åˆ†å¸ƒå¯¹æ¯”å›¾\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# æ™®é€šæ¨¡å‹çš„ç½®ä¿¡åº¦åˆ†å¸ƒ\n",
    "axes[0].hist(normal_train_conf, bins=30, alpha=0.6, label='æˆå‘˜ï¼ˆè®­ç»ƒé›†ï¼‰', \n",
    "             color='#FF6B6B', edgecolor='black')\n",
    "axes[0].hist(normal_test_conf, bins=30, alpha=0.6, label='éæˆå‘˜ï¼ˆæµ‹è¯•é›†ï¼‰', \n",
    "             color='#4ECDC4', edgecolor='black')\n",
    "axes[0].axvline(normal_threshold, color='red', linestyle='--', linewidth=2, \n",
    "                label=f'æ”»å‡»é˜ˆå€¼={normal_threshold:.3f}')\n",
    "axes[0].set_xlabel('é¢„æµ‹ç½®ä¿¡åº¦', fontsize=12)\n",
    "axes[0].set_ylabel('æ ·æœ¬æ•°é‡', fontsize=12)\n",
    "axes[0].set_title(f'æ™®é€šæ¨¡å‹\\næ”»å‡»å‡†ç¡®ç‡: {normal_attack_acc:.1%}', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# DP æ¨¡å‹çš„ç½®ä¿¡åº¦åˆ†å¸ƒ\n",
    "axes[1].hist(dp_train_conf, bins=30, alpha=0.6, label='æˆå‘˜ï¼ˆè®­ç»ƒé›†ï¼‰', \n",
    "             color='#FF6B6B', edgecolor='black')\n",
    "axes[1].hist(dp_test_conf, bins=30, alpha=0.6, label='éæˆå‘˜ï¼ˆæµ‹è¯•é›†ï¼‰', \n",
    "             color='#4ECDC4', edgecolor='black')\n",
    "axes[1].axvline(dp_threshold, color='red', linestyle='--', linewidth=2, \n",
    "                label=f'æ”»å‡»é˜ˆå€¼={dp_threshold:.3f}')\n",
    "axes[1].set_xlabel('é¢„æµ‹ç½®ä¿¡åº¦', fontsize=12)\n",
    "axes[1].set_ylabel('æ ·æœ¬æ•°é‡', fontsize=12)\n",
    "axes[1].set_title(f'å·®åˆ†éšç§æ¨¡å‹ (Ïƒ=1.0)\\næ”»å‡»å‡†ç¡®ç‡: {dp_attack_acc:.1%}', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('ğŸ” æˆå‘˜ vs éæˆå‘˜ç½®ä¿¡åº¦åˆ†å¸ƒå¯¹æ¯”', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š åˆ†æï¼š\")\n",
    "print(\"  - æ™®é€šæ¨¡å‹ï¼šæˆå‘˜å’Œéæˆå‘˜çš„ç½®ä¿¡åº¦åˆ†å¸ƒå·®å¼‚æ˜æ˜¾ï¼Œæ˜“äºåŒºåˆ†\")\n",
    "print(\"  - DP æ¨¡å‹ï¼šä¸¤ä¸ªåˆ†å¸ƒæ›´åŠ é‡å ï¼Œæ”»å‡»è€…éš¾ä»¥åˆ¤æ–­æ ·æœ¬æ˜¯å¦ä¸ºæˆå‘˜\")\n",
    "print(\"  - å·®åˆ†éšç§é€šè¿‡å™ªå£°æ¨¡ç³Šäº†æˆå‘˜å’Œéæˆå‘˜çš„è¾¹ç•Œï¼Œæå‡äº†éšç§ä¿æŠ¤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ğŸ¤” æ€è€ƒä¸€ä¸‹\n\n1. **ä¸ºä»€ä¹ˆæˆå‘˜æ ·æœ¬é€šå¸¸æœ‰æ›´é«˜çš„ç½®ä¿¡åº¦ï¼Ÿ** è¿™ä¸æ¨¡å‹çš„ä»€ä¹ˆç‰¹æ€§æœ‰å…³ï¼Ÿ\n\n2. **å¦‚æœæ”»å‡»å‡†ç¡®ç‡æ¥è¿‘ 50%ï¼Œè¯´æ˜ä»€ä¹ˆï¼Ÿ** è¿™å¯¹éšç§ä¿æŠ¤æ„å‘³ç€ä»€ä¹ˆï¼Ÿ"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ç¬¬äº”éƒ¨åˆ†ï¼šéšç§-æ•ˆç”¨æƒè¡¡åˆ†æ\n\n### 5.1 æ¢ç´¢ä¸åŒå™ªå£°å¼ºåº¦çš„å½±å“"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== å¡«ç©º 5ï¼šéšç§-æ•ˆç”¨æƒè¡¡å®éªŒ ==========\n# \n# ğŸ¯ ä»»åŠ¡ï¼šæµ‹è¯•ä¸åŒå™ªå£°å¼ºåº¦ä¸‹çš„æ¨¡å‹æ€§èƒ½å’Œéšç§ä¿æŠ¤æ•ˆæœ\n# \n# ğŸ’¡ æç¤ºï¼š\n#   - noise_multiplier=0 ç›¸å½“äºæ™®é€šè®­ç»ƒï¼ˆæ— éšç§ä¿æŠ¤ï¼‰\n#   - noise_multiplier è¶Šå¤§ï¼Œéšç§ä¿æŠ¤è¶Šå¼ºï¼Œä½†æ¨¡å‹æ€§èƒ½è¶Šå·®\n#   - éœ€è¦åœ¨å¾ªç¯ä¸­è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼Œæ”¶é›†æ€§èƒ½å’Œæ”»å‡»å‡†ç¡®ç‡æ•°æ®\n#\n# éš¾åº¦ï¼šâ­â­â­â­â˜†\n\nprint(\"=\"*60)\nprint(\"âš–ï¸ éšç§-æ•ˆç”¨æƒè¡¡åˆ†æ\")\nprint(\"=\"*60)\nprint(\"\\næµ‹è¯•ä¸åŒå™ªå£°å¼ºåº¦çš„å½±å“...\\n\")\n\n# å®šä¹‰è¦æµ‹è¯•çš„å™ªå£°å¼ºåº¦èŒƒå›´\nnoise_multipliers = [0.0, 0.5, 1.0, 1.5, 2.0, 3.0]\nresults = []\n\nfor i, noise_mult in enumerate(noise_multipliers):\n    print(f\"[{i+1}/{len(noise_multipliers)}] è®­ç»ƒæ¨¡å‹ (å™ªå£°å¼ºåº¦={noise_mult})...\")\n    \n    # åˆ›å»ºæ–°æ¨¡å‹\n    temp_model = SimpleClassifier()\n    \n    # æ ¹æ®å™ªå£°å¼ºåº¦é€‰æ‹©è®­ç»ƒæ–¹å¼\n    if noise_mult == 0:\n        # noise_mult=0 è¡¨ç¤ºæ™®é€šè®­ç»ƒ\n        # è¯·å°† ___________ æ›¿æ¢ä¸ºè°ƒç”¨æ™®é€šè®­ç»ƒå‡½æ•°çš„ä»£ç \n        ___________\n    else:\n        # noise_mult>0 è¡¨ç¤ºå·®åˆ†éšç§è®­ç»ƒ\n        # è¯·å°† ___________ æ›¿æ¢ä¸ºè°ƒç”¨ DP è®­ç»ƒå‡½æ•°çš„ä»£ç \n        ___________\n    \n    # è¯„ä¼°æ¨¡å‹æ€§èƒ½\n    test_acc = evaluate_model(temp_model, X_test, y_test)\n    \n    # è¯„ä¼°éšç§æ³„éœ²é£é™©ï¼ˆæˆå‘˜æ¨ç†æ”»å‡»ï¼‰\n    attack_acc, _, _, _ = membership_inference_attack(\n        temp_model, X_train, y_train, X_test, y_test\n    )\n    \n    # è®°å½•ç»“æœ\n    results.append({\n        'noise': noise_mult,\n        'test_acc': test_acc,\n        'attack_acc': attack_acc,\n        'privacy_gain': normal_attack_acc - attack_acc,  # ç›¸å¯¹æ™®é€šæ¨¡å‹çš„éšç§æå‡\n        'utility_loss': normal_test_acc - test_acc        # ç›¸å¯¹æ™®é€šæ¨¡å‹çš„æ€§èƒ½æŸå¤±\n    })\n    \n    print(f\"  æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2%} | æ”»å‡»å‡†ç¡®ç‡: {attack_acc:.2%}\\n\")\n\nprint(\"âœ“ å®éªŒå®Œæˆï¼\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.2 å¯è§†åŒ–éšç§-æ•ˆç”¨æƒè¡¡æ›²çº¿"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå–æ•°æ®\n",
    "noise_list = [r['noise'] for r in results]\n",
    "test_acc_list = [r['test_acc'] for r in results]\n",
    "attack_acc_list = [r['attack_acc'] for r in results]\n",
    "\n",
    "# åˆ›å»ºåŒ Y è½´å›¾è¡¨\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# å·¦ä¾§ Y è½´ï¼šæ¨¡å‹æµ‹è¯•å‡†ç¡®ç‡ï¼ˆæ•ˆç”¨ï¼‰\n",
    "color1 = '#4ECDC4'\n",
    "ax1.set_xlabel('å™ªå£°å¼ºåº¦ (noise_multiplier Ïƒ)', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylabel('æ¨¡å‹æµ‹è¯•å‡†ç¡®ç‡ï¼ˆæ•ˆç”¨ï¼‰', color=color1, fontsize=13, fontweight='bold')\n",
    "line1 = ax1.plot(noise_list, test_acc_list, 'o-', color=color1, linewidth=3, \n",
    "                 markersize=10, label='æµ‹è¯•å‡†ç¡®ç‡')\n",
    "ax1.tick_params(axis='y', labelcolor=color1)\n",
    "ax1.set_ylim([0.4, 1.0])\n",
    "ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# å³ä¾§ Y è½´ï¼šæˆå‘˜æ¨ç†æ”»å‡»å‡†ç¡®ç‡ï¼ˆéšç§é£é™©ï¼‰\n",
    "ax2 = ax1.twinx()\n",
    "color2 = '#FF6B6B'\n",
    "ax2.set_ylabel('æˆå‘˜æ¨ç†æ”»å‡»å‡†ç¡®ç‡ï¼ˆéšç§é£é™©ï¼‰', color=color2, fontsize=13, fontweight='bold')\n",
    "line2 = ax2.plot(noise_list, attack_acc_list, 's--', color=color2, linewidth=3, \n",
    "                 markersize=10, label='æ”»å‡»å‡†ç¡®ç‡')\n",
    "ax2.tick_params(axis='y', labelcolor=color2)\n",
    "ax2.set_ylim([0.4, 1.0])\n",
    "\n",
    "# æ·»åŠ éšæœºçŒœæµ‹åŸºå‡†çº¿\n",
    "ax2.axhline(y=0.5, color='gray', linestyle=':', linewidth=2, label='éšæœºçŒœæµ‹ (50%)')\n",
    "\n",
    "# æ·»åŠ å›¾ä¾‹\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines] + ['éšæœºçŒœæµ‹ (50%)']\n",
    "ax1.legend(lines + [ax2.lines[-1]], labels, loc='center left', fontsize=11, \n",
    "          bbox_to_anchor=(0.02, 0.5))\n",
    "\n",
    "plt.title('âš–ï¸ éšç§-æ•ˆç”¨æƒè¡¡ï¼šå™ªå£°å¼ºåº¦çš„å½±å“', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“ˆ å…³é”®è§‚å¯Ÿï¼š\")\n",
    "print(\"  1. å™ªå£°å¼ºåº¦ â†‘ â†’ æ¨¡å‹æ•ˆç”¨ â†“ï¼ˆè“è‰²æ›²çº¿ä¸‹é™ï¼‰\")\n",
    "print(\"  2. å™ªå£°å¼ºåº¦ â†‘ â†’ éšç§é£é™© â†“ï¼ˆçº¢è‰²æ›²çº¿ä¸‹é™ï¼Œè¶‹è¿‘ 50%ï¼‰\")\n",
    "print(\"  3. å­˜åœ¨ä¸€ä¸ªæœ€ä½³å¹³è¡¡ç‚¹ï¼Œåœ¨è¯¥ç‚¹éšç§ä¿æŠ¤å……åˆ†ä¸”æ€§èƒ½æŸå¤±å¯æ§\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.3 ç”Ÿæˆè¯¦ç»†çš„æƒè¡¡åˆ†æè¡¨æ ¼"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰“å°è¯¦ç»†ç»“æœè¡¨æ ¼\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“Š éšç§-æ•ˆç”¨æƒè¡¡è¯¦ç»†åˆ†æè¡¨\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n  {'å™ªå£°å¼ºåº¦':<12} {'æµ‹è¯•å‡†ç¡®ç‡':<12} {'æ”»å‡»å‡†ç¡®ç‡':<12} {'éšç§æå‡':<12} {'æ€§èƒ½æŸå¤±':<12}\")\n",
    "print(\"  \" + \"-\"*68)\n",
    "\n",
    "for r in results:\n",
    "    noise_str = f\"Ïƒ={r['noise']:.1f}\"\n",
    "    if r['noise'] == 0:\n",
    "        noise_str += \" (æ™®é€š)\"\n",
    "    \n",
    "    print(f\"  {noise_str:<12} {r['test_acc']:>10.2%} {r['attack_acc']:>10.2%} \"\n",
    "          f\"{r['privacy_gain']:>10.2%} {r['utility_loss']:>10.2%}\")\n",
    "\n",
    "print(\"\\n  ğŸ’¡ è¯´æ˜ï¼š\")\n",
    "print(\"     - éšç§æå‡ = ç›¸æ¯”æ™®é€šæ¨¡å‹ï¼Œæ”»å‡»å‡†ç¡®ç‡ä¸‹é™çš„å¹…åº¦\")\n",
    "print(\"     - æ€§èƒ½æŸå¤± = ç›¸æ¯”æ™®é€šæ¨¡å‹ï¼Œæµ‹è¯•å‡†ç¡®ç‡ä¸‹é™çš„å¹…åº¦\")\n",
    "\n",
    "# æ‰¾åˆ°æ¨èçš„å™ªå£°å¼ºåº¦\n",
    "# ç­–ç•¥ï¼šéšç§æå‡ > 10% ä¸”æ€§èƒ½æŸå¤± < 15% çš„æœ€å¤§å™ªå£°å¼ºåº¦\n",
    "recommended = None\n",
    "for r in results:\n",
    "    if r['privacy_gain'] > 0.10 and r['utility_loss'] < 0.15 and r['noise'] > 0:\n",
    "        recommended = r\n",
    "\n",
    "if recommended:\n",
    "    print(f\"\\n  ğŸ¯ æ¨èé…ç½®: Ïƒ={recommended['noise']:.1f}\")\n",
    "    print(f\"     - éšç§æå‡: {recommended['privacy_gain']:.2%}\")\n",
    "    print(f\"     - æ€§èƒ½æŸå¤±: {recommended['utility_loss']:.2%}\")\n",
    "    print(f\"     - è¯¥é…ç½®åœ¨éšç§ä¿æŠ¤å’Œæ¨¡å‹æ•ˆç”¨ä¹‹é—´å–å¾—äº†è‰¯å¥½å¹³è¡¡\")\n",
    "else:\n",
    "    print(\"\\n  âš  æç¤º: å¯èƒ½éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´å‚æ•°ä»¥æ‰¾åˆ°æ›´å¥½çš„å¹³è¡¡ç‚¹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "è¿è¡Œåˆ°è¿™é‡Œï¼Œä½ åº”è¯¥çœ‹åˆ°ï¼š\n- æˆåŠŸè®­ç»ƒäº† 6 ä¸ªä¸åŒå™ªå£°å¼ºåº¦çš„æ¨¡å‹\n- æµ‹è¯•å‡†ç¡®ç‡éšå™ªå£°å¼ºåº¦å¢åŠ è€Œä¸‹é™\n- æ”»å‡»å‡†ç¡®ç‡éšå™ªå£°å¼ºåº¦å¢åŠ è€Œä¸‹é™ï¼ˆè¶‹è¿‘ 50%ï¼‰\n- æƒè¡¡æ›²çº¿æ¸…æ™°å±•ç¤ºäº†éšç§ä¸æ•ˆç”¨çš„åå‘å…³ç³»\n\nå¦‚æœå‘ç°æŸäº›é…ç½®ä¸‹æ¨¡å‹å®Œå…¨å¤±æ•ˆï¼ˆå‡†ç¡®ç‡ < 40%ï¼‰ï¼Œè¯´æ˜å™ªå£°å¼ºåº¦è¿‡å¤§ï¼Œå¯ä»¥å°è¯•å¢åŠ è®­ç»ƒè½®æ•°æˆ–è°ƒæ•´å­¦ä¹ ç‡ã€‚"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“‹ å®éªŒå°ç»“\n\n### æ ¸å¿ƒæ”¶è·\n\né€šè¿‡æœ¬å®éªŒï¼Œä½ å·²ç»ï¼š\n\n| ç›®æ ‡ | å®Œæˆæƒ…å†µ |\n|------|----------|\n| ç†è§£å·®åˆ†éšç§æ ¸å¿ƒæœºåˆ¶ | âœ… æŒæ¡æ¢¯åº¦è£å‰ªä¸å™ªå£°æ·»åŠ  |\n| å®ç° DP-SGD è®­ç»ƒ | âœ… å®Œæˆç®€åŒ–ç‰ˆå·®åˆ†éšç§è®­ç»ƒæµç¨‹ |\n| è¯„ä¼°éšç§ä¿æŠ¤æ•ˆæœ | âœ… æˆå‘˜æ¨ç†æ”»å‡»å‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™ |\n| åˆ†æéšç§-æ•ˆç”¨æƒè¡¡ | âœ… ç†è§£ Îµ ä¸æ¨¡å‹æ€§èƒ½çš„å…³ç³» |\n\n### ğŸ”‘ å…³é”®ä»£ç å›é¡¾\n\n**å·®åˆ†éšç§ SGD çš„æ ¸å¿ƒä»£ç ï¼š**\n\n```python\n# 1. æ¢¯åº¦è£å‰ªï¼ˆé™åˆ¶å•ä¸ªæ ·æœ¬çš„å½±å“ï¼‰\ndef clip_gradient(gradient, max_norm):\n    grad_norm = torch.norm(gradient)\n    if grad_norm > max_norm:\n        gradient = gradient * (max_norm / grad_norm)\n    return gradient\n\n# 2. å™ªå£°æ·»åŠ ï¼ˆä¿æŠ¤æ ·æœ¬éšç§ï¼‰\ndef add_noise(avg_gradient, max_norm, noise_multiplier, n_samples):\n    noise_std = (max_norm * noise_multiplier) / n_samples\n    noise = torch.randn_like(avg_gradient) * noise_std\n    return avg_gradient + noise\n```\n\n### ğŸ¤” æ€è€ƒä¸è®¨è®º\n\n1. **å·®åˆ†éšç§èƒ½é˜²å¾¡æ‰€æœ‰éšç§æ”»å‡»å—ï¼Ÿ**\n   - DP ä¸»è¦é˜²å¾¡æˆå‘˜æ¨ç†æ”»å‡»ï¼Œä½†å¯¹è®­ç»ƒæ•°æ®æå–æ”»å‡»çš„æ•ˆæœæœ‰é™\n   - éœ€è¦ç»“åˆå…¶ä»–é˜²å¾¡æªæ–½ï¼ˆå¦‚æ•°æ®å»æ ‡è¯†åŒ–ã€è®¿é—®æ§åˆ¶ç­‰ï¼‰\n\n2. **å¦‚ä½•åœ¨å®é™…é¡¹ç›®ä¸­é€‰æ‹©éšç§å‚æ•°ï¼Ÿ**\n   - é«˜æ•æ„Ÿåœºæ™¯ï¼ˆåŒ»ç–—ã€é‡‘èï¼‰ï¼šÎµ â‰¤ 1\n   - ä¸­ç­‰æ•æ„Ÿåœºæ™¯ï¼ˆæ¨èç³»ç»Ÿï¼‰ï¼šÎµ âˆˆ [1, 3]\n   - ä½æ•æ„Ÿåœºæ™¯ï¼ˆå…¬å¼€æ•°æ®é›†ï¼‰ï¼šÎµ âˆˆ [3, 10]\n\n### ğŸ”— å»¶ä¼¸é˜…è¯»\n\n- **ç»å…¸è®ºæ–‡**ï¼šAbadi et al. (2016) - \"Deep Learning with Differential Privacy\"\n- **å®ç”¨å·¥å…·**ï¼š[Opacus](https://opacus.ai/) - PyTorch çš„å·®åˆ†éšç§åº“\n- **ç†è®ºåŸºç¡€**ï¼šDwork & Roth (2014) - \"The Algorithmic Foundations of Differential Privacy\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“– å‚è€ƒç­”æ¡ˆ\n\n<details>\n<summary>ç‚¹å‡»å±•å¼€å‚è€ƒç­”æ¡ˆ</summary>\n\n**å¡«ç©º 1ï¼šæ¢¯åº¦è£å‰ªå‡½æ•°**\n```python\nif grad_norm > max_norm:\n    gradient = gradient * (max_norm / grad_norm)\n```\n\n**å¡«ç©º 2ï¼šå™ªå£°æ·»åŠ æœºåˆ¶**\n```python\nnoise = torch.randn_like(avg_gradient) * noise_std\n```\n\n**å¡«ç©º 3ï¼šæ¨¡å‹è¯„ä¼°å‡½æ•°**\n```python\naccuracy = (predictions == y).float().mean().item()\n```\n\n**å¡«ç©º 4ï¼šæˆå‘˜æ¨ç†æ”»å‡»**\n```python\ntrain_probs = torch.softmax(model(X_train), dim=1)\ntest_probs = torch.softmax(model(X_test), dim=1)\n```\n\n**å¡«ç©º 5ï¼šéšç§-æ•ˆç”¨æƒè¡¡å®éªŒ**\n```python\n# æ™®é€šè®­ç»ƒ\ntrain_normal(temp_model, X_train, y_train, epochs=50, lr=0.01, verbose=False)\n\n# å·®åˆ†éšç§è®­ç»ƒ\ntrain_with_dp(temp_model, X_train, y_train, epochs=50, lr=0.01,\n             max_grad_norm=1.0, noise_multiplier=noise_mult, verbose=False)\n```\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "âš ï¸ **é‡è¦æé†’**\n\nå®éªŒå®Œæˆåï¼Œè¯·è®°å¾—ï¼š\n\n1. **é‡Šæ”¾ GPU èµ„æº**ï¼šå¦‚æœä½ ä½¿ç”¨çš„æ˜¯ Tencent CloudStudioï¼Œè¯·ç‚¹å‡»å³ä¸Šè§’çš„ã€Œåœæ­¢ã€æŒ‰é’®ï¼Œåœæ­¢è¿è¡Œã€‚\n2. **ä¿å­˜å®éªŒç»“æœ**ï¼šå¦‚éœ€ä¿å­˜ï¼Œè¯·ä¸‹è½½ notebook æ–‡ä»¶\n\nè¿™æ ·å¯ä»¥é¿å…ä¸å¿…è¦çš„èµ„æºå ç”¨å’Œè´¹ç”¨äº§ç”Ÿã€‚**å¦åˆ™ä¼šä¸€ç›´æ¶ˆè€—ä½ çš„å…è´¹èµ„æºé¢åº¦ã€‚**"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}