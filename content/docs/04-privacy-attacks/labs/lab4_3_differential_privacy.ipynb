{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验 4.3：差分隐私对比\n",
    "\n",
    "## 实验目标\n",
    "- 理解差分隐私训练的基本原理\n",
    "- 对比普通训练和差分隐私训练的模型\n",
    "- 观察隐私预算（ε）对模型性能和隐私保护的影响\n",
    "\n",
    "## 实验环境\n",
    "- Python 3.8+\n",
    "- PyTorch\n",
    "- numpy, matplotlib\n",
    "\n",
    "## 预计时间：30 分钟\n",
    "\n",
    "---\n",
    "\n",
    "## 核心概念回顾\n",
    "差分隐私通过在训练过程中添加噪声，保护单个训练样本的隐私。核心操作：梯度裁剪 + 噪声添加。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一部分：环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 设置中文显示\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"环境准备完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据集\n",
    "def create_dataset(n_samples=1000, n_features=20, n_classes=5):\n",
    "    X = np.random.randn(n_samples, n_features).astype(np.float32)\n",
    "    centers = np.random.randn(n_classes, n_features) * 2\n",
    "    y = np.random.randint(0, n_classes, n_samples)\n",
    "    for i in range(n_samples):\n",
    "        X[i] += centers[y[i]]\n",
    "    return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "# 创建并划分数据集\n",
    "X, y = create_dataset(n_samples=1000)\n",
    "train_size = 500\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_test, y_test = X[train_size:], y[train_size:]\n",
    "\n",
    "print(f\"训练集: {len(X_train)} 样本\")\n",
    "print(f\"测试集: {len(X_test)} 样本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义简单的分类器\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, n_features=20, n_classes=5, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_features, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "print(\"模型定义完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二部分：实现简化版差分隐私训练\n",
    "\n",
    "差分隐私 SGD（DP-SGD）的核心操作：\n",
    "1. **梯度裁剪**：限制每个样本的梯度范数\n",
    "2. **噪声添加**：在聚合梯度上添加高斯噪声"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 【填空 1】实现梯度裁剪函数\n",
    "# 提示：如果梯度范数超过阈值，按比例缩小\n",
    "\n",
    "def clip_gradient(gradient, max_norm):\n",
    "    \"\"\"\n",
    "    裁剪梯度，确保范数不超过 max_norm\n",
    "    \n",
    "    参数：\n",
    "        gradient: 梯度张量\n",
    "        max_norm: 最大范数阈值\n",
    "    返回：\n",
    "        裁剪后的梯度\n",
    "    \"\"\"\n",
    "    # 计算梯度的 L2 范数\n",
    "    grad_norm = torch.norm(gradient)\n",
    "    \n",
    "    # 【填空 1】如果范数超过阈值，进行裁剪\n",
    "    # 提示：clipped = gradient * (max_norm / grad_norm) if grad_norm > max_norm\n",
    "    # 参考答案：\n",
    "    # if grad_norm > max_norm:\n",
    "    #     gradient = gradient * (max_norm / grad_norm)\n",
    "    if grad_norm > max_norm:\n",
    "        gradient = ___________________\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "# 测试\n",
    "test_grad = torch.randn(10) * 5\n",
    "print(f\"裁剪前范数: {torch.norm(test_grad):.2f}\")\n",
    "clipped = clip_gradient(test_grad.clone(), max_norm=1.0)\n",
    "print(f\"裁剪后范数: {torch.norm(clipped):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 【填空 2】实现差分隐私训练函数\n",
    "# 提示：在每个 batch 的梯度上添加高斯噪声\n",
    "\n",
    "def train_with_dp(model, X_train, y_train, epochs=50, lr=0.01, \n",
    "                  max_grad_norm=1.0, noise_multiplier=1.0):\n",
    "    \"\"\"\n",
    "    使用差分隐私训练模型\n",
    "    \n",
    "    参数：\n",
    "        max_grad_norm: 梯度裁剪阈值\n",
    "        noise_multiplier: 噪声强度（相对于裁剪阈值）\n",
    "    \"\"\"\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    losses = []\n",
    "    n_samples = len(X_train)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # 对每个样本单独计算梯度（简化版，实际应用中使用批处理）\n",
    "        accumulated_grads = {name: torch.zeros_like(param) \n",
    "                            for name, param in model.named_parameters()}\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_train[i:i+1])\n",
    "            loss = criterion(output, y_train[i:i+1])\n",
    "            loss.backward()\n",
    "            \n",
    "            # 裁剪并累加每个样本的梯度\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    clipped_grad = clip_gradient(param.grad.clone(), max_grad_norm)\n",
    "                    accumulated_grads[name] += clipped_grad\n",
    "        \n",
    "        # 计算平均梯度并添加噪声\n",
    "        for name, param in model.named_parameters():\n",
    "            avg_grad = accumulated_grads[name] / n_samples\n",
    "            \n",
    "            # 【填空 2】添加高斯噪声\n",
    "            # 噪声标准差 = max_grad_norm * noise_multiplier / n_samples\n",
    "            # 参考答案：noise = torch.randn_like(avg_grad) * (max_grad_norm * noise_multiplier / n_samples)\n",
    "            noise_std = max_grad_norm * noise_multiplier / n_samples\n",
    "            noise = ___________________\n",
    "            \n",
    "            # 应用带噪声的梯度\n",
    "            param.data -= lr * (avg_grad + noise)\n",
    "        \n",
    "        # 记录损失\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_train)\n",
    "            epoch_loss = criterion(outputs, y_train).item()\n",
    "            losses.append(epoch_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            acc = (outputs.argmax(dim=1) == y_train).float().mean()\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Acc: {acc:.2%}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "print(\"差分隐私训练函数定义完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 普通训练函数（用于对比）\n",
    "def train_normal(model, X_train, y_train, epochs=50, lr=0.01):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            acc = (outputs.argmax(dim=1) == y_train).float().mean()\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Acc: {acc:.2%}\")\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三部分：对比普通训练和差分隐私训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练普通模型\n",
    "print(\"=\" * 50)\n",
    "print(\"训练普通模型（无隐私保护）\")\n",
    "print(\"=\" * 50)\n",
    "normal_model = SimpleClassifier()\n",
    "normal_losses = train_normal(normal_model, X_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练差分隐私模型\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"训练差分隐私模型（噪声强度=1.0）\")\n",
    "print(\"=\" * 50)\n",
    "dp_model = SimpleClassifier()\n",
    "dp_losses = train_with_dp(dp_model, X_train, y_train, epochs=50, \n",
    "                          noise_multiplier=1.0, max_grad_norm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 【填空 3】评估两个模型的性能\n",
    "# 提示：分别计算训练集和测试集的准确率\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X)\n",
    "        # 【填空 3】计算准确率\n",
    "        # 参考答案：acc = (outputs.argmax(dim=1) == y).float().mean().item()\n",
    "        acc = ___________________\n",
    "    return acc\n",
    "\n",
    "print(\"\\n模型性能对比：\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "normal_train_acc = evaluate_model(normal_model, X_train, y_train)\n",
    "normal_test_acc = evaluate_model(normal_model, X_test, y_test)\n",
    "print(f\"普通模型 - 训练准确率: {normal_train_acc:.2%}, 测试准确率: {normal_test_acc:.2%}\")\n",
    "\n",
    "dp_train_acc = evaluate_model(dp_model, X_train, y_train)\n",
    "dp_test_acc = evaluate_model(dp_model, X_test, y_test)\n",
    "print(f\"DP模型   - 训练准确率: {dp_train_acc:.2%}, 测试准确率: {dp_test_acc:.2%}\")\n",
    "\n",
    "print(f\"\\n准确率下降: {(normal_test_acc - dp_test_acc):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四部分：差分隐私对成员推理的防御效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 成员推理攻击函数\n",
    "def membership_inference(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    执行成员推理攻击并返回攻击准确率\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 获取置信度\n",
    "        train_probs = torch.softmax(model(X_train), dim=1)\n",
    "        test_probs = torch.softmax(model(X_test), dim=1)\n",
    "        \n",
    "        train_conf = train_probs.gather(1, y_train.unsqueeze(1)).squeeze().numpy()\n",
    "        test_conf = test_probs.gather(1, y_test.unsqueeze(1)).squeeze().numpy()\n",
    "    \n",
    "    # 使用中位数作为阈值\n",
    "    threshold = (np.median(train_conf) + np.median(test_conf)) / 2\n",
    "    \n",
    "    # 计算攻击准确率\n",
    "    member_correct = np.mean(train_conf > threshold)\n",
    "    non_member_correct = np.mean(test_conf <= threshold)\n",
    "    attack_acc = (member_correct + non_member_correct) / 2\n",
    "    \n",
    "    return attack_acc, train_conf, test_conf\n",
    "\n",
    "# 对两个模型执行攻击\n",
    "normal_attack_acc, normal_train_conf, normal_test_conf = membership_inference(\n",
    "    normal_model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "dp_attack_acc, dp_train_conf, dp_test_conf = membership_inference(\n",
    "    dp_model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"成员推理攻击结果：\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"普通模型 - 攻击准确率: {normal_attack_acc:.2%}\")\n",
    "print(f\"DP模型   - 攻击准确率: {dp_attack_acc:.2%}\")\n",
    "print(f\"\\n防御效果: 攻击准确率下降 {(normal_attack_acc - dp_attack_acc):.2%}\")\n",
    "\n",
    "if dp_attack_acc < 0.55:\n",
    "    print(\"✓ 差分隐私有效降低了成员推理攻击的成功率！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化置信度分布对比\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 普通模型\n",
    "axes[0].hist(normal_train_conf, bins=25, alpha=0.6, label='成员', color='coral')\n",
    "axes[0].hist(normal_test_conf, bins=25, alpha=0.6, label='非成员', color='steelblue')\n",
    "axes[0].set_xlabel('置信度')\n",
    "axes[0].set_ylabel('样本数量')\n",
    "axes[0].set_title(f'普通模型\\n攻击准确率: {normal_attack_acc:.1%}')\n",
    "axes[0].legend()\n",
    "\n",
    "# DP模型\n",
    "axes[1].hist(dp_train_conf, bins=25, alpha=0.6, label='成员', color='coral')\n",
    "axes[1].hist(dp_test_conf, bins=25, alpha=0.6, label='非成员', color='steelblue')\n",
    "axes[1].set_xlabel('置信度')\n",
    "axes[1].set_ylabel('样本数量')\n",
    "axes[1].set_title(f'差分隐私模型\\n攻击准确率: {dp_attack_acc:.1%}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.suptitle('成员 vs 非成员置信度分布对比', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"观察：DP模型的成员和非成员置信度分布更加重叠，更难区分\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第五部分：不同噪声强度的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试不同噪声强度\n",
    "noise_multipliers = [0.0, 0.5, 1.0, 2.0, 4.0]\n",
    "results = []\n",
    "\n",
    "print(\"测试不同噪声强度...\\n\")\n",
    "\n",
    "for noise_mult in noise_multipliers:\n",
    "    # 训练模型\n",
    "    temp_model = SimpleClassifier()\n",
    "    if noise_mult == 0:\n",
    "        train_normal(temp_model, X_train, y_train, epochs=50)\n",
    "    else:\n",
    "        train_with_dp(temp_model, X_train, y_train, epochs=50, \n",
    "                      noise_multiplier=noise_mult)\n",
    "    \n",
    "    # 评估\n",
    "    test_acc = evaluate_model(temp_model, X_test, y_test)\n",
    "    attack_acc, _, _ = membership_inference(temp_model, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    results.append({\n",
    "        'noise': noise_mult,\n",
    "        'test_acc': test_acc,\n",
    "        'attack_acc': attack_acc\n",
    "    })\n",
    "    print(f\"噪声={noise_mult}: 测试准确率={test_acc:.1%}, 攻击准确率={attack_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化隐私-效用权衡\n",
    "noise_list = [r['noise'] for r in results]\n",
    "test_acc_list = [r['test_acc'] for r in results]\n",
    "attack_acc_list = [r['attack_acc'] for r in results]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "color1 = 'steelblue'\n",
    "ax1.set_xlabel('噪声强度（noise_multiplier）')\n",
    "ax1.set_ylabel('模型测试准确率', color=color1)\n",
    "ax1.plot(noise_list, test_acc_list, 'o-', color=color1, linewidth=2, markersize=8, label='测试准确率')\n",
    "ax1.tick_params(axis='y', labelcolor=color1)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color2 = 'coral'\n",
    "ax2.set_ylabel('成员推理攻击准确率', color=color2)\n",
    "ax2.plot(noise_list, attack_acc_list, 's--', color=color2, linewidth=2, markersize=8, label='攻击准确率')\n",
    "ax2.tick_params(axis='y', labelcolor=color2)\n",
    "ax2.axhline(y=0.5, color='gray', linestyle=':', label='随机猜测')\n",
    "\n",
    "plt.title('隐私-效用权衡：噪声强度的影响')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n结论：\")\n",
    "print(\"- 噪声越大 → 隐私保护越强（攻击准确率越低）\")\n",
    "print(\"- 噪声越大 → 模型效用越低（测试准确率越低）\")\n",
    "print(\"- 需要根据应用场景选择合适的平衡点\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验总结\n",
    "\n",
    "### 观察记录\n",
    "\n",
    "请回答以下问题：\n",
    "\n",
    "1. **差分隐私如何影响模型性能？** 准确率下降了多少？这个代价可接受吗？\n",
    "\n",
    "2. **差分隐私对成员推理的防御效果如何？** 攻击准确率下降了多少？\n",
    "\n",
    "3. **如何选择合适的噪声强度？** 在隐私保护和模型效用之间如何权衡？\n",
    "\n",
    "### 核心概念回顾\n",
    "\n",
    "- **梯度裁剪**：限制单个样本的影响\n",
    "- **噪声添加**：隐藏单个样本的贡献\n",
    "- **隐私-效用权衡**：更强隐私 = 更低效用\n",
    "- **防御效果**：DP训练可有效降低成员推理攻击成功率\n",
    "\n",
    "---\n",
    "\n",
    "**模块四实验完成！** 你已经学习了训练数据提取、成员推理攻击和差分隐私防御的基础知识。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
