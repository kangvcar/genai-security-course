{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# å®éªŒ 4.3ï¼šå·®åˆ†éšç§é˜²å¾¡å®æˆ˜\n",
        "\n",
        "## ğŸ¯ å­¦ä¹ ç›®æ ‡\n",
        "å®Œæˆæœ¬å®éªŒåï¼Œä½ å°†èƒ½å¤Ÿï¼š\n",
        "- âœ… è§£é‡Šå·®åˆ†éšç§ï¼ˆDPï¼‰çš„æ ¸å¿ƒæœºåˆ¶ï¼šæ¢¯åº¦è£å‰ªä¸å™ªå£°æ·»åŠ \n",
        "- âœ… å®ç°ç®€åŒ–ç‰ˆçš„å·®åˆ†éšç§ SGDï¼ˆDP-SGDï¼‰è®­ç»ƒæµç¨‹\n",
        "- âœ… å¯¹æ¯”æ™®é€šè®­ç»ƒä¸ DP è®­ç»ƒåœ¨æ¨¡å‹æ€§èƒ½ä¸Šçš„å·®å¼‚\n",
        "- âœ… è¯„ä¼°å·®åˆ†éšç§å¯¹æˆå‘˜æ¨ç†æ”»å‡»çš„é˜²å¾¡æ•ˆæœ\n",
        "- âœ… åˆ†æéšç§é¢„ç®—ï¼ˆÎµï¼‰ä¸æ¨¡å‹æ•ˆç”¨ä¹‹é—´çš„æƒè¡¡å…³ç³»\n",
        "\n",
        "## ğŸ“š å‰ç½®çŸ¥è¯†\n",
        "- ç†è§£æ¢¯åº¦ä¸‹é™å’Œåå‘ä¼ æ’­çš„åŸºæœ¬åŸç†\n",
        "- äº†è§£æˆå‘˜æ¨ç†æ”»å‡»çš„å¨èƒæ¨¡å‹ï¼ˆå®Œæˆå®éªŒ 4.2ï¼‰\n",
        "- ç†Ÿæ‚‰ PyTorch åŸºç¡€æ“ä½œï¼ˆå¼ é‡ã€æ¨¡å‹è®­ç»ƒï¼‰\n",
        "\n",
        "## ğŸ–¥ï¸ å®éªŒç¯å¢ƒ\n",
        "\n",
        "| é¡¹ç›® | è¯´æ˜ |\n",
        "|------|------|\n",
        "| å¹³å° | æœ¬åœ°ç¯å¢ƒæˆ– Cloud Studio |\n",
        "| GPU | å¯é€‰ï¼ˆCPU å³å¯å®Œæˆï¼‰|\n",
        "| æ¨¡å‹ | è‡ªå®šä¹‰ PyTorch åˆ†ç±»å™¨ |\n",
        "| æ•°æ®é›† | åˆæˆå¤šåˆ†ç±»æ•°æ®é›† |\n",
        "\n",
        "## â±ï¸ é¢„è®¡æ—¶é—´ï¼š35 åˆ†é’Ÿ\n",
        "\n",
        "## ğŸ“ å¡«ç©ºè¯´æ˜\n",
        "æœ¬å®éªŒå…± **5 ä¸ªå¡«ç©º**ï¼Œéš¾åº¦ï¼šâ­â­â­â˜†â˜†\n",
        "- å¡«ç©º 1-2ï¼šå®ç°å·®åˆ†éšç§æ ¸å¿ƒæœºåˆ¶ï¼ˆæ¢¯åº¦è£å‰ªã€å™ªå£°æ·»åŠ ï¼‰\n",
        "- å¡«ç©º 3-4ï¼šæ¨¡å‹è¯„ä¼°ä¸é˜²å¾¡æ•ˆæœåˆ†æ\n",
        "- å¡«ç©º 5ï¼šéšç§-æ•ˆç”¨æƒè¡¡å®éªŒ\n",
        "\n",
        "âš ï¸ **é‡è¦æç¤º**ï¼šå·®åˆ†éšç§æ˜¯ç†è®ºä¸¥è°¨çš„éšç§ä¿æŠ¤æŠ€æœ¯ï¼Œæœ¬å®éªŒä¸ºç®€åŒ–æ•™å­¦ç‰ˆæœ¬ï¼Œå®é™…åº”ç”¨è¯·ä½¿ç”¨ä¸“ä¸šåº“ï¼ˆå¦‚ Opacusï¼‰ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“¦ ç¬¬ä¸€éƒ¨åˆ†ï¼šç¯å¢ƒå‡†å¤‡\n",
        "\n",
        "### å®‰è£…ä¾èµ–"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®‰è£…å¿…è¦çš„åº“\n",
        "!pip install torch numpy matplotlib scikit-learn -q\n",
        "\n",
        "print(\"âœ“ ä¾èµ–å®‰è£…å®Œæˆï¼\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¯¼å…¥å¿…è¦çš„åº“\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ”§ ç¯å¢ƒé…ç½®\")\n",
        "print(\"=\"*60)\n",
        "print(f\"  PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
        "print(f\"  è®¾å¤‡: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
        "print(\"  âœ“ ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### åˆ›å»ºå®éªŒæ•°æ®é›†"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç”Ÿæˆåˆæˆåˆ†ç±»æ•°æ®é›†\n",
        "def create_dataset(n_samples=1000, n_features=20, n_classes=5, test_size=0.5, random_state=42):\n",
        "    \"\"\"\n",
        "    ç”Ÿæˆç”¨äºè®­ç»ƒå’Œæµ‹è¯•çš„åˆ†ç±»æ•°æ®é›†\n",
        "    \n",
        "    å‚æ•°:\n",
        "        n_samples: æ€»æ ·æœ¬æ•°\n",
        "        n_features: ç‰¹å¾ç»´åº¦\n",
        "        n_classes: ç±»åˆ«æ•°\n",
        "        test_size: æµ‹è¯•é›†æ¯”ä¾‹\n",
        "    \n",
        "    è¿”å›:\n",
        "        X_train, y_train, X_test, y_test (å¼ é‡æ ¼å¼)\n",
        "    \"\"\"\n",
        "    # ç”Ÿæˆå¤šåˆ†ç±»æ•°æ®\n",
        "    X, y = make_classification(\n",
        "        n_samples=n_samples,\n",
        "        n_features=n_features,\n",
        "        n_informative=15,\n",
        "        n_redundant=5,\n",
        "        n_classes=n_classes,\n",
        "        random_state=random_state\n",
        "    )\n",
        "    \n",
        "    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "    )\n",
        "    \n",
        "    # è½¬æ¢ä¸º PyTorch å¼ é‡\n",
        "    return (\n",
        "        torch.FloatTensor(X_train),\n",
        "        torch.LongTensor(y_train),\n",
        "        torch.FloatTensor(X_test),\n",
        "        torch.LongTensor(y_test)\n",
        "    )\n",
        "\n",
        "# åˆ›å»ºæ•°æ®é›†\n",
        "X_train, y_train, X_test, y_test = create_dataset()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ“Š æ•°æ®é›†ä¿¡æ¯\")\n",
        "print(\"=\"*60)\n",
        "print(f\"  è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬\")\n",
        "print(f\"  æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬\")\n",
        "print(f\"  ç‰¹å¾ç»´åº¦: {X_train.shape[1]}\")\n",
        "print(f\"  ç±»åˆ«æ•°: {len(torch.unique(y_train))}\")\n",
        "print(\"  âœ“ æ•°æ®é›†åˆ›å»ºå®Œæˆï¼\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### å®šä¹‰æ¨¡å‹æ¶æ„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®šä¹‰ç®€å•çš„å‰é¦ˆç¥ç»ç½‘ç»œåˆ†ç±»å™¨\n",
        "class SimpleClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    ç®€å•çš„ä¸‰å±‚å…¨è¿æ¥ç¥ç»ç½‘ç»œ\n",
        "    \n",
        "    ç»“æ„: è¾“å…¥å±‚ -> éšè—å±‚1(64) -> éšè—å±‚2(64) -> è¾“å‡ºå±‚\n",
        "    \"\"\"\n",
        "    def __init__(self, n_features=20, n_classes=5, hidden_size=64):\n",
        "        super(SimpleClassifier, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(n_features, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_size, n_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# æµ‹è¯•æ¨¡å‹\n",
        "test_model = SimpleClassifier()\n",
        "test_input = torch.randn(4, 20)\n",
        "test_output = test_model(test_input)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ§  æ¨¡å‹æ¶æ„\")\n",
        "print(\"=\"*60)\n",
        "print(test_model)\n",
        "print(f\"\\n  æµ‹è¯•è¾“å…¥å½¢çŠ¶: {test_input.shape}\")\n",
        "print(f\"  æµ‹è¯•è¾“å‡ºå½¢çŠ¶: {test_output.shape}\")\n",
        "print(\"  âœ“ æ¨¡å‹å®šä¹‰å®Œæˆï¼\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### âœ… æ£€æŸ¥ç‚¹ 1\n",
        "\n",
        "è¿è¡Œåˆ°è¿™é‡Œï¼Œä½ åº”è¯¥çœ‹åˆ°ï¼š\n",
        "- [x] ç¯å¢ƒé…ç½®æˆåŠŸï¼Œæ˜¾ç¤º PyTorch ç‰ˆæœ¬\n",
        "- [x] æ•°æ®é›†åŒ…å« 500 ä¸ªè®­ç»ƒæ ·æœ¬å’Œ 500 ä¸ªæµ‹è¯•æ ·æœ¬\n",
        "- [x] æ¨¡å‹æ¶æ„æ¸…æ™°æ˜¾ç¤ºï¼ŒåŒ…å« 3 å±‚å…¨è¿æ¥ç½‘ç»œ\n",
        "\n",
        "å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š\n",
        "- æ˜¯å¦æˆåŠŸå®‰è£…äº† PyTorch å’Œç›¸å…³ä¾èµ–\n",
        "- æ•°æ®é›†çš„ç±»åˆ«æ˜¯å¦å¹³è¡¡åˆ†å¸ƒ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”¬ ç¬¬äºŒéƒ¨åˆ†ï¼šå·®åˆ†éšç§æ ¸å¿ƒæœºåˆ¶\n",
        "\n",
        "### æ¦‚å¿µå›é¡¾ï¼šä»€ä¹ˆæ˜¯å·®åˆ†éšç§ï¼Ÿ\n",
        "\n",
        "**å·®åˆ†éšç§ï¼ˆDifferential Privacy, DPï¼‰** æ˜¯ä¸€ç§ä¸¥æ ¼çš„éšç§ä¿æŠ¤æ ‡å‡†ï¼Œå®ƒç¡®ä¿ï¼š\n",
        "> å³ä½¿æ”»å‡»è€…æ‹¥æœ‰é™¤ä¸€æ¡è®°å½•å¤–çš„æ‰€æœ‰æ•°æ®ï¼Œä¹Ÿæ— æ³•ç¡®å®šè¯¥è®°å½•æ˜¯å¦åœ¨è®­ç»ƒé›†ä¸­ã€‚\n",
        "\n",
        "#### æ ¸å¿ƒæ€æƒ³\n",
        "é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ·»åŠ **ç²¾å¿ƒè®¾è®¡çš„å™ªå£°**ï¼Œä½¿å¾—å•ä¸ªæ ·æœ¬çš„å­˜åœ¨å¯¹æ¨¡å‹è¾“å‡ºçš„å½±å“å¯ä»¥è¢«å¿½ç•¥ã€‚\n",
        "\n",
        "#### DP-SGD çš„ä¸¤ä¸ªå…³é”®æ­¥éª¤\n",
        "\n",
        "```\n",
        "æ™®é€šè®­ç»ƒ:  æ ·æœ¬ â†’ è®¡ç®—æ¢¯åº¦ â†’ ç´¯åŠ  â†’ æ›´æ–°å‚æ•°\n",
        "                      â†“\n",
        "DPè®­ç»ƒ:    æ ·æœ¬ â†’ è®¡ç®—æ¢¯åº¦ â†’ è£å‰ª â†’ æ·»åŠ å™ªå£° â†’ æ›´æ–°å‚æ•°\n",
        "                         [æ§åˆ¶å½±å“] [ä¿æŠ¤éšç§]\n",
        "```\n",
        "\n",
        "1. **æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰**ï¼šé™åˆ¶æ¯ä¸ªæ ·æœ¬çš„æ¢¯åº¦èŒƒæ•°ä¸è¶…è¿‡é˜ˆå€¼ C\n",
        "   - ä½œç”¨ï¼šé˜²æ­¢æŸä¸ªæ ·æœ¬çš„æ¢¯åº¦è¿‡å¤§ï¼Œä¸»å¯¼æ¨¡å‹æ›´æ–°\n",
        "   - å…¬å¼ï¼š`g' = g * min(1, C / ||g||)`\n",
        "\n",
        "2. **å™ªå£°æ·»åŠ ï¼ˆNoise Additionï¼‰**ï¼šåœ¨èšåˆæ¢¯åº¦ä¸Šæ·»åŠ é«˜æ–¯å™ªå£°\n",
        "   - ä½œç”¨ï¼šéšè—å•ä¸ªæ ·æœ¬å¯¹æ¢¯åº¦çš„å…·ä½“è´¡çŒ®\n",
        "   - å…¬å¼ï¼š`g_final = g_avg + N(0, ÏƒÂ²CÂ²)`\n",
        "\n",
        "#### éšç§é¢„ç®— Îµï¼ˆEpsilonï¼‰\n",
        "- **Îµ è¶Šå°** â†’ éšç§ä¿æŠ¤è¶Šå¼ºï¼ˆå™ªå£°è¶Šå¤§ï¼‰â†’ æ¨¡å‹æ€§èƒ½è¶Šå·®\n",
        "- **Îµ è¶Šå¤§** â†’ éšç§ä¿æŠ¤è¶Šå¼±ï¼ˆå™ªå£°è¶Šå°ï¼‰â†’ æ¨¡å‹æ€§èƒ½è¶Šå¥½\n",
        "- å…¸å‹å€¼ï¼šÎµ âˆˆ [0.1, 10]ï¼Œå®è·µä¸­å¸¸ç”¨ Îµ âˆˆ [1, 3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### å®ç°å·®åˆ†éšç§çš„æ ¸å¿ƒå‡½æ•°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== å¡«ç©º 1ï¼šå®ç°æ¢¯åº¦è£å‰ªå‡½æ•° ==========\n",
        "# \n",
        "# ğŸ¯ ä»»åŠ¡ï¼šå®ç°æ¢¯åº¦è£å‰ªï¼Œé™åˆ¶æ¢¯åº¦çš„ L2 èŒƒæ•°ä¸è¶…è¿‡ max_norm\n",
        "# \n",
        "# ğŸ’¡ æç¤ºï¼š\n",
        "#   - å…ˆè®¡ç®—æ¢¯åº¦çš„ L2 èŒƒæ•°ï¼štorch.norm(gradient)\n",
        "#   - å¦‚æœèŒƒæ•° > max_normï¼Œåˆ™æŒ‰æ¯”ä¾‹ç¼©æ”¾ï¼šgradient * (max_norm / å½“å‰èŒƒæ•°)\n",
        "#   - å¦‚æœèŒƒæ•° â‰¤ max_normï¼Œåˆ™ä¸å˜\n",
        "# \n",
        "# ğŸ“– å‚è€ƒæ–‡æ¡£ï¼š\n",
        "#   - torch.norm(): è®¡ç®—å¼ é‡çš„èŒƒæ•°\n",
        "#   - æ¢¯åº¦è£å‰ªæ˜¯é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸çš„å¸¸ç”¨æŠ€æœ¯\n",
        "#\n",
        "# éš¾åº¦ï¼šâ­â­â˜†â˜†â˜†\n",
        "\n",
        "def clip_gradient(gradient, max_norm):\n",
        "    \"\"\"\n",
        "    è£å‰ªæ¢¯åº¦ï¼Œç¡®ä¿å…¶ L2 èŒƒæ•°ä¸è¶…è¿‡ max_norm\n",
        "    \n",
        "    å‚æ•°:\n",
        "        gradient: æ¢¯åº¦å¼ é‡\n",
        "        max_norm: æœ€å¤§èŒƒæ•°é˜ˆå€¼ï¼ˆè£å‰ªé˜ˆå€¼ Cï¼‰\n",
        "    \n",
        "    è¿”å›:\n",
        "        è£å‰ªåçš„æ¢¯åº¦\n",
        "    \"\"\"\n",
        "    # æ­¥éª¤1ï¼šè®¡ç®—æ¢¯åº¦çš„ L2 èŒƒæ•°\n",
        "    grad_norm = torch.norm(gradient)\n",
        "    \n",
        "    # æ­¥éª¤2ï¼šå¦‚æœèŒƒæ•°è¶…è¿‡é˜ˆå€¼ï¼Œè¿›è¡Œè£å‰ª\n",
        "    if grad_norm > max_norm:\n",
        "        # è¯·å°† ___________ æ›¿æ¢ä¸ºæ­£ç¡®çš„è£å‰ªå…¬å¼\n",
        "        gradient = gradient * (max_norm / grad_norm)  # æœŸæœ›ï¼šèŒƒæ•°ç¼©æ”¾åˆ° max_norm\n",
        "    \n",
        "    return gradient\n",
        "\n",
        "# æµ‹è¯•æ¢¯åº¦è£å‰ª\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ§ª æµ‹è¯•æ¢¯åº¦è£å‰ªåŠŸèƒ½\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_grad = torch.randn(10) * 5  # ç”Ÿæˆä¸€ä¸ªå¤§æ¢¯åº¦\n",
        "original_norm = torch.norm(test_grad).item()\n",
        "clipped_grad = clip_gradient(test_grad.clone(), max_norm=1.0)\n",
        "clipped_norm = torch.norm(clipped_grad).item()\n",
        "\n",
        "print(f\"  åŸå§‹æ¢¯åº¦èŒƒæ•°: {original_norm:.4f}\")\n",
        "print(f\"  è£å‰ªåèŒƒæ•°: {clipped_norm:.4f}\")\n",
        "print(f\"  è£å‰ªé˜ˆå€¼: 1.0\")\n",
        "\n",
        "if clipped_norm <= 1.001:  # å…è®¸å¾®å°çš„æ•°å€¼è¯¯å·®\n",
        "    print(\"  âœ“ æ¢¯åº¦è£å‰ªåŠŸèƒ½æ­£å¸¸ï¼\")\n",
        "else:\n",
        "    print(\"  âœ— æ¢¯åº¦è£å‰ªå¯èƒ½æœ‰è¯¯ï¼Œè¯·æ£€æŸ¥å¡«ç©º 1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== å¡«ç©º 2ï¼šå®ç°å™ªå£°æ·»åŠ æœºåˆ¶ ==========\n",
        "# \n",
        "# ğŸ¯ ä»»åŠ¡ï¼šåœ¨å¹³å‡æ¢¯åº¦ä¸Šæ·»åŠ é«˜æ–¯å™ªå£°ï¼Œå®ç°å·®åˆ†éšç§ä¿æŠ¤\n",
        "# \n",
        "# ğŸ’¡ æç¤ºï¼š\n",
        "#   - å™ªå£°æ ‡å‡†å·®è®¡ç®—å…¬å¼ï¼šnoise_std = (max_grad_norm * noise_multiplier) / n_samples\n",
        "#   - ä½¿ç”¨ torch.randn_like(tensor) ç”Ÿæˆä¸ tensor åŒå½¢çŠ¶çš„æ ‡å‡†æ­£æ€åˆ†å¸ƒéšæœºæ•°\n",
        "#   - å°†éšæœºæ•°ä¹˜ä»¥ noise_std å¾—åˆ°æœ€ç»ˆå™ªå£°\n",
        "# \n",
        "# ğŸ“– å‚è€ƒæ–‡æ¡£ï¼š\n",
        "#   - torch.randn_like(): ç”Ÿæˆä¸è¾“å…¥å¼ é‡åŒå½¢çŠ¶çš„æ­£æ€åˆ†å¸ƒéšæœºæ•°\n",
        "#   - å™ªå£°å¼ºåº¦ä¸éšç§é¢„ç®—æˆåæ¯”å…³ç³»\n",
        "#\n",
        "# éš¾åº¦ï¼šâ­â­â­â˜†â˜†\n",
        "\n",
        "def add_noise_to_gradient(avg_gradient, max_grad_norm, noise_multiplier, n_samples):\n",
        "    \"\"\"\n",
        "    åœ¨å¹³å‡æ¢¯åº¦ä¸Šæ·»åŠ é«˜æ–¯å™ªå£°\n",
        "    \n",
        "    å‚æ•°:\n",
        "        avg_gradient: å¹³å‡æ¢¯åº¦\n",
        "        max_grad_norm: æ¢¯åº¦è£å‰ªé˜ˆå€¼ C\n",
        "        noise_multiplier: å™ªå£°ä¹˜æ•° Ïƒï¼ˆæ§åˆ¶éšç§å¼ºåº¦ï¼‰\n",
        "        n_samples: æ ·æœ¬æ•°é‡\n",
        "    \n",
        "    è¿”å›:\n",
        "        æ·»åŠ å™ªå£°åçš„æ¢¯åº¦\n",
        "    \"\"\"\n",
        "    # æ­¥éª¤1ï¼šè®¡ç®—å™ªå£°æ ‡å‡†å·®\n",
        "    # æ ‡å‡†å·®éšæ ·æœ¬æ•°å¢åŠ è€Œå‡å°ï¼ˆæ ·æœ¬è¶Šå¤šï¼Œå•ä¸ªæ ·æœ¬å½±å“è¶Šå°ï¼‰\n",
        "    noise_std = (max_grad_norm * noise_multiplier) / n_samples\n",
        "    \n",
        "    # æ­¥éª¤2ï¼šç”Ÿæˆé«˜æ–¯å™ªå£°\n",
        "    # è¯·å°† ___________ æ›¿æ¢ä¸ºç”Ÿæˆå™ªå£°çš„ä»£ç \n",
        "    noise = torch.randn_like(avg_gradient) * noise_std  # æœŸæœ›ï¼šç”Ÿæˆ N(0, noise_stdÂ²) çš„å™ªå£°\n",
        "    \n",
        "    # æ­¥éª¤3ï¼šå°†å™ªå£°æ·»åŠ åˆ°æ¢¯åº¦ä¸Š\n",
        "    noisy_gradient = avg_gradient + noise\n",
        "    \n",
        "    return noisy_gradient\n",
        "\n",
        "# æµ‹è¯•å™ªå£°æ·»åŠ \n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ§ª æµ‹è¯•å™ªå£°æ·»åŠ åŠŸèƒ½\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_avg_grad = torch.ones(10) * 0.1\n",
        "noisy_grad = add_noise_to_gradient(test_avg_grad.clone(), max_grad_norm=1.0, \n",
        "                                   noise_multiplier=1.0, n_samples=100)\n",
        "\n",
        "noise_magnitude = torch.norm(noisy_grad - test_avg_grad).item()\n",
        "print(f\"  åŸå§‹æ¢¯åº¦å‡å€¼: {test_avg_grad.mean().item():.4f}\")\n",
        "print(f\"  æ·»åŠ å™ªå£°åå‡å€¼: {noisy_grad.mean().item():.4f}\")\n",
        "print(f\"  å™ªå£°å¹…åº¦: {noise_magnitude:.4f}\")\n",
        "\n",
        "if noise_magnitude > 0.001:  # å™ªå£°åº”è¯¥æ˜¯å¯è§‚æµ‹çš„\n",
        "    print(\"  âœ“ å™ªå£°æ·»åŠ åŠŸèƒ½æ­£å¸¸ï¼\")\n",
        "else:\n",
        "    print(\"  âœ— å™ªå£°å¯èƒ½æœªæ­£ç¡®æ·»åŠ ï¼Œè¯·æ£€æŸ¥å¡«ç©º 2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ¤” æ€è€ƒä¸€ä¸‹\n",
        "\n",
        "1. **ä¸ºä»€ä¹ˆéœ€è¦å…ˆè£å‰ªæ¢¯åº¦å†æ·»åŠ å™ªå£°ï¼Ÿ** å¦‚æœä¸è£å‰ªç›´æ¥æ·»åŠ å™ªå£°ä¼šæœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ\n",
        "   \n",
        "   <details>\n",
        "   <summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹æç¤º</summary>\n",
        "   \n",
        "   å¦‚æœä¸è£å‰ªï¼ŒæŸäº›æ ·æœ¬çš„æ¢¯åº¦å¯èƒ½éå¸¸å¤§ã€‚ä¸ºäº†æ©ç›–è¿™äº›å¤§æ¢¯åº¦ï¼Œéœ€è¦æ·»åŠ æ›´å¤§çš„å™ªå£°ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸¥é‡ä¸‹é™ã€‚è£å‰ªç¡®ä¿äº†æ‰€æœ‰æ¢¯åº¦åœ¨åŒä¸€å°ºåº¦ä¸Šï¼Œä»è€Œå¯ä»¥ç”¨è¾ƒå°çš„å™ªå£°å®ç°éšç§ä¿æŠ¤ã€‚\n",
        "   </details>\n",
        "\n",
        "2. **å™ªå£°æ ‡å‡†å·®ä¸ºä»€ä¹ˆé™¤ä»¥æ ·æœ¬æ•° n_samplesï¼Ÿ** è¿™ä¸éšç§ä¿æŠ¤æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ\n",
        "   \n",
        "   <details>\n",
        "   <summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹æç¤º</summary>\n",
        "   \n",
        "   æ ·æœ¬è¶Šå¤šï¼Œå•ä¸ªæ ·æœ¬å¯¹å¹³å‡æ¢¯åº¦çš„å½±å“è¶Šå°ï¼ˆç¨€é‡Šæ•ˆåº”ï¼‰ã€‚å› æ­¤éœ€è¦çš„å™ªå£°ä¹Ÿè¶Šå°ã€‚è¿™æ˜¯å·®åˆ†éšç§ç†è®ºä¸­çš„é‡è¦æ€§è´¨ï¼šæ›´å¤šæ•°æ®å¯ä»¥æä¾›æ›´å¥½çš„éšç§-æ•ˆç”¨æƒè¡¡ã€‚\n",
        "   </details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ‹ï¸ ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®ç°å·®åˆ†éšç§è®­ç»ƒ\n",
        "\n",
        "### å®ç°å®Œæ•´çš„ DP-SGD è®­ç»ƒæµç¨‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_with_dp(model, X_train, y_train, epochs=50, lr=0.01, \n",
        "                  max_grad_norm=1.0, noise_multiplier=1.0, verbose=True):\n",
        "    \"\"\"\n",
        "    ä½¿ç”¨å·®åˆ†éšç§è®­ç»ƒæ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆ DP-SGDï¼‰\n",
        "    \n",
        "    å‚æ•°:\n",
        "        model: PyTorch æ¨¡å‹\n",
        "        X_train, y_train: è®­ç»ƒæ•°æ®\n",
        "        epochs: è®­ç»ƒè½®æ•°\n",
        "        lr: å­¦ä¹ ç‡\n",
        "        max_grad_norm: æ¢¯åº¦è£å‰ªé˜ˆå€¼ C\n",
        "        noise_multiplier: å™ªå£°ä¹˜æ•° Ïƒ\n",
        "        verbose: æ˜¯å¦æ‰“å°è®­ç»ƒè¿‡ç¨‹\n",
        "    \n",
        "    è¿”å›:\n",
        "        losses: æ¯ä¸ª epoch çš„æŸå¤±å€¼åˆ—è¡¨\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    n_samples = len(X_train)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\nğŸ” å¼€å§‹å·®åˆ†éšç§è®­ç»ƒï¼ˆå™ªå£°å¼ºåº¦={noise_multiplier}ï¼‰\")\n",
        "        print(f\"  æ¢¯åº¦è£å‰ªé˜ˆå€¼: {max_grad_norm}\")\n",
        "        print(f\"  è®­ç»ƒæ ·æœ¬æ•°: {n_samples}\")\n",
        "        print(f\"  å­¦ä¹ ç‡: {lr}\\n\")\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        \n",
        "        # åˆå§‹åŒ–ç´¯ç§¯æ¢¯åº¦å­—å…¸\n",
        "        accumulated_grads = {name: torch.zeros_like(param) \n",
        "                            for name, param in model.named_parameters()}\n",
        "        \n",
        "        # å¯¹æ¯ä¸ªæ ·æœ¬å•ç‹¬è®¡ç®—æ¢¯åº¦ï¼ˆper-sample gradientï¼‰\n",
        "        for i in range(n_samples):\n",
        "            # å‰å‘ä¼ æ’­\n",
        "            output = model(X_train[i:i+1])\n",
        "            loss = criterion(output, y_train[i:i+1])\n",
        "            \n",
        "            # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦\n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "            \n",
        "            # å¯¹æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦è¿›è¡Œè£å‰ªå¹¶ç´¯åŠ \n",
        "            for name, param in model.named_parameters():\n",
        "                if param.grad is not None:\n",
        "                    # è£å‰ªå•ä¸ªæ ·æœ¬çš„æ¢¯åº¦\n",
        "                    clipped_grad = clip_gradient(param.grad.clone(), max_grad_norm)\n",
        "                    # ç´¯åŠ åˆ°æ€»æ¢¯åº¦\n",
        "                    accumulated_grads[name] += clipped_grad\n",
        "        \n",
        "        # è®¡ç®—å¹³å‡æ¢¯åº¦å¹¶æ·»åŠ å™ªå£°\n",
        "        for name, param in model.named_parameters():\n",
        "            # è®¡ç®—å¹³å‡æ¢¯åº¦\n",
        "            avg_grad = accumulated_grads[name] / n_samples\n",
        "            \n",
        "            # æ·»åŠ é«˜æ–¯å™ªå£°\n",
        "            noisy_grad = add_noise_to_gradient(\n",
        "                avg_grad, max_grad_norm, noise_multiplier, n_samples\n",
        "            )\n",
        "            \n",
        "            # ä½¿ç”¨å¸¦å™ªå£°çš„æ¢¯åº¦æ›´æ–°å‚æ•°\n",
        "            param.data -= lr * noisy_grad\n",
        "        \n",
        "        # è®¡ç®—å½“å‰ epoch çš„æŸå¤±å’Œå‡†ç¡®ç‡\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = model(X_train)\n",
        "            epoch_loss = criterion(outputs, y_train).item()\n",
        "            epoch_acc = (outputs.argmax(dim=1) == y_train).float().mean().item()\n",
        "            losses.append(epoch_loss)\n",
        "            accuracies.append(epoch_acc)\n",
        "        \n",
        "        # æ‰“å°è®­ç»ƒè¿›åº¦\n",
        "        if verbose and (epoch + 1) % 10 == 0:\n",
        "            print(f\"  Epoch [{epoch+1:2d}/{epochs}] - Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.2%}\")\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"  âœ“ è®­ç»ƒå®Œæˆï¼\\n\")\n",
        "    \n",
        "    return losses, accuracies\n",
        "\n",
        "print(\"âœ“ å·®åˆ†éšç§è®­ç»ƒå‡½æ•°å®šä¹‰å®Œæˆï¼\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### å®ç°æ™®é€šè®­ç»ƒå‡½æ•°ï¼ˆç”¨äºå¯¹æ¯”ï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_normal(model, X_train, y_train, epochs=50, lr=0.01, verbose=True):\n",
        "    \"\"\"\n",
        "    æ™®é€šè®­ç»ƒï¼ˆæ— éšç§ä¿æŠ¤ï¼‰\n",
        "    \n",
        "    å‚æ•°:\n",
        "        model: PyTorch æ¨¡å‹\n",
        "        X_train, y_train: è®­ç»ƒæ•°æ®\n",
        "        epochs: è®­ç»ƒè½®æ•°\n",
        "        lr: å­¦ä¹ ç‡\n",
        "        verbose: æ˜¯å¦æ‰“å°è®­ç»ƒè¿‡ç¨‹\n",
        "    \n",
        "    è¿”å›:\n",
        "        losses: æ¯ä¸ª epoch çš„æŸå¤±å€¼åˆ—è¡¨\n",
        "    \"\"\"\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\nğŸ”“ å¼€å§‹æ™®é€šè®­ç»ƒï¼ˆæ— éšç§ä¿æŠ¤ï¼‰\")\n",
        "        print(f\"  å­¦ä¹ ç‡: {lr}\\n\")\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        \n",
        "        # æ ‡å‡†çš„æ‰¹é‡æ¢¯åº¦ä¸‹é™\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # è®°å½•æŸå¤±å’Œå‡†ç¡®ç‡\n",
        "        losses.append(loss.item())\n",
        "        acc = (outputs.argmax(dim=1) == y_train).float().mean().item()\n",
        "        accuracies.append(acc)\n",
        "        \n",
        "        # æ‰“å°è®­ç»ƒè¿›åº¦\n",
        "        if verbose and (epoch + 1) % 10 == 0:\n",
        "            print(f\"  Epoch [{epoch+1:2d}/{epochs}] - Loss: {loss.item():.4f} | Acc: {acc:.2%}\")\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"  âœ“ è®­ç»ƒå®Œæˆï¼\\n\")\n",
        "    \n",
        "    return losses, accuracies\n",
        "\n",
        "print(\"âœ“ æ™®é€šè®­ç»ƒå‡½æ•°å®šä¹‰å®Œæˆï¼\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### è®­ç»ƒä¸¤ä¸ªæ¨¡å‹è¿›è¡Œå¯¹æ¯”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# è®­ç»ƒæ™®é€šæ¨¡å‹\n",
        "normal_model = SimpleClassifier()\n",
        "normal_losses, normal_accs = train_normal(normal_model, X_train, y_train, epochs=50, lr=0.01)\n",
        "\n",
        "# è®­ç»ƒå·®åˆ†éšç§æ¨¡å‹\n",
        "dp_model = SimpleClassifier()\n",
        "dp_losses, dp_accs = train_with_dp(dp_model, X_train, y_train, epochs=50, lr=0.01,\n",
        "                                    max_grad_norm=1.0, noise_multiplier=1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç»˜åˆ¶è®­ç»ƒæŸå¤±å’Œå‡†ç¡®ç‡å¯¹æ¯”\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# æŸå¤±æ›²çº¿\n",
        "axes[0].plot(normal_losses, label='æ™®é€šè®­ç»ƒ', color='steelblue', linewidth=2)\n",
        "axes[0].plot(dp_losses, label='å·®åˆ†éšç§è®­ç»ƒ (Ïƒ=1.0)', color='coral', linewidth=2)\n",
        "axes[0].set_xlabel('è®­ç»ƒè½®æ•° (Epoch)', fontsize=12)\n",
        "axes[0].set_ylabel('æŸå¤±å€¼ (Loss)', fontsize=12)\n",
        "axes[0].set_title('ğŸ“‰ è®­ç»ƒæŸå¤±å¯¹æ¯”', fontsize=13, fontweight='bold')\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# å‡†ç¡®ç‡æ›²çº¿\n",
        "axes[1].plot(normal_accs, label='æ™®é€šè®­ç»ƒ', color='steelblue', linewidth=2)\n",
        "axes[1].plot(dp_accs, label='å·®åˆ†éšç§è®­ç»ƒ (Ïƒ=1.0)', color='coral', linewidth=2)\n",
        "axes[1].set_xlabel('è®­ç»ƒè½®æ•° (Epoch)', fontsize=12)\n",
        "axes[1].set_ylabel('å‡†ç¡®ç‡ (Accuracy)', fontsize=12)\n",
        "axes[1].set_title('ğŸ“ˆ è®­ç»ƒå‡†ç¡®ç‡å¯¹æ¯”', fontsize=13, fontweight='bold')\n",
        "axes[1].legend(fontsize=11)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nğŸ“Š è§‚å¯Ÿï¼š\")\n",
        "print(\"  - æ™®é€šæ¨¡å‹æ”¶æ•›æ›´å¿«ï¼ŒæŸå¤±ä¸‹é™æ›´æ˜æ˜¾\")\n",
        "print(\"  - DP æ¨¡å‹ç”±äºå™ªå£°å½±å“ï¼Œè®­ç»ƒæ›²çº¿æ›´åŠ æ³¢åŠ¨\")\n",
        "print(\"  - è¿™æ˜¯éšç§ä¿æŠ¤çš„ä»£ä»·ï¼šå™ªå£°å¹²æ‰°äº†æ­£å¸¸çš„æ¢¯åº¦ä¸‹é™\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### âœ… æ£€æŸ¥ç‚¹ 2\n",
        "\n",
        "è¿è¡Œåˆ°è¿™é‡Œï¼Œä½ åº”è¯¥çœ‹åˆ°ï¼š\n",
        "- [x] ä¸¤ä¸ªæ¨¡å‹éƒ½æˆåŠŸå®Œæˆ 50 è½®è®­ç»ƒ\n",
        "- [x] æ™®é€šæ¨¡å‹çš„è®­ç»ƒå‡†ç¡®ç‡é€šå¸¸åœ¨ 85% ä»¥ä¸Š\n",
        "- [x] DP æ¨¡å‹çš„è®­ç»ƒå‡†ç¡®ç‡ç•¥ä½äºæ™®é€šæ¨¡å‹ï¼ˆçº¦ 75-85%ï¼‰\n",
        "- [x] DP æ¨¡å‹çš„æŸå¤±æ›²çº¿æ›´åŠ æ³¢åŠ¨ï¼ˆå™ªå£°å¯¼è‡´ï¼‰\n",
        "\n",
        "å¦‚æœ DP æ¨¡å‹å‡†ç¡®ç‡è¿‡ä½ï¼ˆ<60%ï¼‰ï¼Œå¯èƒ½éœ€è¦ï¼š\n",
        "- å‡å° noise_multiplierï¼ˆé™ä½å™ªå£°å¼ºåº¦ï¼‰\n",
        "- å¢åŠ è®­ç»ƒè½®æ•°\n",
        "- è°ƒæ•´å­¦ä¹ ç‡"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“Š ç¬¬å››éƒ¨åˆ†ï¼šè¯„ä¼°æ¨¡å‹æ€§èƒ½ä¸éšç§ä¿æŠ¤\n",
        "\n",
        "### è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ³›åŒ–æ€§èƒ½"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== å¡«ç©º 3ï¼šå®ç°æ¨¡å‹è¯„ä¼°å‡½æ•° ==========\n",
        "# \n",
        "# ğŸ¯ ä»»åŠ¡ï¼šè®¡ç®—æ¨¡å‹åœ¨ç»™å®šæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡\n",
        "# \n",
        "# ğŸ’¡ æç¤ºï¼š\n",
        "#   - ä½¿ç”¨ model.eval() åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼\n",
        "#   - ä½¿ç”¨ torch.no_grad() ç¦ç”¨æ¢¯åº¦è®¡ç®—\n",
        "#   - outputs.argmax(dim=1) è·å–é¢„æµ‹ç±»åˆ«\n",
        "#   - (pred == y).float().mean() è®¡ç®—å‡†ç¡®ç‡\n",
        "# \n",
        "# éš¾åº¦ï¼šâ­â­â˜†â˜†â˜†\n",
        "\n",
        "def evaluate_model(model, X, y):\n",
        "    \"\"\"\n",
        "    è¯„ä¼°æ¨¡å‹åœ¨ç»™å®šæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡\n",
        "    \n",
        "    å‚æ•°:\n",
        "        model: PyTorch æ¨¡å‹\n",
        "        X, y: è¯„ä¼°æ•°æ®\n",
        "    \n",
        "    è¿”å›:\n",
        "        accuracy: å‡†ç¡®ç‡ï¼ˆ0-1 ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼‰\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X)\n",
        "        predictions = outputs.argmax(dim=1)\n",
        "        \n",
        "        # è¯·å°† ___________ æ›¿æ¢ä¸ºè®¡ç®—å‡†ç¡®ç‡çš„ä»£ç \n",
        "        accuracy = (predictions == y).float().mean().item()  # æœŸæœ›ï¼šè¿”å› 0-1 ä¹‹é—´çš„å‡†ç¡®ç‡\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "# è¯„ä¼°ä¸¤ä¸ªæ¨¡å‹\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ“Š æ¨¡å‹æ€§èƒ½è¯„ä¼°\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# æ™®é€šæ¨¡å‹\n",
        "normal_train_acc = evaluate_model(normal_model, X_train, y_train)\n",
        "normal_test_acc = evaluate_model(normal_model, X_test, y_test)\n",
        "\n",
        "# DP æ¨¡å‹\n",
        "dp_train_acc = evaluate_model(dp_model, X_train, y_train)\n",
        "dp_test_acc = evaluate_model(dp_model, X_test, y_test)\n",
        "\n",
        "print(f\"\\n  {'æ¨¡å‹ç±»å‹':<15} {'è®­ç»ƒå‡†ç¡®ç‡':<15} {'æµ‹è¯•å‡†ç¡®ç‡':<15}\")\n",
        "print(\"  \" + \"-\"*45)\n",
        "print(f\"  {'æ™®é€šæ¨¡å‹':<15} {normal_train_acc:>13.2%} {normal_test_acc:>13.2%}\")\n",
        "print(f\"  {'DPæ¨¡å‹ (Ïƒ=1.0)':<15} {dp_train_acc:>13.2%} {dp_test_acc:>13.2%}\")\n",
        "print(\"\\n  ğŸ“‰ æ€§èƒ½ä¸‹é™ï¼š\")\n",
        "print(f\"     è®­ç»ƒé›†: {(normal_train_acc - dp_train_acc):.2%}\")\n",
        "print(f\"     æµ‹è¯•é›†: {(normal_test_acc - dp_test_acc):.2%}\")\n",
        "\n",
        "if (normal_test_acc - dp_test_acc) < 0.15:\n",
        "    print(\"\\n  âœ“ æ€§èƒ½ä¸‹é™åœ¨å¯æ¥å—èŒƒå›´å†…ï¼ˆ<15%ï¼‰\")\n",
        "else:\n",
        "    print(\"\\n  âš  æ€§èƒ½ä¸‹é™è¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´éšç§å‚æ•°\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### æµ‹è¯•å¯¹æˆå‘˜æ¨ç†æ”»å‡»çš„é˜²å¾¡æ•ˆæœ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== å¡«ç©º 4ï¼šå®ç°æˆå‘˜æ¨ç†æ”»å‡» ==========\n",
        "# \n",
        "# ğŸ¯ ä»»åŠ¡ï¼šåŸºäºæ¨¡å‹ç½®ä¿¡åº¦æ‰§è¡Œæˆå‘˜æ¨ç†æ”»å‡»ï¼Œè¯„ä¼°éšç§æ³„éœ²é£é™©\n",
        "# \n",
        "# ğŸ’¡ æç¤ºï¼š\n",
        "#   - æˆå‘˜æ ·æœ¬é€šå¸¸æœ‰æ›´é«˜çš„é¢„æµ‹ç½®ä¿¡åº¦\n",
        "#   - ä½¿ç”¨ torch.softmax() å°† logits è½¬æ¢ä¸ºæ¦‚ç‡\n",
        "#   - gather() å‡½æ•°å¯ä»¥æå–æ­£ç¡®ç±»åˆ«çš„æ¦‚ç‡\n",
        "#   - ä½¿ç”¨é˜ˆå€¼åˆ¤æ–­ï¼šç½®ä¿¡åº¦ > é˜ˆå€¼ â†’ åˆ¤å®šä¸ºæˆå‘˜\n",
        "# \n",
        "# ğŸ“– æ ¸å¿ƒæ€æƒ³ï¼š\n",
        "#   å¦‚æœæ¨¡å‹è®°ä½äº†è®­ç»ƒæ•°æ®ï¼Œå¯¹è®­ç»ƒæ ·æœ¬çš„é¢„æµ‹ä¼šæ›´è‡ªä¿¡\n",
        "#\n",
        "# éš¾åº¦ï¼šâ­â­â­â˜†â˜†\n",
        "\n",
        "def membership_inference_attack(model, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    æ‰§è¡ŒåŸºäºç½®ä¿¡åº¦çš„æˆå‘˜æ¨ç†æ”»å‡»\n",
        "    \n",
        "    å‚æ•°:\n",
        "        model: ç›®æ ‡æ¨¡å‹\n",
        "        X_train, y_train: è®­ç»ƒé›†ï¼ˆæˆå‘˜æ ·æœ¬ï¼‰\n",
        "        X_test, y_test: æµ‹è¯•é›†ï¼ˆéæˆå‘˜æ ·æœ¬ï¼‰\n",
        "    \n",
        "    è¿”å›:\n",
        "        attack_acc: æ”»å‡»å‡†ç¡®ç‡\n",
        "        train_conf: è®­ç»ƒæ ·æœ¬çš„ç½®ä¿¡åº¦æ•°ç»„\n",
        "        test_conf: æµ‹è¯•æ ·æœ¬çš„ç½®ä¿¡åº¦æ•°ç»„\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # æ­¥éª¤1ï¼šè·å–æ¨¡å‹åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„è¾“å‡ºæ¦‚ç‡\n",
        "        # è¯·å°† ___________ æ›¿æ¢ä¸ºè®¡ç®— softmax æ¦‚ç‡çš„ä»£ç \n",
        "        train_probs = torch.softmax(model(X_train), dim=1)  # æœŸæœ›ï¼šå½¢çŠ¶ [n_train, n_classes]\n",
        "        test_probs = torch.softmax(model(X_test), dim=1)    # æœŸæœ›ï¼šå½¢çŠ¶ [n_test, n_classes]\n",
        "        \n",
        "        # æ­¥éª¤2ï¼šæå–æ­£ç¡®ç±»åˆ«çš„ç½®ä¿¡åº¦ï¼ˆçœŸå®æ ‡ç­¾å¯¹åº”çš„æ¦‚ç‡ï¼‰\n",
        "        train_conf = train_probs.gather(1, y_train.unsqueeze(1)).squeeze().cpu().numpy()\n",
        "        test_conf = test_probs.gather(1, y_test.unsqueeze(1)).squeeze().cpu().numpy()\n",
        "    \n",
        "    # æ­¥éª¤3ï¼šè®¾ç½®æ”»å‡»é˜ˆå€¼ï¼ˆä½¿ç”¨ä¸¤ç»„æ•°æ®çš„ä¸­ä½æ•°ç½®ä¿¡åº¦çš„å¹³å‡å€¼ï¼‰\n",
        "    threshold = (np.median(train_conf) + np.median(test_conf)) / 2\n",
        "    \n",
        "    # æ­¥éª¤4ï¼šæ‰§è¡Œæ”»å‡»\n",
        "    # è§„åˆ™ï¼šå¦‚æœç½®ä¿¡åº¦ > é˜ˆå€¼ï¼Œåˆ¤å®šä¸ºæˆå‘˜ï¼›å¦åˆ™åˆ¤å®šä¸ºéæˆå‘˜\n",
        "    member_correct = np.mean(train_conf > threshold)      # æˆå‘˜æ ·æœ¬è¢«æ­£ç¡®è¯†åˆ«çš„æ¯”ä¾‹\n",
        "    non_member_correct = np.mean(test_conf <= threshold)  # éæˆå‘˜æ ·æœ¬è¢«æ­£ç¡®è¯†åˆ«çš„æ¯”ä¾‹\n",
        "    \n",
        "    # æ”»å‡»æ€»å‡†ç¡®ç‡\n",
        "    attack_acc = (member_correct + non_member_correct) / 2\n",
        "    \n",
        "    return attack_acc, train_conf, test_conf, threshold\n",
        "\n",
        "# å¯¹ä¸¤ä¸ªæ¨¡å‹æ‰§è¡Œæˆå‘˜æ¨ç†æ”»å‡»\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ¯ æˆå‘˜æ¨ç†æ”»å‡»è¯„ä¼°\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "normal_attack_acc, normal_train_conf, normal_test_conf, normal_threshold = \\\n",
        "    membership_inference_attack(normal_model, X_train, y_train, X_test, y_test)\n",
        "\n",
        "dp_attack_acc, dp_train_conf, dp_test_conf, dp_threshold = \\\n",
        "    membership_inference_attack(dp_model, X_train, y_train, X_test, y_test)\n",
        "\n",
        "print(f\"\\n  {'æ¨¡å‹ç±»å‹':<15} {'æ”»å‡»å‡†ç¡®ç‡':<15} {'åˆ¤æ–­é˜ˆå€¼':<15}\")\n",
        "print(\"  \" + \"-\"*45)\n",
        "print(f\"  {'æ™®é€šæ¨¡å‹':<15} {normal_attack_acc:>13.2%} {normal_threshold:>13.4f}\")\n",
        "print(f\"  {'DPæ¨¡å‹ (Ïƒ=1.0)':<15} {dp_attack_acc:>13.2%} {dp_threshold:>13.4f}\")\n",
        "\n",
        "defense_improvement = normal_attack_acc - dp_attack_acc\n",
        "print(f\"\\n  ğŸ›¡ï¸ é˜²å¾¡æ•ˆæœ: æ”»å‡»å‡†ç¡®ç‡é™ä½ {defense_improvement:.2%}\")\n",
        "\n",
        "if dp_attack_acc < 0.55:\n",
        "    print(\"  âœ“ å·®åˆ†éšç§æ˜¾è‘—é™ä½äº†éšç§æ³„éœ²é£é™©ï¼\")\n",
        "elif dp_attack_acc < 0.65:\n",
        "    print(\"  âš  å·®åˆ†éšç§æä¾›äº†ä¸€å®šä¿æŠ¤ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´\")\n",
        "else:\n",
        "    print(\"  âœ— å½“å‰éšç§å‚æ•°å¯èƒ½ä¸è¶³ï¼Œå»ºè®®å¢åŠ å™ªå£°å¼ºåº¦\")\n",
        "\n",
        "print(f\"\\n  ğŸ’¡ å‚è€ƒï¼šéšæœºçŒœæµ‹çš„æ”»å‡»å‡†ç¡®ç‡ä¸º 50%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### å¯è§†åŒ–æˆå‘˜ä¸éæˆå‘˜çš„ç½®ä¿¡åº¦åˆ†å¸ƒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç»˜åˆ¶ç½®ä¿¡åº¦åˆ†å¸ƒå¯¹æ¯”å›¾\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# æ™®é€šæ¨¡å‹çš„ç½®ä¿¡åº¦åˆ†å¸ƒ\n",
        "axes[0].hist(normal_train_conf, bins=30, alpha=0.6, label='æˆå‘˜ï¼ˆè®­ç»ƒé›†ï¼‰', \n",
        "             color='#FF6B6B', edgecolor='black')\n",
        "axes[0].hist(normal_test_conf, bins=30, alpha=0.6, label='éæˆå‘˜ï¼ˆæµ‹è¯•é›†ï¼‰', \n",
        "             color='#4ECDC4', edgecolor='black')\n",
        "axes[0].axvline(normal_threshold, color='red', linestyle='--', linewidth=2, \n",
        "                label=f'æ”»å‡»é˜ˆå€¼={normal_threshold:.3f}')\n",
        "axes[0].set_xlabel('é¢„æµ‹ç½®ä¿¡åº¦', fontsize=12)\n",
        "axes[0].set_ylabel('æ ·æœ¬æ•°é‡', fontsize=12)\n",
        "axes[0].set_title(f'æ™®é€šæ¨¡å‹\\næ”»å‡»å‡†ç¡®ç‡: {normal_attack_acc:.1%}', \n",
        "                  fontsize=13, fontweight='bold')\n",
        "axes[0].legend(fontsize=10)\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# DP æ¨¡å‹çš„ç½®ä¿¡åº¦åˆ†å¸ƒ\n",
        "axes[1].hist(dp_train_conf, bins=30, alpha=0.6, label='æˆå‘˜ï¼ˆè®­ç»ƒé›†ï¼‰', \n",
        "             color='#FF6B6B', edgecolor='black')\n",
        "axes[1].hist(dp_test_conf, bins=30, alpha=0.6, label='éæˆå‘˜ï¼ˆæµ‹è¯•é›†ï¼‰', \n",
        "             color='#4ECDC4', edgecolor='black')\n",
        "axes[1].axvline(dp_threshold, color='red', linestyle='--', linewidth=2, \n",
        "                label=f'æ”»å‡»é˜ˆå€¼={dp_threshold:.3f}')\n",
        "axes[1].set_xlabel('é¢„æµ‹ç½®ä¿¡åº¦', fontsize=12)\n",
        "axes[1].set_ylabel('æ ·æœ¬æ•°é‡', fontsize=12)\n",
        "axes[1].set_title(f'å·®åˆ†éšç§æ¨¡å‹ (Ïƒ=1.0)\\næ”»å‡»å‡†ç¡®ç‡: {dp_attack_acc:.1%}', \n",
        "                  fontsize=13, fontweight='bold')\n",
        "axes[1].legend(fontsize=10)\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.suptitle('ğŸ” æˆå‘˜ vs éæˆå‘˜ç½®ä¿¡åº¦åˆ†å¸ƒå¯¹æ¯”', fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nğŸ“Š åˆ†æï¼š\")\n",
        "print(\"  - æ™®é€šæ¨¡å‹ï¼šæˆå‘˜å’Œéæˆå‘˜çš„ç½®ä¿¡åº¦åˆ†å¸ƒå·®å¼‚æ˜æ˜¾ï¼Œæ˜“äºåŒºåˆ†\")\n",
        "print(\"  - DP æ¨¡å‹ï¼šä¸¤ä¸ªåˆ†å¸ƒæ›´åŠ é‡å ï¼Œæ”»å‡»è€…éš¾ä»¥åˆ¤æ–­æ ·æœ¬æ˜¯å¦ä¸ºæˆå‘˜\")\n",
        "print(\"  - å·®åˆ†éšç§é€šè¿‡å™ªå£°æ¨¡ç³Šäº†æˆå‘˜å’Œéæˆå‘˜çš„è¾¹ç•Œï¼Œæå‡äº†éšç§ä¿æŠ¤\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ¤” æ€è€ƒä¸€ä¸‹\n",
        "\n",
        "1. **ä¸ºä»€ä¹ˆæˆå‘˜æ ·æœ¬é€šå¸¸æœ‰æ›´é«˜çš„ç½®ä¿¡åº¦ï¼Ÿ** è¿™ä¸æ¨¡å‹çš„ä»€ä¹ˆç‰¹æ€§æœ‰å…³ï¼Ÿ\n",
        "   \n",
        "   <details>\n",
        "   <summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹æç¤º</summary>\n",
        "   \n",
        "   æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ã€Œè§è¿‡ã€å¹¶ã€Œè®°ä½ã€äº†è®­ç»ƒæ ·æœ¬ï¼Œå› æ­¤å¯¹è¿™äº›æ ·æœ¬çš„é¢„æµ‹ä¼šæ›´åŠ è‡ªä¿¡ã€‚è¿™ç§ç°è±¡ç§°ä¸ºã€Œè¿‡æ‹Ÿåˆã€æˆ–ã€Œè®°å¿†åŒ–ã€ã€‚å·®åˆ†éšç§é€šè¿‡æ·»åŠ å™ªå£°ï¼Œå‰Šå¼±äº†æ¨¡å‹å¯¹å•ä¸ªæ ·æœ¬çš„è®°å¿†èƒ½åŠ›ã€‚\n",
        "   </details>\n",
        "\n",
        "2. **å¦‚æœæ”»å‡»å‡†ç¡®ç‡æ¥è¿‘ 50%ï¼Œè¯´æ˜ä»€ä¹ˆï¼Ÿ** è¿™å¯¹éšç§ä¿æŠ¤æ„å‘³ç€ä»€ä¹ˆï¼Ÿ\n",
        "   \n",
        "   <details>\n",
        "   <summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹æç¤º</summary>\n",
        "   \n",
        "   50% ç­‰ä»·äºéšæœºçŒœæµ‹çš„å‡†ç¡®ç‡ï¼Œè¯´æ˜æ”»å‡»è€…æ— æ³•ä»æ¨¡å‹è¾“å‡ºä¸­è·å¾—å…³äºæ ·æœ¬æ˜¯å¦åœ¨è®­ç»ƒé›†ä¸­çš„æœ‰æ•ˆä¿¡æ¯ã€‚è¿™æ˜¯ç†æƒ³çš„éšç§ä¿æŠ¤çŠ¶æ€ï¼\n",
        "   </details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš–ï¸ ç¬¬äº”éƒ¨åˆ†ï¼šéšç§-æ•ˆç”¨æƒè¡¡åˆ†æ\n",
        "\n",
        "### æ¢ç´¢ä¸åŒå™ªå£°å¼ºåº¦çš„å½±å“"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== å¡«ç©º 5ï¼šéšç§-æ•ˆç”¨æƒè¡¡å®éªŒ ==========\n",
        "# \n",
        "# ğŸ¯ ä»»åŠ¡ï¼šæµ‹è¯•ä¸åŒå™ªå£°å¼ºåº¦ä¸‹çš„æ¨¡å‹æ€§èƒ½å’Œéšç§ä¿æŠ¤æ•ˆæœ\n",
        "# \n",
        "# ğŸ’¡ æç¤ºï¼š\n",
        "#   - noise_multiplier=0 ç›¸å½“äºæ™®é€šè®­ç»ƒï¼ˆæ— éšç§ä¿æŠ¤ï¼‰\n",
        "#   - noise_multiplier è¶Šå¤§ï¼Œéšç§ä¿æŠ¤è¶Šå¼ºï¼Œä½†æ¨¡å‹æ€§èƒ½è¶Šå·®\n",
        "#   - éœ€è¦åœ¨å¾ªç¯ä¸­è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼Œæ”¶é›†æ€§èƒ½å’Œæ”»å‡»å‡†ç¡®ç‡æ•°æ®\n",
        "# \n",
        "# ğŸ“– å®éªŒç›®æ ‡ï¼š\n",
        "#   æ‰¾åˆ°éšç§ä¿æŠ¤ä¸æ¨¡å‹æ•ˆç”¨ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ç‚¹\n",
        "#\n",
        "# éš¾åº¦ï¼šâ­â­â­â­â˜†\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"âš–ï¸ éšç§-æ•ˆç”¨æƒè¡¡åˆ†æ\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\næµ‹è¯•ä¸åŒå™ªå£°å¼ºåº¦çš„å½±å“...\\n\")\n",
        "\n",
        "# å®šä¹‰è¦æµ‹è¯•çš„å™ªå£°å¼ºåº¦èŒƒå›´\n",
        "noise_multipliers = [0.0, 0.5, 1.0, 1.5, 2.0, 3.0]\n",
        "results = []\n",
        "\n",
        "for i, noise_mult in enumerate(noise_multipliers):\n",
        "    print(f\"[{i+1}/{len(noise_multipliers)}] è®­ç»ƒæ¨¡å‹ (å™ªå£°å¼ºåº¦={noise_mult})...\")\n",
        "    \n",
        "    # åˆ›å»ºæ–°æ¨¡å‹\n",
        "    temp_model = SimpleClassifier()\n",
        "    \n",
        "    # æ ¹æ®å™ªå£°å¼ºåº¦é€‰æ‹©è®­ç»ƒæ–¹å¼\n",
        "    if noise_mult == 0:\n",
        "        # noise_mult=0 è¡¨ç¤ºæ™®é€šè®­ç»ƒ\n",
        "        # è¯·å°† ___________ æ›¿æ¢ä¸ºè°ƒç”¨æ™®é€šè®­ç»ƒå‡½æ•°çš„ä»£ç \n",
        "        train_normal(temp_model, X_train, y_train, epochs=50, lr=0.01, verbose=False)\n",
        "    else:\n",
        "        # noise_mult>0 è¡¨ç¤ºå·®åˆ†éšç§è®­ç»ƒ\n",
        "        # è¯·å°† ___________ æ›¿æ¢ä¸ºè°ƒç”¨ DP è®­ç»ƒå‡½æ•°çš„ä»£ç \n",
        "        train_with_dp(temp_model, X_train, y_train, epochs=50, lr=0.01,\n",
        "                     max_grad_norm=1.0, noise_multiplier=noise_mult, verbose=False)\n",
        "    \n",
        "    # è¯„ä¼°æ¨¡å‹æ€§èƒ½\n",
        "    test_acc = evaluate_model(temp_model, X_test, y_test)\n",
        "    \n",
        "    # è¯„ä¼°éšç§æ³„éœ²é£é™©ï¼ˆæˆå‘˜æ¨ç†æ”»å‡»ï¼‰\n",
        "    attack_acc, _, _, _ = membership_inference_attack(\n",
        "        temp_model, X_train, y_train, X_test, y_test\n",
        "    )\n",
        "    \n",
        "    # è®°å½•ç»“æœ\n",
        "    results.append({\n",
        "        'noise': noise_mult,\n",
        "        'test_acc': test_acc,\n",
        "        'attack_acc': attack_acc,\n",
        "        'privacy_gain': normal_attack_acc - attack_acc,  # ç›¸å¯¹æ™®é€šæ¨¡å‹çš„éšç§æå‡\n",
        "        'utility_loss': normal_test_acc - test_acc        # ç›¸å¯¹æ™®é€šæ¨¡å‹çš„æ€§èƒ½æŸå¤±\n",
        "    })\n",
        "    \n",
        "    print(f\"  æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2%} | æ”»å‡»å‡†ç¡®ç‡: {attack_acc:.2%}\\n\")\n",
        "\n",
        "print(\"âœ“ å®éªŒå®Œæˆï¼\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### å¯è§†åŒ–éšç§-æ•ˆç”¨æƒè¡¡æ›²çº¿"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æå–æ•°æ®\n",
        "noise_list = [r['noise'] for r in results]\n",
        "test_acc_list = [r['test_acc'] for r in results]\n",
        "attack_acc_list = [r['attack_acc'] for r in results]\n",
        "\n",
        "# åˆ›å»ºåŒ Y è½´å›¾è¡¨\n",
        "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# å·¦ä¾§ Y è½´ï¼šæ¨¡å‹æµ‹è¯•å‡†ç¡®ç‡ï¼ˆæ•ˆç”¨ï¼‰\n",
        "color1 = '#4ECDC4'\n",
        "ax1.set_xlabel('å™ªå£°å¼ºåº¦ (noise_multiplier Ïƒ)', fontsize=13, fontweight='bold')\n",
        "ax1.set_ylabel('æ¨¡å‹æµ‹è¯•å‡†ç¡®ç‡ï¼ˆæ•ˆç”¨ï¼‰', color=color1, fontsize=13, fontweight='bold')\n",
        "line1 = ax1.plot(noise_list, test_acc_list, 'o-', color=color1, linewidth=3, \n",
        "                 markersize=10, label='æµ‹è¯•å‡†ç¡®ç‡')\n",
        "ax1.tick_params(axis='y', labelcolor=color1)\n",
        "ax1.set_ylim([0.4, 1.0])\n",
        "ax1.grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "# å³ä¾§ Y è½´ï¼šæˆå‘˜æ¨ç†æ”»å‡»å‡†ç¡®ç‡ï¼ˆéšç§é£é™©ï¼‰\n",
        "ax2 = ax1.twinx()\n",
        "color2 = '#FF6B6B'\n",
        "ax2.set_ylabel('æˆå‘˜æ¨ç†æ”»å‡»å‡†ç¡®ç‡ï¼ˆéšç§é£é™©ï¼‰', color=color2, fontsize=13, fontweight='bold')\n",
        "line2 = ax2.plot(noise_list, attack_acc_list, 's--', color=color2, linewidth=3, \n",
        "                 markersize=10, label='æ”»å‡»å‡†ç¡®ç‡')\n",
        "ax2.tick_params(axis='y', labelcolor=color2)\n",
        "ax2.set_ylim([0.4, 1.0])\n",
        "\n",
        "# æ·»åŠ éšæœºçŒœæµ‹åŸºå‡†çº¿\n",
        "ax2.axhline(y=0.5, color='gray', linestyle=':', linewidth=2, label='éšæœºçŒœæµ‹ (50%)')\n",
        "\n",
        "# æ·»åŠ å›¾ä¾‹\n",
        "lines = line1 + line2\n",
        "labels = [l.get_label() for l in lines] + ['éšæœºçŒœæµ‹ (50%)']\n",
        "ax1.legend(lines + [ax2.lines[-1]], labels, loc='center left', fontsize=11, \n",
        "          bbox_to_anchor=(0.02, 0.5))\n",
        "\n",
        "plt.title('âš–ï¸ éšç§-æ•ˆç”¨æƒè¡¡ï¼šå™ªå£°å¼ºåº¦çš„å½±å“', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nğŸ“ˆ å…³é”®è§‚å¯Ÿï¼š\")\n",
        "print(\"  1. å™ªå£°å¼ºåº¦ â†‘ â†’ æ¨¡å‹æ•ˆç”¨ â†“ï¼ˆè“è‰²æ›²çº¿ä¸‹é™ï¼‰\")\n",
        "print(\"  2. å™ªå£°å¼ºåº¦ â†‘ â†’ éšç§é£é™© â†“ï¼ˆçº¢è‰²æ›²çº¿ä¸‹é™ï¼Œè¶‹è¿‘ 50%ï¼‰\")\n",
        "print(\"  3. å­˜åœ¨ä¸€ä¸ªæœ€ä½³å¹³è¡¡ç‚¹ï¼Œåœ¨è¯¥ç‚¹éšç§ä¿æŠ¤å……åˆ†ä¸”æ€§èƒ½æŸå¤±å¯æ§\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ç”Ÿæˆè¯¦ç»†çš„æƒè¡¡åˆ†æè¡¨æ ¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ‰“å°è¯¦ç»†ç»“æœè¡¨æ ¼\n",
        "print(\"=\"*80)\n",
        "print(\"ğŸ“Š éšç§-æ•ˆç”¨æƒè¡¡è¯¦ç»†åˆ†æè¡¨\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n  {'å™ªå£°å¼ºåº¦':<12} {'æµ‹è¯•å‡†ç¡®ç‡':<12} {'æ”»å‡»å‡†ç¡®ç‡':<12} {'éšç§æå‡':<12} {'æ€§èƒ½æŸå¤±':<12}\")\n",
        "print(\"  \" + \"-\"*68)\n",
        "\n",
        "for r in results:\n",
        "    noise_str = f\"Ïƒ={r['noise']:.1f}\"\n",
        "    if r['noise'] == 0:\n",
        "        noise_str += \" (æ™®é€š)\"\n",
        "    \n",
        "    print(f\"  {noise_str:<12} {r['test_acc']:>10.2%} {r['attack_acc']:>10.2%} \"\n",
        "          f\"{r['privacy_gain']:>10.2%} {r['utility_loss']:>10.2%}\")\n",
        "\n",
        "print(\"\\n  ğŸ’¡ è¯´æ˜ï¼š\")\n",
        "print(\"     - éšç§æå‡ = ç›¸æ¯”æ™®é€šæ¨¡å‹ï¼Œæ”»å‡»å‡†ç¡®ç‡ä¸‹é™çš„å¹…åº¦\")\n",
        "print(\"     - æ€§èƒ½æŸå¤± = ç›¸æ¯”æ™®é€šæ¨¡å‹ï¼Œæµ‹è¯•å‡†ç¡®ç‡ä¸‹é™çš„å¹…åº¦\")\n",
        "\n",
        "# æ‰¾åˆ°æ¨èçš„å™ªå£°å¼ºåº¦\n",
        "# ç­–ç•¥ï¼šéšç§æå‡ > 10% ä¸”æ€§èƒ½æŸå¤± < 15% çš„æœ€å¤§å™ªå£°å¼ºåº¦\n",
        "recommended = None\n",
        "for r in results:\n",
        "    if r['privacy_gain'] > 0.10 and r['utility_loss'] < 0.15 and r['noise'] > 0:\n",
        "        recommended = r\n",
        "\n",
        "if recommended:\n",
        "    print(f\"\\n  ğŸ¯ æ¨èé…ç½®: Ïƒ={recommended['noise']:.1f}\")\n",
        "    print(f\"     - éšç§æå‡: {recommended['privacy_gain']:.2%}\")\n",
        "    print(f\"     - æ€§èƒ½æŸå¤±: {recommended['utility_loss']:.2%}\")\n",
        "    print(f\"     - è¯¥é…ç½®åœ¨éšç§ä¿æŠ¤å’Œæ¨¡å‹æ•ˆç”¨ä¹‹é—´å–å¾—äº†è‰¯å¥½å¹³è¡¡\")\n",
        "else:\n",
        "    print(\"\\n  âš  æç¤º: å¯èƒ½éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´å‚æ•°ä»¥æ‰¾åˆ°æ›´å¥½çš„å¹³è¡¡ç‚¹\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### âœ… æ£€æŸ¥ç‚¹ 3\n",
        "\n",
        "è¿è¡Œåˆ°è¿™é‡Œï¼Œä½ åº”è¯¥çœ‹åˆ°ï¼š\n",
        "- [x] æˆåŠŸè®­ç»ƒäº† 6 ä¸ªä¸åŒå™ªå£°å¼ºåº¦çš„æ¨¡å‹\n",
        "- [x] æµ‹è¯•å‡†ç¡®ç‡éšå™ªå£°å¼ºåº¦å¢åŠ è€Œä¸‹é™\n",
        "- [x] æ”»å‡»å‡†ç¡®ç‡éšå™ªå£°å¼ºåº¦å¢åŠ è€Œä¸‹é™ï¼ˆè¶‹è¿‘ 50%ï¼‰\n",
        "- [x] æƒè¡¡æ›²çº¿æ¸…æ™°å±•ç¤ºäº†éšç§ä¸æ•ˆç”¨çš„åå‘å…³ç³»\n",
        "\n",
        "å¦‚æœå‘ç°æŸäº›é…ç½®ä¸‹æ¨¡å‹å®Œå…¨å¤±æ•ˆï¼ˆå‡†ç¡®ç‡ < 40%ï¼‰ï¼Œè¯´æ˜ï¼š\n",
        "- å™ªå£°å¼ºåº¦è¿‡å¤§ï¼Œè¶…å‡ºäº†æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›\n",
        "- å¯ä»¥å°è¯•å¢åŠ è®­ç»ƒè½®æ•°æˆ–è°ƒæ•´å­¦ä¹ ç‡"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“‹ å®éªŒå°ç»“\n",
        "\n",
        "### ğŸ“ æ ¸å¿ƒæ”¶è·\n",
        "\n",
        "é€šè¿‡æœ¬å®éªŒï¼Œä½ åº”è¯¥æŒæ¡äº†ä»¥ä¸‹å…³é”®çŸ¥è¯†ï¼š\n",
        "\n",
        "1. **å·®åˆ†éšç§çš„æ ¸å¿ƒæœºåˆ¶**\n",
        "   - **æ¢¯åº¦è£å‰ª**ï¼šé™åˆ¶å•ä¸ªæ ·æœ¬å¯¹æ¨¡å‹çš„å½±å“èŒƒå›´ï¼ˆæ§åˆ¶æ•æ„Ÿåº¦ï¼‰\n",
        "   - **å™ªå£°æ·»åŠ **ï¼šé€šè¿‡éšæœºå™ªå£°æ©ç›–å•ä¸ªæ ·æœ¬çš„å…·ä½“è´¡çŒ®\n",
        "   - ä¸¤è€…ç»“åˆï¼Œç¡®ä¿å•ä¸ªæ ·æœ¬çš„å­˜åœ¨å¯¹æ¨¡å‹è¾“å‡ºçš„å½±å“å¯ä»¥è¢«å¿½ç•¥\n",
        "\n",
        "2. **éšç§ä¿æŠ¤çš„æ•ˆæœ**\n",
        "   - DP è®­ç»ƒæ˜¾è‘—é™ä½äº†æˆå‘˜æ¨ç†æ”»å‡»çš„æˆåŠŸç‡ï¼ˆä» 65-75% é™è‡³ 50-60%ï¼‰\n",
        "   - æˆå‘˜å’Œéæˆå‘˜çš„ç½®ä¿¡åº¦åˆ†å¸ƒæ›´åŠ é‡å ï¼Œéš¾ä»¥åŒºåˆ†\n",
        "   - æ”»å‡»å‡†ç¡®ç‡æ¥è¿‘ 50%ï¼ˆéšæœºçŒœæµ‹ï¼‰è¯´æ˜éšç§ä¿æŠ¤æœ‰æ•ˆ\n",
        "\n",
        "3. **éšç§-æ•ˆç”¨æƒè¡¡**\n",
        "   - æ›´å¼ºçš„éšç§ä¿æŠ¤ï¼ˆæ›´å¤§çš„å™ªå£°ï¼‰ä¼šå¯¼è‡´æ›´ä½çš„æ¨¡å‹æ€§èƒ½\n",
        "   - éœ€è¦æ ¹æ®å®é™…åœºæ™¯åœ¨éšç§å’Œæ•ˆç”¨ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ç‚¹\n",
        "   - å…¸å‹çš„æ€§èƒ½æŸå¤±åœ¨ 5-15% ä¹‹é—´æ˜¯å¯æ¥å—çš„\n",
        "\n",
        "### ğŸ’» å…³é”®ä»£ç å›é¡¾\n",
        "\n",
        "**å·®åˆ†éšç§ SGD çš„æ ¸å¿ƒä»£ç ï¼š**\n",
        "\n",
        "```python\n",
        "# 1. æ¢¯åº¦è£å‰ªï¼ˆé™åˆ¶å•ä¸ªæ ·æœ¬çš„å½±å“ï¼‰\n",
        "def clip_gradient(gradient, max_norm):\n",
        "    grad_norm = torch.norm(gradient)\n",
        "    if grad_norm > max_norm:\n",
        "        gradient = gradient * (max_norm / grad_norm)\n",
        "    return gradient\n",
        "\n",
        "# 2. å™ªå£°æ·»åŠ ï¼ˆä¿æŠ¤æ ·æœ¬éšç§ï¼‰\n",
        "def add_noise(avg_gradient, max_norm, noise_multiplier, n_samples):\n",
        "    noise_std = (max_norm * noise_multiplier) / n_samples\n",
        "    noise = torch.randn_like(avg_gradient) * noise_std\n",
        "    return avg_gradient + noise\n",
        "\n",
        "# 3. DP-SGD è®­ç»ƒæµç¨‹\n",
        "for sample in training_data:\n",
        "    gradient = compute_gradient(sample)      # è®¡ç®—æ¢¯åº¦\n",
        "    gradient = clip_gradient(gradient, C)    # è£å‰ªæ¢¯åº¦\n",
        "    accumulated_gradients += gradient        # ç´¯åŠ æ¢¯åº¦\n",
        "\n",
        "avg_gradient = accumulated_gradients / n_samples\n",
        "noisy_gradient = add_noise(avg_gradient)    # æ·»åŠ å™ªå£°\n",
        "update_parameters(noisy_gradient)           # æ›´æ–°å‚æ•°\n",
        "```\n",
        "\n",
        "### ğŸ¤” æ€è€ƒä¸è®¨è®º\n",
        "\n",
        "1. **å·®åˆ†éšç§èƒ½é˜²å¾¡æ‰€æœ‰éšç§æ”»å‡»å—ï¼Ÿ**\n",
        "   - DP ä¸»è¦é˜²å¾¡æˆå‘˜æ¨ç†æ”»å‡»ï¼Œä½†å¯¹è®­ç»ƒæ•°æ®æå–æ”»å‡»çš„æ•ˆæœæœ‰é™\n",
        "   - å¦‚æœæ¨¡å‹è§„æ¨¡è¿‡å¤§ã€è®­ç»ƒæ•°æ®è¿‡å°ï¼Œä»å¯èƒ½è®°å¿†ç‰¹å®šæ ·æœ¬\n",
        "   - éœ€è¦ç»“åˆå…¶ä»–é˜²å¾¡æªæ–½ï¼ˆå¦‚æ•°æ®å»æ ‡è¯†åŒ–ã€è®¿é—®æ§åˆ¶ç­‰ï¼‰\n",
        "\n",
        "2. **å¦‚ä½•åœ¨å®é™…é¡¹ç›®ä¸­é€‰æ‹©éšç§å‚æ•°ï¼Ÿ**\n",
        "   - **é«˜æ•æ„Ÿåœºæ™¯**ï¼ˆåŒ»ç–—ã€é‡‘èï¼‰ï¼šÎµ â‰¤ 1ï¼Œå®¹å¿æ›´å¤§çš„æ€§èƒ½æŸå¤±\n",
        "   - **ä¸­ç­‰æ•æ„Ÿåœºæ™¯**ï¼ˆæ¨èç³»ç»Ÿï¼‰ï¼šÎµ âˆˆ [1, 3]ï¼Œå¹³è¡¡éšç§å’Œæ•ˆç”¨\n",
        "   - **ä½æ•æ„Ÿåœºæ™¯**ï¼ˆå…¬å¼€æ•°æ®é›†ï¼‰ï¼šÎµ âˆˆ [3, 10] æˆ–ä¸ä½¿ç”¨ DP\n",
        "\n",
        "3. **DP è®­ç»ƒçš„è®¡ç®—å¼€é”€æœ‰å¤šå¤§ï¼Ÿ**\n",
        "   - éœ€è¦å¯¹æ¯ä¸ªæ ·æœ¬å•ç‹¬è®¡ç®—æ¢¯åº¦ï¼ˆper-sample gradientï¼‰ï¼Œè®¡ç®—é‡æ˜¯æ™®é€šè®­ç»ƒçš„ O(n) å€\n",
        "   - å®é™…åº”ç”¨ä¸­å¯ä½¿ç”¨é«˜æ•ˆå®ç°ï¼ˆå¦‚ Opacus åº“ï¼‰å’Œæ‰¹å¤„ç†ä¼˜åŒ–\n",
        "   - GPU å¹¶è¡ŒåŒ–å¯ä»¥æ˜¾è‘—å‡å°‘è®­ç»ƒæ—¶é—´\n",
        "\n",
        "4. **å·®åˆ†éšç§çš„ç†è®ºä¿è¯æ˜¯ä»€ä¹ˆï¼Ÿ**\n",
        "   - (Îµ, Î´)-å·®åˆ†éšç§ï¼šå¯¹äºä»»æ„ä¸¤ä¸ªä»…ç›¸å·®ä¸€æ¡è®°å½•çš„æ•°æ®é›†ï¼Œæ¨¡å‹è¾“å‡ºåˆ†å¸ƒçš„å·®å¼‚åœ¨ Îµ å’Œ Î´ çš„æ§åˆ¶ä¸‹\n",
        "   - Îµ ç§°ä¸ºéšç§é¢„ç®—ï¼Œè¶Šå°éšç§ä¿æŠ¤è¶Šå¼º\n",
        "   - Î´ æ˜¯å¤±è´¥æ¦‚ç‡ï¼Œé€šå¸¸è®¾ç½®ä¸ºè¿œå°äº 1/n çš„å€¼ï¼ˆå¦‚ 10â»âµï¼‰\n",
        "\n",
        "### ğŸ”— å»¶ä¼¸é˜…è¯»\n",
        "\n",
        "- **ç»å…¸è®ºæ–‡**ï¼šAbadi et al. (2016) - \"Deep Learning with Differential Privacy\" (æå‡º DP-SGD)\n",
        "- **å®ç”¨å·¥å…·**ï¼š\n",
        "  - [Opacus](https://opacus.ai/) - PyTorch çš„å·®åˆ†éšç§åº“\n",
        "  - [TensorFlow Privacy](https://github.com/tensorflow/privacy) - TensorFlow çš„ DP å®ç°\n",
        "- **ç†è®ºåŸºç¡€**ï¼š\n",
        "  - Dwork & Roth (2014) - \"The Algorithmic Foundations of Differential Privacy\"\n",
        "  - [å·®åˆ†éšç§å…¥é—¨æ•™ç¨‹](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf)\n",
        "\n",
        "### ğŸš€ è¿›é˜¶æŒ‘æˆ˜\n",
        "\n",
        "å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼Œè¿›ä¸€æ­¥æå‡ä½ çš„æŠ€èƒ½ï¼š\n",
        "\n",
        "1. **ä½¿ç”¨ Opacus åº“é‡æ„å®éªŒ**\n",
        "   - Opacus æä¾›äº†é«˜æ•ˆçš„ per-sample gradient è®¡ç®—\n",
        "   - æ”¯æŒè‡ªåŠ¨è®¡ç®—éšç§é¢„ç®— Îµ\n",
        "   - å°è¯•åœ¨ CIFAR-10 ç­‰çœŸå®æ•°æ®é›†ä¸Šåº”ç”¨\n",
        "\n",
        "2. **å®ç°éšç§é¢„ç®—è¿½è¸ª**\n",
        "   - ä½¿ç”¨ Moments Accountant æˆ– RÃ©nyi DP è®¡ç®—ç´¯ç§¯çš„éšç§æŸå¤±\n",
        "   - å¯è§†åŒ– Îµ éšè®­ç»ƒè½®æ•°çš„å˜åŒ–\n",
        "\n",
        "3. **å¯¹æ¯”ä¸åŒçš„éšç§ä¼šè®¡æ–¹æ³•**\n",
        "   - æ¯”è¾ƒ Moments Accountantã€RÃ©nyi DPã€GDP ç­‰æ–¹æ³•\n",
        "   - åˆ†æå®ƒä»¬åœ¨ä¸åŒåœºæ™¯ä¸‹çš„ç´§è‡´æ€§ï¼ˆtightnessï¼‰\n",
        "\n",
        "4. **æµ‹è¯• DP å¯¹å…¶ä»–æ”»å‡»çš„é˜²å¾¡æ•ˆæœ**\n",
        "   - å±æ€§æ¨ç†æ”»å‡»ï¼ˆAttribute Inferenceï¼‰\n",
        "   - æ¨¡å‹åæ¼”æ”»å‡»ï¼ˆModel Inversionï¼‰\n",
        "   - è®­ç»ƒæ•°æ®æå–æ”»å‡»ï¼ˆTraining Data Extractionï¼‰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“š å‚è€ƒç­”æ¡ˆ\n",
        "\n",
        "<details>\n",
        "<summary><strong>ç‚¹å‡»å±•å¼€æŸ¥çœ‹æ‰€æœ‰å¡«ç©ºçš„å‚è€ƒç­”æ¡ˆ</strong></summary>\n",
        "\n",
        "### å¡«ç©º 1ï¼šæ¢¯åº¦è£å‰ªå‡½æ•°\n",
        "\n",
        "```python\n",
        "if grad_norm > max_norm:\n",
        "    gradient = gradient * (max_norm / grad_norm)\n",
        "```\n",
        "\n",
        "**è§£é‡Š**ï¼šå½“æ¢¯åº¦èŒƒæ•°è¶…è¿‡é˜ˆå€¼æ—¶ï¼ŒæŒ‰æ¯”ä¾‹ç¼©æ”¾ä½¿å…¶èŒƒæ•°ç­‰äº `max_norm`ã€‚è¿™ç¡®ä¿äº†æ¯ä¸ªæ ·æœ¬çš„æ¢¯åº¦è´¡çŒ®è¢«é™åˆ¶åœ¨å›ºå®šèŒƒå›´å†…ã€‚\n",
        "\n",
        "### å¡«ç©º 2ï¼šå™ªå£°æ·»åŠ æœºåˆ¶\n",
        "\n",
        "```python\n",
        "noise = torch.randn_like(avg_gradient) * noise_std\n",
        "```\n",
        "\n",
        "**è§£é‡Š**ï¼š`torch.randn_like()` ç”Ÿæˆä¸æ¢¯åº¦åŒå½¢çŠ¶çš„æ ‡å‡†æ­£æ€åˆ†å¸ƒéšæœºæ•°ï¼ˆå‡å€¼ 0ï¼Œæ ‡å‡†å·® 1ï¼‰ï¼Œä¹˜ä»¥ `noise_std` å¾—åˆ°æ‰€éœ€æ ‡å‡†å·®çš„å™ªå£°ã€‚\n",
        "\n",
        "### å¡«ç©º 3ï¼šæ¨¡å‹è¯„ä¼°å‡½æ•°\n",
        "\n",
        "```python\n",
        "accuracy = (predictions == y).float().mean().item()\n",
        "```\n",
        "\n",
        "**è§£é‡Š**ï¼š\n",
        "- `predictions == y` è¿”å›å¸ƒå°”å¼ é‡ï¼ˆTrue/Falseï¼‰\n",
        "- `.float()` è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼ˆ1.0/0.0ï¼‰\n",
        "- `.mean()` è®¡ç®—å¹³å‡å€¼ï¼ˆå³å‡†ç¡®ç‡ï¼‰\n",
        "- `.item()` è½¬æ¢ä¸º Python æ ‡é‡\n",
        "\n",
        "### å¡«ç©º 4ï¼šæˆå‘˜æ¨ç†æ”»å‡»\n",
        "\n",
        "```python\n",
        "train_probs = torch.softmax(model(X_train), dim=1)\n",
        "test_probs = torch.softmax(model(X_test), dim=1)\n",
        "```\n",
        "\n",
        "**è§£é‡Š**ï¼š`torch.softmax(logits, dim=1)` å°†æ¨¡å‹è¾“å‡ºï¼ˆlogitsï¼‰è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œæ¯è¡Œçš„å’Œä¸º 1ã€‚`dim=1` è¡¨ç¤ºåœ¨ç±»åˆ«ç»´åº¦ä¸Šå½’ä¸€åŒ–ã€‚\n",
        "\n",
        "### å¡«ç©º 5ï¼šéšç§-æ•ˆç”¨æƒè¡¡å®éªŒ\n",
        "\n",
        "```python\n",
        "# æ™®é€šè®­ç»ƒ\n",
        "train_normal(temp_model, X_train, y_train, epochs=50, lr=0.01, verbose=False)\n",
        "\n",
        "# å·®åˆ†éšç§è®­ç»ƒ\n",
        "train_with_dp(temp_model, X_train, y_train, epochs=50, lr=0.01,\n",
        "             max_grad_norm=1.0, noise_multiplier=noise_mult, verbose=False)\n",
        "```\n",
        "\n",
        "**è§£é‡Š**ï¼šæ ¹æ® `noise_mult` çš„å€¼é€‰æ‹©è®­ç»ƒæ–¹å¼ã€‚å½“ `noise_mult=0` æ—¶ä½¿ç”¨æ™®é€šè®­ç»ƒï¼Œå¦åˆ™ä½¿ç”¨å·®åˆ†éšç§è®­ç»ƒï¼Œå™ªå£°å¼ºåº¦ç”± `noise_mult` æ§åˆ¶ã€‚\n",
        "\n",
        "### å…³é”®å‚æ•°è¯´æ˜\n",
        "\n",
        "| å‚æ•° | å«ä¹‰ | å…¸å‹å€¼ | å½±å“ |\n",
        "|------|------|--------|------|\n",
        "| `max_grad_norm` (C) | æ¢¯åº¦è£å‰ªé˜ˆå€¼ | 0.1-5.0 | è¶Šå°éšç§è¶Šå¼ºï¼Œä½†è®­ç»ƒè¶Šæ…¢ |\n",
        "| `noise_multiplier` (Ïƒ) | å™ªå£°å¼ºåº¦ | 0.5-3.0 | è¶Šå¤§éšç§è¶Šå¼ºï¼Œæ€§èƒ½è¶Šå·® |\n",
        "| `epsilon` (Îµ) | éšç§é¢„ç®— | 1.0-10.0 | è¶Šå°éšç§è¶Šå¼ºï¼ˆéœ€é€šè¿‡ Ïƒ å’Œè®­ç»ƒè½®æ•°è®¡ç®—ï¼‰|\n",
        "| `delta` (Î´) | å¤±è´¥æ¦‚ç‡ | 1e-5 | é€šå¸¸å›ºå®šä¸º 1/nÂ² |\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ‰ æ­å–œï¼\n",
        "\n",
        "ä½ å·²ç»å®Œæˆäº†å·®åˆ†éšç§é˜²å¾¡å®éªŒï¼\n",
        "\n",
        "### ä½ å­¦åˆ°äº†ä»€ä¹ˆï¼š\n",
        "- âœ… å·®åˆ†éšç§çš„æ ¸å¿ƒåŸç†å’Œå®ç°æ–¹æ³•\n",
        "- âœ… å¦‚ä½•é‡åŒ–è¯„ä¼°éšç§ä¿æŠ¤æ•ˆæœ\n",
        "- âœ… éšç§ä¸æ•ˆç”¨ä¹‹é—´çš„æƒè¡¡å…³ç³»\n",
        "- âœ… æˆå‘˜æ¨ç†æ”»å‡»çš„é˜²å¾¡ç­–ç•¥\n",
        "\n",
        "### ä¸‹ä¸€æ­¥ï¼š\n",
        "- ğŸ“– å­¦ä¹ æ›´å¤šéšç§ä¿æŠ¤æŠ€æœ¯ï¼ˆè”é‚¦å­¦ä¹ ã€å®‰å…¨å¤šæ–¹è®¡ç®—ç­‰ï¼‰\n",
        "- ğŸ”¬ åœ¨çœŸå®æ•°æ®é›†ä¸Šåº”ç”¨å·®åˆ†éšç§ï¼ˆä½¿ç”¨ Opacus ç­‰ä¸“ä¸šåº“ï¼‰\n",
        "- ğŸ¤ æ¢ç´¢éšç§ä¿æŠ¤ä¸æ¨¡å‹å…¬å¹³æ€§ã€å¯è§£é‡Šæ€§çš„å…³ç³»\n",
        "\n",
        "**ç»§ç»­åŠ æ²¹ï¼ğŸš€**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
