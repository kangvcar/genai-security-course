{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验 1.1：环境搭建与模型调用\n",
    "\n",
    "## 实验目标\n",
    "- 熟悉 Cloud Studio 实验环境\n",
    "- 学会加载和调用大语言模型\n",
    "- 理解模型参数（Temperature）对输出的影响\n",
    "- 为后续安全实验打下基础\n",
    "\n",
    "## 实验环境\n",
    "- 平台：腾讯 Cloud Studio（https://cloudstudio.net/）\n",
    "- GPU：NVIDIA Tesla T4（16GB 显存）\n",
    "- 模型：Qwen2-1.5B-Instruct（阿里通义千问）\n",
    "\n",
    "## 预计时间：20分钟\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一部分：环境准备与验证\n",
    "\n",
    "> 💡 **为什么要检查环境？**\n",
    "> \n",
    "> 在进行 AI 实验前，需要确认 GPU 环境是否正确配置。GPU（图形处理器）能让模型推理速度提升 10-100 倍！\n",
    "> Cloud Studio 免费提供 NVIDIA T4 GPU（16GB 显存），足够运行本课程所有实验。\n",
    "\n",
    "### 1.1 安装依赖\n",
    "\n",
    "首先安装本实验需要的 Python 库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装必要的依赖（首次运行需要几分钟）\n",
    "!pip install torch transformers accelerate -q\n",
    "\n",
    "print(\"✓ 依赖安装完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 验证 GPU 环境\n",
    "\n",
    "运行下面的代码，检查你的实验环境是否配置正确："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 环境验证脚本 ======\n",
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"🔍 AI 安全实验环境检查\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 检查 Python 版本\n",
    "print(f\"\\n[1] Python 版本: {sys.version.split()[0]}\")\n",
    "\n",
    "# 检查 PyTorch 和 CUDA\n",
    "print(f\"[2] PyTorch 版本: {torch.__version__}\")\n",
    "print(f\"    CUDA 可用: {'✓ 是' if torch.cuda.is_available() else '✗ 否'}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"    GPU 型号: {gpu_name}\")\n",
    "    print(f\"    GPU 显存: {gpu_memory:.1f} GB\")\n",
    "\n",
    "# 检查 Transformers\n",
    "print(f\"[3] Transformers 版本: {transformers.__version__}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"✅ 环境检查通过！GPU 已就绪，可以开始实验\")\n",
    "else:\n",
    "    print(\"⚠️ 警告：未检测到 GPU\")\n",
    "    print(\"   请在 Cloud Studio 中选择 GPU 环境\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ 检查点 1\n",
    "\n",
    "运行上面的代码后，你应该看到：\n",
    "- [ ] CUDA 可用显示 \"✓ 是\"\n",
    "- [ ] GPU 型号显示 \"Tesla T4\" 或类似\n",
    "- [ ] GPU 显存约 16 GB\n",
    "\n",
    "如果显示 \"未检测到 GPU\"，请确认已在 Cloud Studio 中选择 GPU 环境。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二部分：加载中文大语言模型\n",
    "\n",
    "> 💡 **什么是大语言模型（LLM）？**\n",
    "> \n",
    "> 大语言模型是一种 AI 模型，通过学习海量文本数据，能够理解和生成人类语言。\n",
    "> 常见的 LLM 包括：ChatGPT、文心一言、通义千问等。\n",
    "\n",
    "### 为什么选择 Qwen2-1.5B？\n",
    "\n",
    "| 优势 | 说明 |\n",
    "|------|------|\n",
    "| 🇨🇳 中文优秀 | 专门针对中文优化，理解能力强 |\n",
    "| 💾 资源友好 | 1.5B 参数，T4 GPU 轻松运行（约 4GB 显存） |\n",
    "| 🔒 安全护栏 | 内置安全机制，适合安全实验 |\n",
    "| 💬 对话优化 | Instruct 版本经过指令微调，对话体验好 |\n",
    "\n",
    "### 关键概念\n",
    "\n",
    "- **Tokenizer（分词器）**：将文本转换为模型能理解的数字序列\n",
    "- **半精度（float16）**：使用 16 位浮点数存储参数，显存减半，速度更快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 加载 Qwen2-1.5B 模型 ======\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 指定模型名称\n",
    "model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "\n",
    "print(f\"正在加载模型: {model_name}\")\n",
    "print(\"首次加载需要下载模型文件（约 3GB），请耐心等待...\")\n",
    "print()\n",
    "\n",
    "# ========== 填空 1：加载分词器 ==========\n",
    "# \n",
    "# 🎯 任务：使用 AutoTokenizer 加载分词器\n",
    "# \n",
    "# 💡 提示：\n",
    "#   - AutoTokenizer 可以自动识别模型类型\n",
    "#   - 使用 from_pretrained() 方法，传入模型名称\n",
    "#\n",
    "# 请将 ___________ 替换为正确的代码\n",
    "\n",
    "tokenizer = ___________\n",
    "\n",
    "# 加载模型到 GPU（使用半精度节省显存）\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # 半精度，显存减半\n",
    "    device_map=\"auto\"           # 自动选择设备（优先 GPU）\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"✅ 模型加载成功！\")\n",
    "print(f\"   模型参数量: {model.num_parameters() / 1e9:.2f}B（十亿）\")\n",
    "print(f\"   运行设备: {next(model.parameters()).device}\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ 检查点 2\n",
    "\n",
    "运行上面的代码后，你应该看到：\n",
    "- [ ] 模型加载成功\n",
    "- [ ] 参数量约 1.54B\n",
    "- [ ] 运行设备显示 cuda:0（表示在 GPU 上）\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三部分：与模型对话\n",
    "\n",
    "> 💡 **对话格式说明**\n",
    "> \n",
    "> 与 LLM 对话需要按照特定格式构建输入。Qwen 使用 `messages` 格式：\n",
    "> ```python\n",
    "> messages = [\n",
    ">     {\"role\": \"system\", \"content\": \"系统提示，定义 AI 角色\"},\n",
    ">     {\"role\": \"user\", \"content\": \"用户的问题\"}\n",
    "> ]\n",
    "> ```\n",
    "> - **system**：系统提示，设定 AI 的行为规范（用户通常看不到）\n",
    "> - **user**：用户输入的问题或指令"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 定义对话函数 ======\n",
    "\n",
    "def chat(user_message, system_prompt=\"你是一个有帮助的AI助手。\", temperature=0.7):\n",
    "    \"\"\"\n",
    "    与模型进行对话\n",
    "    \n",
    "    参数:\n",
    "        user_message: 用户输入的消息\n",
    "        system_prompt: 系统提示词（定义AI的角色和行为）\n",
    "        temperature: 控制输出随机性（0-2，越高越随机）\n",
    "    \n",
    "    返回:\n",
    "        模型的回复文本\n",
    "    \"\"\"\n",
    "    # 构建对话格式\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    # ========== 填空 2：将对话转换为模型输入格式 ==========\n",
    "    # \n",
    "    # 🎯 任务：使用 tokenizer 的 apply_chat_template 方法转换格式\n",
    "    # \n",
    "    # 💡 提示：\n",
    "    #   - apply_chat_template 将 messages 列表转换为模型需要的文本格式\n",
    "    #   - 需要设置 tokenize=False（返回文本而非数字）\n",
    "    #   - 需要设置 add_generation_prompt=True（添加生成提示）\n",
    "    #\n",
    "    # 请将 ___________ 替换为正确的代码\n",
    "    \n",
    "    text = ___________\n",
    "    \n",
    "    # 编码输入并移到 GPU\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # 生成回复\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,      # 最多生成 256 个 token\n",
    "        temperature=temperature, # 控制随机性\n",
    "        do_sample=True,          # 启用采样\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # 解码输出（只取新生成的部分）\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    return response\n",
    "\n",
    "print(\"✓ 对话函数定义完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试对话功能\n",
    "print(\"🧪 测试对话功能...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "response = chat(\"你好，请用一句话介绍你自己。\")\n",
    "\n",
    "print(f\"👤 用户: 你好，请用一句话介绍你自己。\")\n",
    "print(f\"🤖 AI: {response}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第四部分：探索模型能力\n",
    "\n",
    "> 💡 **为什么要了解模型能力？**\n",
    "> \n",
    "> 了解模型的能力边界，是进行安全测试的基础。\n",
    "> 我们需要知道模型\"能做什么\"，才能探索它\"可能被滥用做什么\"。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 测试模型的不同能力 ======\n",
    "\n",
    "test_questions = [\n",
    "    \"什么是人工智能？请用简单的话解释。\",\n",
    "    \"用 Python 写一个计算 1 到 100 求和的代码。\",\n",
    "    \"帮我写一首关于春天的五言绝句。\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"🧪 模型能力测试\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n【问题 {i}】{question}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # ========== 填空 3：调用对话函数 ==========\n",
    "    # \n",
    "    # 🎯 任务：调用 chat() 函数获取模型回复\n",
    "    # \n",
    "    # 💡 提示：\n",
    "    #   - 使用前面定义的 chat() 函数\n",
    "    #   - 传入 question 作为参数\n",
    "    #\n",
    "    # 请将 ___________ 替换为正确的代码\n",
    "    \n",
    "    response = ___________\n",
    "    \n",
    "    print(f\"【回复】\\n{response}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤔 思考一下\n",
    "\n",
    "观察上面的输出，思考以下问题：\n",
    "\n",
    "1. **能力评估**：模型在哪些任务上表现较好？哪些较弱？\n",
    "2. **安全视角**：如果有人想让模型做\"不该做的事\"，会从哪些能力入手？\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第五部分：Temperature 参数实验\n",
    "\n",
    "> 💡 **什么是 Temperature？**\n",
    "> \n",
    "> Temperature（温度）是控制模型输出随机性的关键参数。\n",
    "> 你可以把它想象成\"创意开关\"：\n",
    "\n",
    "| 温度值 | 特点 | 类比 | 适用场景 |\n",
    "|--------|------|------|----------|\n",
    "| 低温（0.1-0.3） | 输出确定、保守、一致 | 严谨的学者 | 代码生成、客服 |\n",
    "| 中温（0.5-0.7） | 平衡创意与准确性 | 理性的作家 | 通用对话 |\n",
    "| 高温（0.9-1.5） | 输出随机、有创意、多样 | 天马行空的艺术家 | 创意写作 |\n",
    "\n",
    "### ⚠️ 安全警示\n",
    "\n",
    "**高温设置可能导致模型产生意外输出**——这在安全测试中很重要：\n",
    "- 攻击者可能利用高温设置诱导模型生成不当内容\n",
    "- 高温下模型更容易\"突破\"安全护栏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Temperature 对比实验 ======\n",
    "\n",
    "prompt = \"给我讲一个关于机器人的故事的开头。\"\n",
    "\n",
    "print(f\"📝 测试问题: {prompt}\")\n",
    "print()\n",
    "\n",
    "# 低温测试\n",
    "print(\"=\" * 55)\n",
    "print(\"❄️ 低温生成 (Temperature = 0.1) - 结果更一致\")\n",
    "print(\"=\" * 55)\n",
    "for i in range(3):\n",
    "    result = chat(prompt, temperature=0.1)\n",
    "    print(f\"第 {i+1} 次: {result[:80]}...\")\n",
    "\n",
    "# 高温测试\n",
    "print()\n",
    "print(\"=\" * 55)\n",
    "print(\"🔥 高温生成 (Temperature = 1.2) - 结果更多样\")\n",
    "print(\"=\" * 55)\n",
    "for i in range(3):\n",
    "    result = chat(prompt, temperature=1.2)\n",
    "    print(f\"第 {i+1} 次: {result[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤔 思考一下\n",
    "\n",
    "1. **观察差异**：低温和高温的输出有什么明显区别？\n",
    "2. **应用场景**：如果你在开发一个银行客服 AI，应该用高温还是低温？为什么？\n",
    "3. **安全风险**：高温设置可能带来什么安全风险？\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第六部分：系统提示的安全作用\n",
    "\n",
    "> 💡 **系统提示（System Prompt）是什么？**\n",
    "> \n",
    "> 系统提示是开发者写给 AI 的\"隐藏指令\"，定义了 AI 的角色、能力边界和行为规范。\n",
    "> \n",
    "> **关键安全概念**：\n",
    "> - 系统提示是 AI 安全的\"第一道防线\"\n",
    "> - 但它并不是万能的——很多攻击就是试图绕过系统提示的限制\n",
    "> - 这就是我们后续要学习的\"提示词注入攻击\"的基础"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 系统提示对比实验 ======\n",
    "\n",
    "# ========== 填空 4：定义一个严格限制的系统提示 ==========\n",
    "# \n",
    "# 🎯 任务：创建一个只允许回答编程问题的系统提示\n",
    "# \n",
    "# 💡 提示：\n",
    "#   - 明确说明 AI 的角色（编程助手）\n",
    "#   - 明确说明限制（只回答编程问题）\n",
    "#   - 明确说明对其他问题的处理方式（礼貌拒绝）\n",
    "#\n",
    "# 示例格式：\"你是一个..., 你只能..., 对于其他问题...\"\n",
    "\n",
    "strict_system_prompt = ___________\n",
    "\n",
    "# 定义其他系统提示用于对比\n",
    "system_prompts = {\n",
    "    \"默认助手\": \"你是一个有帮助的AI助手。\",\n",
    "    \"安全专家\": \"你是一位网络安全专家。你热情地回答安全相关问题，对于其他问题，礼貌地引导回安全话题。\",\n",
    "    \"严格模式\": strict_system_prompt,\n",
    "}\n",
    "\n",
    "# 测试不同系统提示的效果\n",
    "test_message = \"今天天气怎么样？\"\n",
    "\n",
    "print(f\"📝 测试问题: {test_message}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, prompt in system_prompts.items():\n",
    "    print(f\"\\n【{name}】\")\n",
    "    print(f\"   系统提示: {prompt[:50]}...\")\n",
    "    print(\"-\" * 50)\n",
    "    response = chat(test_message, system_prompt=prompt)\n",
    "    print(f\"   回复: {response[:150]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤔 思考一下\n",
    "\n",
    "1. **观察差异**：不同系统提示下，模型对同一问题的回答有什么不同？\n",
    "2. **有效性**：\"严格模式\"下模型是否成功拒绝了非编程问题？\n",
    "3. **安全思考**：如果有人想让模型\"忽略\"系统提示，可能会怎么做？（这就是提示词注入攻击的核心思想！）\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 实验小结\n",
    "\n",
    "### 核心收获\n",
    "\n",
    "通过本实验，你已经：\n",
    "\n",
    "| 目标 | 完成情况 |\n",
    "|------|----------|\n",
    "| 验证 GPU 环境 | ✅ 确认 T4 GPU 可用 |\n",
    "| 加载中文 LLM | ✅ 掌握 Tokenizer + Model 加载方式 |\n",
    "| 理解 Temperature | ✅ 低温一致，高温多样 |\n",
    "| 理解系统提示 | ✅ 第一道防线，但可能被绕过 |\n",
    "\n",
    "### 🔑 关键概念回顾\n",
    "\n",
    "```python\n",
    "# 1. 加载模型的标准流程\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# 2. 对话格式\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"系统提示\"},\n",
    "    {\"role\": \"user\", \"content\": \"用户消息\"}\n",
    "]\n",
    "\n",
    "# 3. Temperature 的安全影响\n",
    "# 低温 = 稳定输出，高温 = 可能意外输出\n",
    "```\n",
    "\n",
    "### 🔮 预告：安全隐患\n",
    "\n",
    "本实验我们学习了 LLM 的基本使用，但你可能已经注意到一些潜在问题：\n",
    "\n",
    "- 系统提示可能被\"绕过\"？\n",
    "- 模型可能被诱导生成不当内容？\n",
    "- 高温设置可能导致意外输出？\n",
    "\n",
    "这些都是 **AI 安全** 需要关注的问题，我们将在后续模块深入探讨！\n",
    "\n",
    "---\n",
    "\n",
    "## ⏭️ 下一步\n",
    "\n",
    "继续完成 **实验 1.2：AI 漏洞侦察初体验**，学习如何探测模型的安全边界。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📖 参考答案\n",
    "\n",
    "<details>\n",
    "<summary>点击展开参考答案</summary>\n",
    "\n",
    "**填空 1**：\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "```\n",
    "\n",
    "**填空 2**：\n",
    "```python\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "```\n",
    "\n",
    "**填空 3**：\n",
    "```python\n",
    "response = chat(question)\n",
    "```\n",
    "\n",
    "**填空 4**（示例）：\n",
    "```python\n",
    "strict_system_prompt = \"你是一个专业的编程助手。你只能回答编程和技术相关的问题。对于其他问题，请礼貌地说明你只能帮助解决编程问题。\"\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实验结束，清理显存（可选）\n",
    "# 如果要继续进行下一个实验，可以运行这段代码释放显存\n",
    "\n",
    "# del model, tokenizer\n",
    "# import torch\n",
    "# torch.cuda.empty_cache()\n",
    "# print(\"✓ 显存已清理\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
