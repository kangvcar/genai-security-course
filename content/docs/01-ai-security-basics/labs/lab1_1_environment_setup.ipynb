{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§ª å®éªŒ 1.1ï¼šç¯å¢ƒæ­å»ºä¸æ¨¡å‹è°ƒç”¨\n",
    "\n",
    "## ğŸ¯ å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "å®Œæˆæœ¬å®éªŒåï¼Œä½ å°†èƒ½å¤Ÿï¼š\n",
    "- âœ… ç†Ÿæ‚‰ Cloud Studio å®éªŒç¯å¢ƒå¹¶å®Œæˆ GPU éªŒè¯\n",
    "- âœ… å­¦ä¼šåŠ è½½å’Œè°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆQwen2-1.5B-Instructï¼‰\n",
    "- âœ… ç†è§£æ¨¡å‹å‚æ•°ï¼ˆTemperatureï¼‰å¯¹è¾“å‡ºçš„å½±å“\n",
    "- âœ… ä¸ºåç»­å®‰å…¨å®éªŒæ‰“ä¸‹åŸºç¡€\n",
    "\n",
    "## ğŸ“š å‰ç½®çŸ¥è¯†\n",
    "- äº†è§£ Python åŸºç¡€è¯­æ³•ï¼ˆå˜é‡ã€å‡½æ•°ã€å­—ç¬¦ä¸²æ“ä½œï¼‰\n",
    "- äº†è§£å¤§è¯­è¨€æ¨¡å‹çš„åŸºæœ¬æ¦‚å¿µï¼ˆä»€ä¹ˆæ˜¯ LLMï¼‰\n",
    "- ç›¸å…³ç†è®ºï¼š[æ¨¡å—ä¸€ï¼šLLM åŸç†](../llm-principles)\n",
    "\n",
    "## ğŸ–¥ï¸ å®éªŒç¯å¢ƒ\n",
    "\n",
    "- å¹³å°ï¼šè…¾è®¯ Cloud Studioï¼ˆhttps://cloudstudio.net/ï¼‰\n",
    "- GPUï¼šNVIDIA Tesla T4ï¼ˆ16GB æ˜¾å­˜ï¼‰\n",
    "- æ¨¡å‹ï¼šQwen2-1.5B-Instructï¼ˆé˜¿é‡Œé€šä¹‰åƒé—®ï¼‰\n",
    "- Pythonï¼šâ‰¥ 3.10\n",
    "- å…³é”®ä¾èµ–ï¼štransformers â‰¥ 4.37, torch â‰¥ 2.0, accelerate â‰¥ 0.26\n",
    "\n",
    "## ğŸ“ å¡«ç©ºè¯´æ˜\n",
    "æœ¬å®éªŒå…± **4 ä¸ªå¡«ç©º**ï¼Œéš¾åº¦ï¼šâ­â­â˜†â˜†â˜†\n",
    "\n",
    "## â±ï¸ é¢„è®¡ç”¨æ—¶\n",
    "çº¦ **25 åˆ†é’Ÿ**\n",
    "\n",
    "## ğŸ“‘ ç›®å½•\n",
    "1. [ç¬¬ä¸€éƒ¨åˆ†ï¼šç¯å¢ƒå‡†å¤‡ä¸éªŒè¯](#ç¬¬ä¸€éƒ¨åˆ†ï¼šç¯å¢ƒå‡†å¤‡ä¸éªŒè¯)ï¼ˆçº¦ 5 åˆ†é’Ÿï¼‰\n",
    "2. [ç¬¬äºŒéƒ¨åˆ†ï¼šåŠ è½½ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹](#ç¬¬äºŒéƒ¨åˆ†ï¼šåŠ è½½ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹)ï¼ˆçº¦ 5 åˆ†é’Ÿï¼‰\n",
    "3. [ç¬¬ä¸‰éƒ¨åˆ†ï¼šä¸æ¨¡å‹å¯¹è¯](#ç¬¬ä¸‰éƒ¨åˆ†ï¼šä¸æ¨¡å‹å¯¹è¯)ï¼ˆçº¦ 5 åˆ†é’Ÿï¼‰\n",
    "4. [ç¬¬å››éƒ¨åˆ†ï¼šæ¢ç´¢æ¨¡å‹èƒ½åŠ›](#ç¬¬å››éƒ¨åˆ†ï¼šæ¢ç´¢æ¨¡å‹èƒ½åŠ›)ï¼ˆçº¦ 3 åˆ†é’Ÿï¼‰\n",
    "5. [ç¬¬äº”éƒ¨åˆ†ï¼šTemperature å‚æ•°å®éªŒ](#ç¬¬äº”éƒ¨åˆ†ï¼šTemperature-å‚æ•°å®éªŒ)ï¼ˆçº¦ 4 åˆ†é’Ÿï¼‰\n",
    "6. [ç¬¬å…­éƒ¨åˆ†ï¼šç³»ç»Ÿæç¤ºçš„å®‰å…¨ä½œç”¨](#ç¬¬å…­éƒ¨åˆ†ï¼šç³»ç»Ÿæç¤ºçš„å®‰å…¨ä½œç”¨)ï¼ˆçº¦ 3 åˆ†é’Ÿï¼‰\n",
    "\n",
    "## ğŸ“¤ æäº¤è¯´æ˜\n",
    "å®Œæˆæ‰€æœ‰å¡«ç©ºåï¼Œè¯·å°†æœ¬ Notebook æ–‡ä»¶ï¼ˆ.ipynbï¼‰å¯¼å‡ºå¹¶æäº¤è‡³è¯¾ç¨‹å¹³å°ã€‚è¯„åˆ†æ ‡å‡†ï¼š\n",
    "- 4 ä¸ªå¡«ç©ºæ­£ç¡®å®Œæˆï¼ˆæ¯ä¸ª 20 åˆ†ï¼Œå…± 80 åˆ†ï¼‰\n",
    "- æ€è€ƒé¢˜å›ç­”è´¨é‡ï¼ˆ20 åˆ†ï¼‰\n",
    "\n",
    "âš ï¸ **å®‰å…¨æé†’**ï¼šæœ¬å®éªŒä»…ç”¨äºæ•™è‚²ç›®çš„ï¼Œè¯·å‹¿å°†æ‰€å­¦æŠ€æœ¯ç”¨äºæœªæˆæƒç³»ç»Ÿã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸€éƒ¨åˆ†ï¼šç¯å¢ƒå‡†å¤‡ä¸éªŒè¯\n",
    "\n",
    "ğŸ’¡ **ä¸ºä»€ä¹ˆè¦æ£€æŸ¥ç¯å¢ƒï¼Ÿ**\n",
    "\n",
    "åœ¨è¿›è¡Œ AI å®éªŒå‰ï¼Œéœ€è¦ç¡®è®¤ GPU ç¯å¢ƒæ˜¯å¦æ­£ç¡®é…ç½®ã€‚GPUï¼ˆå›¾å½¢å¤„ç†å™¨ï¼‰èƒ½è®©æ¨¡å‹æ¨ç†é€Ÿåº¦æå‡ 10-100 å€ï¼\n",
    "\n",
    "Cloud Studio å…è´¹æä¾› NVIDIA T4 GPUï¼ˆ16GB æ˜¾å­˜ï¼‰ï¼Œè¶³å¤Ÿè¿è¡Œæœ¬è¯¾ç¨‹æ‰€æœ‰å®éªŒã€‚\n",
    "\n",
    "### 1.1 å®‰è£…ä¾èµ–\n",
    "\n",
    "é¦–å…ˆå®‰è£…æœ¬å®éªŒéœ€è¦çš„ Python åº“ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…å¿…è¦çš„ä¾èµ–ï¼ˆé¦–æ¬¡è¿è¡Œéœ€è¦å‡ åˆ†é’Ÿï¼‰\n",
    "%pip install \"torch>=2.0,<3.0\" \"transformers>=4.37,<5.0\" \"accelerate>=0.26,<1.0\" -q\n",
    "\n",
    "print(\"âœ“ ä¾èµ–å®‰è£…å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 éªŒè¯ GPU ç¯å¢ƒ\n",
    "\n",
    "è¿è¡Œä¸‹é¢çš„ä»£ç ï¼Œæ£€æŸ¥ä½ çš„å®éªŒç¯å¢ƒæ˜¯å¦é…ç½®æ­£ç¡®ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== ç¯å¢ƒéªŒè¯è„šæœ¬ ======\n",
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"ğŸ” AI å®‰å…¨å®éªŒç¯å¢ƒæ£€æŸ¥\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# æ£€æŸ¥ Python ç‰ˆæœ¬\n",
    "print(f\"\\n[1] Python ç‰ˆæœ¬: {sys.version.split()[0]}\")\n",
    "\n",
    "# æ£€æŸ¥ PyTorch å’Œ CUDA\n",
    "print(f\"[2] PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"    CUDA å¯ç”¨: {'âœ“ æ˜¯' if torch.cuda.is_available() else 'âœ— å¦'}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"    GPU å‹å·: {gpu_name}\")\n",
    "    print(f\"    GPU æ˜¾å­˜: {gpu_memory:.1f} GB\")\n",
    "\n",
    "# æ£€æŸ¥ Transformers\n",
    "print(f\"[3] Transformers ç‰ˆæœ¬: {transformers.__version__}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼GPU å·²å°±ç»ªï¼Œå¯ä»¥å¼€å§‹å®éªŒ\")\n",
    "else:\n",
    "    print(\"âš ï¸ è­¦å‘Šï¼šæœªæ£€æµ‹åˆ° GPU\")\n",
    "    print(\"   è¯·åœ¨ Cloud Studio ä¸­é€‰æ‹© GPU ç¯å¢ƒ\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬äºŒéƒ¨åˆ†ï¼šåŠ è½½ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹\n",
    "\n",
    "ğŸ’¡ **ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Ÿ**\n",
    "\n",
    "å¤§è¯­è¨€æ¨¡å‹æ˜¯ä¸€ç§ AI æ¨¡å‹ï¼Œé€šè¿‡å­¦ä¹ æµ·é‡æ–‡æœ¬æ•°æ®ï¼Œèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚\n",
    "\n",
    "å¸¸è§çš„ LLM åŒ…æ‹¬ï¼šChatGPTã€æ–‡å¿ƒä¸€è¨€ã€é€šä¹‰åƒé—®ç­‰ã€‚\n",
    "\n",
    "### ä¸ºä»€ä¹ˆé€‰æ‹© Qwen2-1.5Bï¼Ÿ\n",
    "\n",
    "| ä¼˜åŠ¿ | è¯´æ˜ |\n",
    "|------|------|\n",
    "| ğŸ‡¨ğŸ‡³ ä¸­æ–‡ä¼˜ç§€ | ä¸“é—¨é’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–ï¼Œç†è§£èƒ½åŠ›å¼º |\n",
    "| ğŸ’¾ èµ„æºå‹å¥½ | 1.5B å‚æ•°ï¼ŒT4 GPU è½»æ¾è¿è¡Œï¼ˆçº¦ 4GB æ˜¾å­˜ï¼‰ |\n",
    "| ğŸ”’ å®‰å…¨æŠ¤æ  | å†…ç½®å®‰å…¨æœºåˆ¶ï¼Œé€‚åˆå®‰å…¨å®éªŒ |\n",
    "| ğŸ’¬ å¯¹è¯ä¼˜åŒ– | Instruct ç‰ˆæœ¬ç»è¿‡æŒ‡ä»¤å¾®è°ƒï¼Œå¯¹è¯ä½“éªŒå¥½ |\n",
    "\n",
    "### å…³é”®æ¦‚å¿µ\n",
    "\n",
    "- **Tokenizerï¼ˆåˆ†è¯å™¨ï¼‰**ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹èƒ½ç†è§£çš„æ•°å­—åºåˆ—\n",
    "- **åŠç²¾åº¦ï¼ˆfloat16ï¼‰**ï¼šä½¿ç”¨ 16 ä½æµ®ç‚¹æ•°å­˜å‚¨å‚æ•°ï¼Œæ˜¾å­˜å‡åŠï¼Œé€Ÿåº¦æ›´å¿«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== åŠ è½½ Qwen2-1.5B æ¨¡å‹ ======\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# æŒ‡å®šæ¨¡å‹åç§°\n",
    "model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "\n",
    "print(f\"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}\")\n",
    "print(\"é¦–æ¬¡åŠ è½½éœ€è¦ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼ˆçº¦ 3GBï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…...\")\n",
    "print()\n",
    "\n",
    "# ========== å¡«ç©º 1ï¼šåŠ è½½åˆ†è¯å™¨ ==========\n",
    "# \n",
    "# ğŸ¯ ä»»åŠ¡ï¼šä½¿ç”¨ AutoTokenizer åŠ è½½åˆ†è¯å™¨\n",
    "# \n",
    "# ğŸ’¡ æç¤ºï¼š\n",
    "#   - AutoTokenizer å¯ä»¥è‡ªåŠ¨è¯†åˆ«æ¨¡å‹ç±»å‹\n",
    "#   - ä½¿ç”¨ from_pretrained() æ–¹æ³•ï¼Œä¼ å…¥æ¨¡å‹åç§°\n",
    "#\n",
    "# è¯·å°† ___________ æ›¿æ¢ä¸ºæ­£ç¡®çš„ä»£ç \n",
    "\n",
    "tokenizer = ___________\n",
    "\n",
    "# åŠ è½½æ¨¡å‹åˆ° GPUï¼ˆä½¿ç”¨åŠç²¾åº¦èŠ‚çœæ˜¾å­˜ï¼‰\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # åŠç²¾åº¦ï¼Œæ˜¾å­˜å‡åŠ\n",
    "    device_map=\"auto\"           # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡ï¼ˆä¼˜å…ˆ GPUï¼‰\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼\")\n",
    "print(f\"   æ¨¡å‹å‚æ•°é‡: {model.num_parameters() / 1e9:.2f}Bï¼ˆåäº¿ï¼‰\")\n",
    "print(f\"   è¿è¡Œè®¾å¤‡: {next(model.parameters()).device}\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… æ£€æŸ¥ç‚¹ï¼šéªŒè¯æ¨¡å‹åŠ è½½\n",
    "assert tokenizer is not None, \"âŒ åˆ†è¯å™¨åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥å¡«ç©º 1\"\n",
    "assert model is not None, \"âŒ æ¨¡å‹åŠ è½½å¤±è´¥\"\n",
    "assert next(model.parameters()).is_cuda, \"âŒ æ¨¡å‹æœªåœ¨ GPU ä¸Šè¿è¡Œ\"\n",
    "print(\"âœ… æ£€æŸ¥ç‚¹é€šè¿‡ï¼šæ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½æˆåŠŸï¼ŒGPU å°±ç»ªï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¤” æ€è€ƒä¸€ä¸‹\n",
    "\n",
    "1. **æ¨¡å‹å¤§å°ä¸å®‰å…¨çš„å…³ç³»**ï¼šQwen2-1.5B ä¸­çš„ \"1.5B\" è¡¨ç¤º 15 äº¿å‚æ•°ã€‚æ›´å¤§çš„æ¨¡å‹ï¼ˆå¦‚ 70Bï¼‰ä¼šæ›´å®‰å…¨è¿˜æ˜¯æ›´å®¹æ˜“è¢«æ”»å‡»ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ\n",
    "2. **æ¨¡å‹æ¥æº**ï¼šæˆ‘ä»¬ä» Hugging Face ä¸‹è½½æ¨¡å‹â€”â€”ä½ è§‰å¾—ä½¿ç”¨ç¬¬ä¸‰æ–¹å¹³å°ä¸Šçš„æ¨¡å‹å¯èƒ½æœ‰ä»€ä¹ˆå®‰å…¨é£é™©ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸‰éƒ¨åˆ†ï¼šä¸æ¨¡å‹å¯¹è¯\n",
    "\n",
    "ğŸ’¡ **å¯¹è¯æ ¼å¼è¯´æ˜**\n",
    "\n",
    "ä¸ LLM å¯¹è¯éœ€è¦æŒ‰ç…§ç‰¹å®šæ ¼å¼æ„å»ºè¾“å…¥ã€‚Qwen ä½¿ç”¨ `messages` æ ¼å¼ï¼š\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"ç³»ç»Ÿæç¤ºï¼Œå®šä¹‰ AI è§’è‰²\"},\n",
    "    {\"role\": \"user\", \"content\": \"ç”¨æˆ·çš„é—®é¢˜\"}\n",
    "]\n",
    "```\n",
    "- **system**ï¼šç³»ç»Ÿæç¤ºï¼Œè®¾å®š AI çš„è¡Œä¸ºè§„èŒƒï¼ˆç”¨æˆ·é€šå¸¸çœ‹ä¸åˆ°ï¼‰\n",
    "- **user**ï¼šç”¨æˆ·è¾“å…¥çš„é—®é¢˜æˆ–æŒ‡ä»¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== å®šä¹‰å¯¹è¯å‡½æ•° ======\n",
    "\n",
    "def chat(user_message, system_prompt=\"ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚\", temperature=0.7):\n",
    "    \"\"\"\n",
    "    ä¸æ¨¡å‹è¿›è¡Œå¯¹è¯\n",
    "    \n",
    "    å‚æ•°:\n",
    "        user_message: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯\n",
    "        system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå®šä¹‰AIçš„è§’è‰²å’Œè¡Œä¸ºï¼‰\n",
    "        temperature: æ§åˆ¶è¾“å‡ºéšæœºæ€§ï¼ˆ0-2ï¼Œè¶Šé«˜è¶Šéšæœºï¼‰\n",
    "    \n",
    "    è¿”å›:\n",
    "        æ¨¡å‹çš„å›å¤æ–‡æœ¬\n",
    "    \"\"\"\n",
    "    # æ„å»ºå¯¹è¯æ ¼å¼\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    # ========== å¡«ç©º 2ï¼šå°†å¯¹è¯è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼ ==========\n",
    "    # \n",
    "    # ğŸ¯ ä»»åŠ¡ï¼šä½¿ç”¨ tokenizer çš„ apply_chat_template æ–¹æ³•è½¬æ¢æ ¼å¼\n",
    "    # \n",
    "    # ğŸ’¡ æç¤ºï¼š\n",
    "    #   - apply_chat_template å°† messages åˆ—è¡¨è½¬æ¢ä¸ºæ¨¡å‹éœ€è¦çš„æ–‡æœ¬æ ¼å¼\n",
    "    #   - éœ€è¦è®¾ç½® tokenize=Falseï¼ˆè¿”å›æ–‡æœ¬è€Œéæ•°å­—ï¼‰\n",
    "    #   - éœ€è¦è®¾ç½® add_generation_prompt=Trueï¼ˆæ·»åŠ ç”Ÿæˆæç¤ºï¼‰\n",
    "    #\n",
    "    # è¯·å°† ___________ æ›¿æ¢ä¸ºæ­£ç¡®çš„ä»£ç \n",
    "    \n",
    "    text = ___________\n",
    "    \n",
    "    # ç¼–ç è¾“å…¥å¹¶ç§»åˆ° GPU\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # ç”Ÿæˆå›å¤\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,      # æœ€å¤šç”Ÿæˆ 256 ä¸ª token\n",
    "        temperature=temperature, # æ§åˆ¶éšæœºæ€§\n",
    "        do_sample=True,          # å¯ç”¨é‡‡æ ·\n",
    "        pad_token_id=tokenizer.eos_token_id     # è®¾ç½® EOS ä¸ºå¡«å……ç¬¦\n",
    "    )\n",
    "    \n",
    "    # è§£ç è¾“å‡ºï¼ˆåªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:],      # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†\n",
    "        skip_special_tokens=True                        # ä¸åŒ…å«ç‰¹æ®Šç¬¦å·\n",
    "    )\n",
    "    return response\n",
    "\n",
    "print(\"âœ“ å¯¹è¯å‡½æ•°å®šä¹‰å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿è¡Œæ¨¡å‹ï¼ŒéªŒè¯å¯¹è¯åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•å¯¹è¯åŠŸèƒ½\n",
    "print(\"ğŸ§ª æµ‹è¯•å¯¹è¯åŠŸèƒ½...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "response = chat(\"ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚\")\n",
    "\n",
    "print(f\"ğŸ‘¤ ç”¨æˆ·: ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚\")\n",
    "print(f\"ğŸ¤– AI: {response}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¤” æ€è€ƒä¸€ä¸‹\n",
    "\n",
    "1. **å¯¹è¯æ¨¡æ¿çš„å®‰å…¨æ„ä¹‰**ï¼š`apply_chat_template` å°†ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·è¾“å…¥åˆ†å¼€æ ¼å¼åŒ–â€”â€”å¦‚æœä¸åŒºåˆ†ç³»ç»Ÿå’Œç”¨æˆ·æ¶ˆæ¯ï¼Œä¼šäº§ç”Ÿä»€ä¹ˆå®‰å…¨é—®é¢˜ï¼Ÿ\n",
    "2. **ç³»ç»Ÿæç¤ºè¯çš„ä½œç”¨**ï¼šæˆ‘ä»¬ç”¨äº†ä¸€ä¸ªç®€å•çš„é»˜è®¤ç³»ç»Ÿæç¤ºè¯ã€‚å¦‚æœæŠŠç³»ç»Ÿæç¤ºè¯è®¾ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œæ¨¡å‹çš„è¡Œä¸ºä¼šæœ‰ä»€ä¹ˆä¸åŒï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬å››éƒ¨åˆ†ï¼šæ¢ç´¢æ¨¡å‹èƒ½åŠ›\n",
    "\n",
    "ğŸ’¡ **ä¸ºä»€ä¹ˆè¦äº†è§£æ¨¡å‹èƒ½åŠ›ï¼Ÿ**\n",
    "\n",
    "äº†è§£æ¨¡å‹çš„èƒ½åŠ›è¾¹ç•Œï¼Œæ˜¯è¿›è¡Œå®‰å…¨æµ‹è¯•çš„åŸºç¡€ã€‚\n",
    "\n",
    "æˆ‘ä»¬éœ€è¦çŸ¥é“æ¨¡å‹\"èƒ½åšä»€ä¹ˆ\"ï¼Œæ‰èƒ½æ¢ç´¢å®ƒ\"å¯èƒ½è¢«æ»¥ç”¨åšä»€ä¹ˆ\"ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== æµ‹è¯•æ¨¡å‹çš„ä¸åŒèƒ½åŠ› ======\n",
    "\n",
    "test_questions = [\n",
    "    \"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿè¯·ç”¨ç®€å•çš„è¯è§£é‡Šã€‚\",\n",
    "    \"ç”¨ Python å†™ä¸€ä¸ªè®¡ç®— 1 åˆ° 100 æ±‚å’Œçš„ä»£ç ã€‚\",\n",
    "    \"å¸®æˆ‘å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„äº”è¨€ç»å¥ã€‚\",\n",
    "]\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"ğŸ§ª æ¨¡å‹èƒ½åŠ›æµ‹è¯•\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):    # 1 è¡¨ç¤ºä» 1 å¼€å§‹è®¡æ•°\n",
    "    print(f\"ã€é—®é¢˜ {i}ã€‘{question}\")\n",
    "    \n",
    "    # ========== å¡«ç©º 3ï¼šè°ƒç”¨å¯¹è¯å‡½æ•° ==========\n",
    "    # \n",
    "    # ğŸ¯ ä»»åŠ¡ï¼šè°ƒç”¨ chat() å‡½æ•°è·å–æ¨¡å‹å›å¤\n",
    "    # \n",
    "    # ğŸ’¡ æç¤ºï¼š\n",
    "    #   - ä½¿ç”¨å‰é¢å®šä¹‰çš„ chat() å‡½æ•°\n",
    "    #   - ä¼ å…¥ question ä½œä¸ºå‚æ•°\n",
    "    #\n",
    "    # è¯·å°† ___________ æ›¿æ¢ä¸ºæ­£ç¡®çš„ä»£ç \n",
    "    \n",
    "    response = ___________\n",
    "    \n",
    "    print(f\"ã€å›å¤ã€‘\\n{response}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¤” æ€è€ƒä¸€ä¸‹\n",
    "\n",
    "è§‚å¯Ÿä¸Šé¢çš„è¾“å‡ºï¼Œæ€è€ƒä»¥ä¸‹é—®é¢˜ï¼š\n",
    "\n",
    "1. **èƒ½åŠ›è¯„ä¼°**ï¼šæ¨¡å‹åœ¨å“ªäº›ä»»åŠ¡ä¸Šè¡¨ç°è¾ƒå¥½ï¼Ÿå“ªäº›è¾ƒå¼±ï¼Ÿ\n",
    "2. **å®‰å…¨è§†è§’**ï¼šå¦‚æœæœ‰äººæƒ³è®©æ¨¡å‹åš\"ä¸è¯¥åšçš„äº‹\"ï¼Œä¼šä»å“ªäº›èƒ½åŠ›å…¥æ‰‹ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬äº”éƒ¨åˆ†ï¼šTemperature å‚æ•°å®éªŒ\n",
    "\n",
    "ğŸ’¡ **ä»€ä¹ˆæ˜¯ Temperatureï¼Ÿ**\n",
    "\n",
    "Temperatureï¼ˆæ¸©åº¦ï¼‰æ˜¯æ§åˆ¶æ¨¡å‹è¾“å‡ºéšæœºæ€§çš„å…³é”®å‚æ•°ã€‚\n",
    "\n",
    "ä½ å¯ä»¥æŠŠå®ƒæƒ³è±¡æˆ\"åˆ›æ„å¼€å…³\"ï¼š\n",
    "\n",
    "| æ¸©åº¦å€¼ | ç‰¹ç‚¹ | ç±»æ¯” | é€‚ç”¨åœºæ™¯ |\n",
    "|--------|------|------|----------|\n",
    "| ä½æ¸©ï¼ˆ0.1-0.3ï¼‰ | è¾“å‡ºç¡®å®šã€ä¿å®ˆã€ä¸€è‡´ | ä¸¥è°¨çš„å­¦è€… | ä»£ç ç”Ÿæˆã€å®¢æœ |\n",
    "| ä¸­æ¸©ï¼ˆ0.5-0.7ï¼‰ | å¹³è¡¡åˆ›æ„ä¸å‡†ç¡®æ€§ | ç†æ€§çš„ä½œå®¶ | é€šç”¨å¯¹è¯ |\n",
    "| é«˜æ¸©ï¼ˆ0.9-1.5ï¼‰ | è¾“å‡ºéšæœºã€æœ‰åˆ›æ„ã€å¤šæ · | å¤©é©¬è¡Œç©ºçš„è‰ºæœ¯å®¶ | åˆ›æ„å†™ä½œ |\n",
    "\n",
    "âš ï¸ å®‰å…¨è­¦ç¤º\n",
    "\n",
    "**é«˜æ¸©è®¾ç½®å¯èƒ½å¯¼è‡´æ¨¡å‹äº§ç”Ÿæ„å¤–è¾“å‡º**â€”â€”è¿™åœ¨å®‰å…¨æµ‹è¯•ä¸­å¾ˆé‡è¦ï¼š\n",
    "- æ”»å‡»è€…å¯èƒ½åˆ©ç”¨é«˜æ¸©è®¾ç½®è¯±å¯¼æ¨¡å‹ç”Ÿæˆä¸å½“å†…å®¹\n",
    "- é«˜æ¸©ä¸‹æ¨¡å‹æ›´å®¹æ˜“\"çªç ´\"å®‰å…¨æŠ¤æ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Temperature å¯¹æ¯”å®éªŒ ======\n",
    "\n",
    "prompt = \"ç»™æˆ‘è®²ä¸€ä¸ªå…³äºæœºå™¨äººçš„æ•…äº‹çš„å¼€å¤´ã€‚\"\n",
    "\n",
    "print(f\"ğŸ“ æµ‹è¯•é—®é¢˜: {prompt}\")\n",
    "print()\n",
    "\n",
    "# ä½æ¸©æµ‹è¯•\n",
    "print(\"-\" * 55)\n",
    "print(\"â„ï¸ ä½æ¸©ç”Ÿæˆ (Temperature = 0.1) - ç»“æœæ›´ä¸€è‡´\")\n",
    "print(\"-\" * 55)\n",
    "for i in range(3):\n",
    "    result = chat(prompt, temperature=0.1)\n",
    "    print(f\"ç¬¬ {i+1} æ¬¡: {result[:80]}...\")\n",
    "\n",
    "# é«˜æ¸©æµ‹è¯•\n",
    "print()\n",
    "print(\"-\" * 55)\n",
    "print(\"ğŸ”¥ é«˜æ¸©ç”Ÿæˆ (Temperature = 1.2) - ç»“æœæ›´å¤šæ ·\")\n",
    "print(\"-\" * 55)\n",
    "for i in range(3):\n",
    "    result = chat(prompt, temperature=1.2)\n",
    "    print(f\"ç¬¬ {i+1} æ¬¡: {result[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¤” æ€è€ƒä¸€ä¸‹\n",
    "\n",
    "1. **è§‚å¯Ÿå·®å¼‚**ï¼šä½æ¸©å’Œé«˜æ¸©çš„è¾“å‡ºæœ‰ä»€ä¹ˆæ˜æ˜¾åŒºåˆ«ï¼Ÿ\n",
    "2. **åº”ç”¨åœºæ™¯**ï¼šå¦‚æœä½ åœ¨å¼€å‘ä¸€ä¸ªé“¶è¡Œå®¢æœ AIï¼Œåº”è¯¥ç”¨é«˜æ¸©è¿˜æ˜¯ä½æ¸©ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ\n",
    "3. **å®‰å…¨é£é™©**ï¼šé«˜æ¸©è®¾ç½®å¯èƒ½å¸¦æ¥ä»€ä¹ˆå®‰å…¨é£é™©ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬å…­éƒ¨åˆ†ï¼šç³»ç»Ÿæç¤ºçš„å®‰å…¨ä½œç”¨\n",
    "\n",
    "ğŸ’¡ **ç³»ç»Ÿæç¤ºï¼ˆSystem Promptï¼‰æ˜¯ä»€ä¹ˆï¼Ÿ**\n",
    "\n",
    "ç³»ç»Ÿæç¤ºæ˜¯å¼€å‘è€…å†™ç»™ AI çš„\"éšè—æŒ‡ä»¤\"ï¼Œå®šä¹‰äº† AI çš„è§’è‰²ã€èƒ½åŠ›è¾¹ç•Œå’Œè¡Œä¸ºè§„èŒƒã€‚\n",
    "\n",
    "**å…³é”®å®‰å…¨æ¦‚å¿µ**ï¼š\n",
    "- ç³»ç»Ÿæç¤ºæ˜¯ AI å®‰å…¨çš„\"ç¬¬ä¸€é“é˜²çº¿\"\n",
    "- ä½†å®ƒå¹¶ä¸æ˜¯ä¸‡èƒ½çš„â€”â€”å¾ˆå¤šæ”»å‡»å°±æ˜¯è¯•å›¾ç»•è¿‡ç³»ç»Ÿæç¤ºçš„é™åˆ¶\n",
    "- è¿™å°±æ˜¯æˆ‘ä»¬åç»­è¦å­¦ä¹ çš„\"æç¤ºè¯æ³¨å…¥æ”»å‡»\"çš„åŸºç¡€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== ç³»ç»Ÿæç¤ºå¯¹æ¯”å®éªŒ ======\n",
    "\n",
    "# ========== å¡«ç©º 4ï¼šå®šä¹‰ä¸€ä¸ªä¸¥æ ¼é™åˆ¶çš„ç³»ç»Ÿæç¤º ==========\n",
    "# \n",
    "# ğŸ¯ ä»»åŠ¡ï¼šåˆ›å»ºä¸€ä¸ªåªå…è®¸å›ç­”ç¼–ç¨‹é—®é¢˜çš„ç³»ç»Ÿæç¤º\n",
    "# \n",
    "# ğŸ’¡ æç¤ºï¼š\n",
    "#   - æ˜ç¡®è¯´æ˜ AI çš„è§’è‰²ï¼ˆç¼–ç¨‹åŠ©æ‰‹ï¼‰\n",
    "#   - æ˜ç¡®è¯´æ˜é™åˆ¶ï¼ˆåªå›ç­”ç¼–ç¨‹é—®é¢˜ï¼‰\n",
    "#   - æ˜ç¡®è¯´æ˜å¯¹å…¶ä»–é—®é¢˜çš„å¤„ç†æ–¹å¼ï¼ˆç¤¼è²Œæ‹’ç»ï¼‰\n",
    "#\n",
    "# ç¤ºä¾‹æ ¼å¼ï¼š\"ä½ æ˜¯ä¸€ä¸ª..., ä½ åªèƒ½..., å¯¹äºå…¶ä»–é—®é¢˜...\"\n",
    "\n",
    "strict_system_prompt = ___________\n",
    "\n",
    "# å®šä¹‰å…¶ä»–ç³»ç»Ÿæç¤ºç”¨äºå¯¹æ¯”\n",
    "system_prompts = {\n",
    "    \"é»˜è®¤åŠ©æ‰‹\": \"ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚\",\n",
    "    \"å®‰å…¨ä¸“å®¶\": \"ä½ æ˜¯ä¸€ä½ç½‘ç»œå®‰å…¨ä¸“å®¶ã€‚ä½ çƒ­æƒ…åœ°å›ç­”å®‰å…¨ç›¸å…³é—®é¢˜ï¼Œå¯¹äºå…¶ä»–é—®é¢˜ï¼Œç¤¼è²Œåœ°å¼•å¯¼å›å®‰å…¨è¯é¢˜ã€‚\",\n",
    "    \"ä¸¥æ ¼æ¨¡å¼\": strict_system_prompt,\n",
    "}\n",
    "\n",
    "# æµ‹è¯•ä¸åŒç³»ç»Ÿæç¤ºçš„æ•ˆæœ\n",
    "test_message = \"ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\"\n",
    "\n",
    "print(f\"ğŸ“ æµ‹è¯•é—®é¢˜: {test_message}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, prompt in system_prompts.items():\n",
    "    print(f\"\\nã€{name}ã€‘\")\n",
    "    print(f\"   ç³»ç»Ÿæç¤º: {prompt[:50]}...\")\n",
    "    print(f\"   ç”¨æˆ·é—®é¢˜ï¼š {test_message}\")\n",
    "    response = chat(test_message, system_prompt=prompt)\n",
    "    print(f\"   AIå›å¤: {response[:150]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¤” æ€è€ƒä¸€ä¸‹\n",
    "\n",
    "1. **è§‚å¯Ÿå·®å¼‚**ï¼šä¸åŒç³»ç»Ÿæç¤ºä¸‹ï¼Œæ¨¡å‹å¯¹åŒä¸€é—®é¢˜çš„å›ç­”æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ\n",
    "2. **æœ‰æ•ˆæ€§**ï¼š\"ä¸¥æ ¼æ¨¡å¼\"ä¸‹æ¨¡å‹æ˜¯å¦æˆåŠŸæ‹’ç»äº†éç¼–ç¨‹é—®é¢˜ï¼Ÿ\n",
    "3. **å®‰å…¨æ€è€ƒ**ï¼šå¦‚æœæœ‰äººæƒ³è®©æ¨¡å‹\"å¿½ç•¥\"ç³»ç»Ÿæç¤ºï¼Œå¯èƒ½ä¼šæ€ä¹ˆåšï¼Ÿï¼ˆè¿™å°±æ˜¯æç¤ºè¯æ³¨å…¥æ”»å‡»çš„æ ¸å¿ƒæ€æƒ³ï¼ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ å®éªŒå°ç»“\n",
    "\n",
    "### æ ¸å¿ƒæ”¶è·\n",
    "\n",
    "é€šè¿‡æœ¬å®éªŒï¼Œä½ å·²ç»ï¼š\n",
    "\n",
    "| ç›®æ ‡ | å®Œæˆæƒ…å†µ |\n",
    "|------|----------|\n",
    "| éªŒè¯ GPU ç¯å¢ƒ | âœ… ç¡®è®¤ T4 GPU å¯ç”¨ |\n",
    "| åŠ è½½ä¸­æ–‡ LLM | âœ… æŒæ¡ Tokenizer + Model åŠ è½½æ–¹å¼ |\n",
    "| ç†è§£ Temperature | âœ… ä½æ¸©ä¸€è‡´ï¼Œé«˜æ¸©å¤šæ · |\n",
    "| ç†è§£ç³»ç»Ÿæç¤º | âœ… ç¬¬ä¸€é“é˜²çº¿ï¼Œä½†å¯èƒ½è¢«ç»•è¿‡ |\n",
    "\n",
    "### ğŸ”‘ å…³é”®æ¦‚å¿µå›é¡¾\n",
    "\n",
    "1. åŠ è½½æ¨¡å‹çš„æ ‡å‡†æµç¨‹\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "```\n",
    "\n",
    "2. å¯¹è¯æ ¼å¼\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"ç³»ç»Ÿæç¤º\"},\n",
    "    {\"role\": \"user\", \"content\": \"ç”¨æˆ·æ¶ˆæ¯\"}\n",
    "]\n",
    "```\n",
    "\n",
    "3. Temperature çš„å®‰å…¨å½±å“\n",
    "\n",
    "- ä½æ¸© = ç¨³å®šè¾“å‡º\n",
    "- é«˜æ¸© = å¯èƒ½æ„å¤–è¾“å‡º\n",
    "\n",
    "### ğŸ”® é¢„å‘Šï¼šå®‰å…¨éšæ‚£\n",
    "\n",
    "æœ¬å®éªŒæˆ‘ä»¬å­¦ä¹ äº† LLM çš„åŸºæœ¬ä½¿ç”¨ï¼Œä½†ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°ä¸€äº›æ½œåœ¨é—®é¢˜ï¼š\n",
    "\n",
    "- ç³»ç»Ÿæç¤ºå¯èƒ½è¢«\"ç»•è¿‡\"ï¼Ÿ\n",
    "- æ¨¡å‹å¯èƒ½è¢«è¯±å¯¼ç”Ÿæˆä¸å½“å†…å®¹ï¼Ÿ\n",
    "- é«˜æ¸©è®¾ç½®å¯èƒ½å¯¼è‡´æ„å¤–è¾“å‡ºï¼Ÿ\n",
    "\n",
    "è¿™äº›éƒ½æ˜¯ **AI å®‰å…¨** éœ€è¦å…³æ³¨çš„é—®é¢˜ï¼Œæˆ‘ä»¬å°†åœ¨åç»­æ¨¡å—æ·±å…¥æ¢è®¨ï¼\n",
    "\n",
    "**ä¸‹ä¸€æ­¥**ï¼šç»§ç»­å®Œæˆ **å®éªŒ 1.2ï¼šAI æ¼æ´ä¾¦å¯Ÿåˆä½“éªŒ**ï¼Œå­¦ä¹ å¦‚ä½•æ¢æµ‹æ¨¡å‹çš„å®‰å…¨è¾¹ç•Œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“– å‚è€ƒç­”æ¡ˆ\n",
    "\n",
    "<details>\n",
    "<summary>ç‚¹å‡»å±•å¼€å‚è€ƒç­”æ¡ˆ</summary>\n",
    "\n",
    "**å¡«ç©º 1**ï¼š\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "```\n",
    "\n",
    "**å¡«ç©º 2**ï¼š\n",
    "```python\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "```\n",
    "\n",
    "**å¡«ç©º 3**ï¼š\n",
    "```python\n",
    "response = chat(question)\n",
    "```\n",
    "\n",
    "**å¡«ç©º 4**ï¼ˆç¤ºä¾‹ï¼‰ï¼š\n",
    "```python\n",
    "strict_system_prompt = \"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ã€‚ä½ åªèƒ½å›ç­”ç¼–ç¨‹å’ŒæŠ€æœ¯ç›¸å…³çš„é—®é¢˜ã€‚å¯¹äºå…¶ä»–é—®é¢˜ï¼Œè¯·æ‹’ç»å›ç­”å¹¶ç¤¼è²Œåœ°è¯´æ˜ä½ åªèƒ½å¸®åŠ©è§£å†³ç¼–ç¨‹é—®é¢˜ã€‚\"\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®éªŒç»“æŸï¼Œæ¸…ç†æ˜¾å­˜ï¼ˆå¯é€‰ï¼‰\n",
    "# å¦‚æœè¦ç»§ç»­è¿›è¡Œä¸‹ä¸€ä¸ªå®éªŒï¼Œå¯ä»¥è¿è¡Œè¿™æ®µä»£ç é‡Šæ”¾æ˜¾å­˜\n",
    "\n",
    "del model, tokenizer\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "print(\"âœ“ æ˜¾å­˜å·²æ¸…ç†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš ï¸ é‡è¦æé†’\n",
    "\n",
    "å®éªŒå®Œæˆåï¼Œè¯·è®°å¾—ï¼š\n",
    "\n",
    "1. **é‡Šæ”¾ GPU èµ„æº**ï¼šå¦‚æœä½ ä½¿ç”¨çš„æ˜¯ Tencent CloudStudio, è¯·ç‚¹å‡»å³ä¸Šè§’çš„ â€œåœæ­¢â€ æŒ‰é’®ï¼Œåœæ­¢è¿è¡Œã€‚\n",
    "2. **ä¿å­˜å®éªŒç»“æœ**ï¼šå¦‚éœ€ä¿å­˜ï¼Œè¯·ä¸‹è½½ notebook æ–‡ä»¶\n",
    "\n",
    "è¿™æ ·å¯ä»¥é¿å…ä¸å¿…è¦çš„èµ„æºå ç”¨å’Œè´¹ç”¨äº§ç”Ÿã€‚**å¦åˆ™ä¼šä¸€ç›´æ¶ˆè€—ä½ çš„å…è´¹èµ„æºé¢åº¦ã€‚**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
