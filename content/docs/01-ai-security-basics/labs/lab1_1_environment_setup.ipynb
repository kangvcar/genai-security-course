{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å®éªŒ 1.1ï¼šç¯å¢ƒæ­å»ºä¸æ¨¡å‹è°ƒç”¨\n",
    "\n",
    "## å®éªŒç›®æ ‡\n",
    "- ç†Ÿæ‚‰ Cloud Studio å®éªŒç¯å¢ƒ\n",
    "- å­¦ä¼šåŠ è½½å’Œè°ƒç”¨å¤§è¯­è¨€æ¨¡å‹\n",
    "- ç†è§£æ¨¡å‹å‚æ•°ï¼ˆTemperatureï¼‰å¯¹è¾“å‡ºçš„å½±å“\n",
    "- ä¸ºåç»­å®‰å…¨å®éªŒæ‰“ä¸‹åŸºç¡€\n",
    "\n",
    "## å®éªŒç¯å¢ƒ\n",
    "- å¹³å°ï¼šè…¾è®¯ Cloud Studioï¼ˆhttps://cloudstudio.net/ï¼‰\n",
    "- GPUï¼šNVIDIA Tesla T4ï¼ˆ16GB æ˜¾å­˜ï¼‰\n",
    "- æ¨¡å‹ï¼šQwen2-1.5B-Instructï¼ˆé˜¿é‡Œé€šä¹‰åƒé—®ï¼‰\n",
    "\n",
    "## é¢„è®¡æ—¶é—´ï¼š20åˆ†é’Ÿ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸€éƒ¨åˆ†ï¼šç¯å¢ƒå‡†å¤‡ä¸éªŒè¯\n",
    "\n",
    "> ğŸ’¡ **ä¸ºä»€ä¹ˆè¦æ£€æŸ¥ç¯å¢ƒï¼Ÿ**\n",
    "> \n",
    "> åœ¨è¿›è¡Œ AI å®éªŒå‰ï¼Œéœ€è¦ç¡®è®¤ GPU ç¯å¢ƒæ˜¯å¦æ­£ç¡®é…ç½®ã€‚GPUï¼ˆå›¾å½¢å¤„ç†å™¨ï¼‰èƒ½è®©æ¨¡å‹æ¨ç†é€Ÿåº¦æå‡ 10-100 å€ï¼\n",
    "> Cloud Studio å…è´¹æä¾› NVIDIA T4 GPUï¼ˆ16GB æ˜¾å­˜ï¼‰ï¼Œè¶³å¤Ÿè¿è¡Œæœ¬è¯¾ç¨‹æ‰€æœ‰å®éªŒã€‚\n",
    "\n",
    "### 1.1 å®‰è£…ä¾èµ–\n",
    "\n",
    "é¦–å…ˆå®‰è£…æœ¬å®éªŒéœ€è¦çš„ Python åº“ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…å¿…è¦çš„ä¾èµ–ï¼ˆé¦–æ¬¡è¿è¡Œéœ€è¦å‡ åˆ†é’Ÿï¼‰\n",
    "!pip install torch transformers accelerate -q\n",
    "\n",
    "print(\"âœ“ ä¾èµ–å®‰è£…å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 éªŒè¯ GPU ç¯å¢ƒ\n",
    "\n",
    "è¿è¡Œä¸‹é¢çš„ä»£ç ï¼Œæ£€æŸ¥ä½ çš„å®éªŒç¯å¢ƒæ˜¯å¦é…ç½®æ­£ç¡®ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== ç¯å¢ƒéªŒè¯è„šæœ¬ ======\n",
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ” AI å®‰å…¨å®éªŒç¯å¢ƒæ£€æŸ¥\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# æ£€æŸ¥ Python ç‰ˆæœ¬\n",
    "print(f\"\\n[1] Python ç‰ˆæœ¬: {sys.version.split()[0]}\")\n",
    "\n",
    "# æ£€æŸ¥ PyTorch å’Œ CUDA\n",
    "print(f\"[2] PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"    CUDA å¯ç”¨: {'âœ“ æ˜¯' if torch.cuda.is_available() else 'âœ— å¦'}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"    GPU å‹å·: {gpu_name}\")\n",
    "    print(f\"    GPU æ˜¾å­˜: {gpu_memory:.1f} GB\")\n",
    "\n",
    "# æ£€æŸ¥ Transformers\n",
    "print(f\"[3] Transformers ç‰ˆæœ¬: {transformers.__version__}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼GPU å·²å°±ç»ªï¼Œå¯ä»¥å¼€å§‹å®éªŒ\")\n",
    "else:\n",
    "    print(\"âš ï¸ è­¦å‘Šï¼šæœªæ£€æµ‹åˆ° GPU\")\n",
    "    print(\"   è¯·åœ¨ Cloud Studio ä¸­é€‰æ‹© GPU ç¯å¢ƒ\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… æ£€æŸ¥ç‚¹ 1\n",
    "\n",
    "è¿è¡Œä¸Šé¢çš„ä»£ç åï¼Œä½ åº”è¯¥çœ‹åˆ°ï¼š\n",
    "- [ ] CUDA å¯ç”¨æ˜¾ç¤º \"âœ“ æ˜¯\"\n",
    "- [ ] GPU å‹å·æ˜¾ç¤º \"Tesla T4\" æˆ–ç±»ä¼¼\n",
    "- [ ] GPU æ˜¾å­˜çº¦ 16 GB\n",
    "\n",
    "å¦‚æœæ˜¾ç¤º \"æœªæ£€æµ‹åˆ° GPU\"ï¼Œè¯·ç¡®è®¤å·²åœ¨ Cloud Studio ä¸­é€‰æ‹© GPU ç¯å¢ƒã€‚\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬äºŒéƒ¨åˆ†ï¼šåŠ è½½ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹\n",
    "\n",
    "> ğŸ’¡ **ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Ÿ**\n",
    "> \n",
    "> å¤§è¯­è¨€æ¨¡å‹æ˜¯ä¸€ç§ AI æ¨¡å‹ï¼Œé€šè¿‡å­¦ä¹ æµ·é‡æ–‡æœ¬æ•°æ®ï¼Œèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚\n",
    "> å¸¸è§çš„ LLM åŒ…æ‹¬ï¼šChatGPTã€æ–‡å¿ƒä¸€è¨€ã€é€šä¹‰åƒé—®ç­‰ã€‚\n",
    "\n",
    "### ä¸ºä»€ä¹ˆé€‰æ‹© Qwen2-1.5Bï¼Ÿ\n",
    "\n",
    "| ä¼˜åŠ¿ | è¯´æ˜ |\n",
    "|------|------|\n",
    "| ğŸ‡¨ğŸ‡³ ä¸­æ–‡ä¼˜ç§€ | ä¸“é—¨é’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–ï¼Œç†è§£èƒ½åŠ›å¼º |\n",
    "| ğŸ’¾ èµ„æºå‹å¥½ | 1.5B å‚æ•°ï¼ŒT4 GPU è½»æ¾è¿è¡Œï¼ˆçº¦ 4GB æ˜¾å­˜ï¼‰ |\n",
    "| ğŸ”’ å®‰å…¨æŠ¤æ  | å†…ç½®å®‰å…¨æœºåˆ¶ï¼Œé€‚åˆå®‰å…¨å®éªŒ |\n",
    "| ğŸ’¬ å¯¹è¯ä¼˜åŒ– | Instruct ç‰ˆæœ¬ç»è¿‡æŒ‡ä»¤å¾®è°ƒï¼Œå¯¹è¯ä½“éªŒå¥½ |\n",
    "\n",
    "### å…³é”®æ¦‚å¿µ\n",
    "\n",
    "- **Tokenizerï¼ˆåˆ†è¯å™¨ï¼‰**ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹èƒ½ç†è§£çš„æ•°å­—åºåˆ—\n",
    "- **åŠç²¾åº¦ï¼ˆfloat16ï¼‰**ï¼šä½¿ç”¨ 16 ä½æµ®ç‚¹æ•°å­˜å‚¨å‚æ•°ï¼Œæ˜¾å­˜å‡åŠï¼Œé€Ÿåº¦æ›´å¿«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== åŠ è½½ Qwen2-1.5B æ¨¡å‹ ======\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# æŒ‡å®šæ¨¡å‹åç§°\n",
    "model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "\n",
    "print(f\"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}\")\n",
    "print(\"é¦–æ¬¡åŠ è½½éœ€è¦ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼ˆçº¦ 3GBï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…...\")\n",
    "print()\n",
    "\n",
    "# ========== å¡«ç©º 1ï¼šåŠ è½½åˆ†è¯å™¨ ==========\n",
    "# \n",
    "# ğŸ¯ ä»»åŠ¡ï¼šä½¿ç”¨ AutoTokenizer åŠ è½½åˆ†è¯å™¨\n",
    "# \n",
    "# ğŸ’¡ æç¤ºï¼š\n",
    "#   - AutoTokenizer å¯ä»¥è‡ªåŠ¨è¯†åˆ«æ¨¡å‹ç±»å‹\n",
    "#   - ä½¿ç”¨ from_pretrained() æ–¹æ³•ï¼Œä¼ å…¥æ¨¡å‹åç§°\n",
    "#\n",
    "# è¯·å°† ___________ æ›¿æ¢ä¸ºæ­£ç¡®çš„ä»£ç \n",
    "\n",
    "tokenizer = ___________\n",
    "\n",
    "# åŠ è½½æ¨¡å‹åˆ° GPUï¼ˆä½¿ç”¨åŠç²¾åº¦èŠ‚çœæ˜¾å­˜ï¼‰\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # åŠç²¾åº¦ï¼Œæ˜¾å­˜å‡åŠ\n",
    "    device_map=\"auto\"           # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡ï¼ˆä¼˜å…ˆ GPUï¼‰\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼\")\n",
    "print(f\"   æ¨¡å‹å‚æ•°é‡: {model.num_parameters() / 1e9:.2f}Bï¼ˆåäº¿ï¼‰\")\n",
    "print(f\"   è¿è¡Œè®¾å¤‡: {next(model.parameters()).device}\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… æ£€æŸ¥ç‚¹ 2\n",
    "\n",
    "è¿è¡Œä¸Šé¢çš„ä»£ç åï¼Œä½ åº”è¯¥çœ‹åˆ°ï¼š\n",
    "- [ ] æ¨¡å‹åŠ è½½æˆåŠŸ\n",
    "- [ ] å‚æ•°é‡çº¦ 1.54B\n",
    "- [ ] è¿è¡Œè®¾å¤‡æ˜¾ç¤º cuda:0ï¼ˆè¡¨ç¤ºåœ¨ GPU ä¸Šï¼‰\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸‰éƒ¨åˆ†ï¼šä¸æ¨¡å‹å¯¹è¯\n",
    "\n",
    "> ğŸ’¡ **å¯¹è¯æ ¼å¼è¯´æ˜**\n",
    "> \n",
    "> ä¸ LLM å¯¹è¯éœ€è¦æŒ‰ç…§ç‰¹å®šæ ¼å¼æ„å»ºè¾“å…¥ã€‚Qwen ä½¿ç”¨ `messages` æ ¼å¼ï¼š\n",
    "> ```python\n",
    "> messages = [\n",
    ">     {\"role\": \"system\", \"content\": \"ç³»ç»Ÿæç¤ºï¼Œå®šä¹‰ AI è§’è‰²\"},\n",
    ">     {\"role\": \"user\", \"content\": \"ç”¨æˆ·çš„é—®é¢˜\"}\n",
    "> ]\n",
    "> ```\n",
    "> - **system**ï¼šç³»ç»Ÿæç¤ºï¼Œè®¾å®š AI çš„è¡Œä¸ºè§„èŒƒï¼ˆç”¨æˆ·é€šå¸¸çœ‹ä¸åˆ°ï¼‰\n",
    "> - **user**ï¼šç”¨æˆ·è¾“å…¥çš„é—®é¢˜æˆ–æŒ‡ä»¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== å®šä¹‰å¯¹è¯å‡½æ•° ======\n",
    "\n",
    "def chat(user_message, system_prompt=\"ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚\", temperature=0.7):\n",
    "    \"\"\"\n",
    "    ä¸æ¨¡å‹è¿›è¡Œå¯¹è¯\n",
    "    \n",
    "    å‚æ•°:\n",
    "        user_message: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯\n",
    "        system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå®šä¹‰AIçš„è§’è‰²å’Œè¡Œä¸ºï¼‰\n",
    "        temperature: æ§åˆ¶è¾“å‡ºéšæœºæ€§ï¼ˆ0-2ï¼Œè¶Šé«˜è¶Šéšæœºï¼‰\n",
    "    \n",
    "    è¿”å›:\n",
    "        æ¨¡å‹çš„å›å¤æ–‡æœ¬\n",
    "    \"\"\"\n",
    "    # æ„å»ºå¯¹è¯æ ¼å¼\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    # ========== å¡«ç©º 2ï¼šå°†å¯¹è¯è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼ ==========\n",
    "    # \n",
    "    # ğŸ¯ ä»»åŠ¡ï¼šä½¿ç”¨ tokenizer çš„ apply_chat_template æ–¹æ³•è½¬æ¢æ ¼å¼\n",
    "    # \n",
    "    # ğŸ’¡ æç¤ºï¼š\n",
    "    #   - apply_chat_template å°† messages åˆ—è¡¨è½¬æ¢ä¸ºæ¨¡å‹éœ€è¦çš„æ–‡æœ¬æ ¼å¼\n",
    "    #   - éœ€è¦è®¾ç½® tokenize=Falseï¼ˆè¿”å›æ–‡æœ¬è€Œéæ•°å­—ï¼‰\n",
    "    #   - éœ€è¦è®¾ç½® add_generation_prompt=Trueï¼ˆæ·»åŠ ç”Ÿæˆæç¤ºï¼‰\n",
    "    #\n",
    "    # è¯·å°† ___________ æ›¿æ¢ä¸ºæ­£ç¡®çš„ä»£ç \n",
    "    \n",
    "    text = ___________\n",
    "    \n",
    "    # ç¼–ç è¾“å…¥å¹¶ç§»åˆ° GPU\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # ç”Ÿæˆå›å¤\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,      # æœ€å¤šç”Ÿæˆ 256 ä¸ª token\n",
    "        temperature=temperature, # æ§åˆ¶éšæœºæ€§\n",
    "        do_sample=True,          # å¯ç”¨é‡‡æ ·\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # è§£ç è¾“å‡ºï¼ˆåªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    return response\n",
    "\n",
    "print(\"âœ“ å¯¹è¯å‡½æ•°å®šä¹‰å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•å¯¹è¯åŠŸèƒ½\n",
    "print(\"ğŸ§ª æµ‹è¯•å¯¹è¯åŠŸèƒ½...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "response = chat(\"ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚\")\n",
    "\n",
    "print(f\"ğŸ‘¤ ç”¨æˆ·: ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚\")\n",
    "print(f\"ğŸ¤– AI: {response}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬å››éƒ¨åˆ†ï¼šæ¢ç´¢æ¨¡å‹èƒ½åŠ›\n",
    "\n",
    "> ğŸ’¡ **ä¸ºä»€ä¹ˆè¦äº†è§£æ¨¡å‹èƒ½åŠ›ï¼Ÿ**\n",
    "> \n",
    "> äº†è§£æ¨¡å‹çš„èƒ½åŠ›è¾¹ç•Œï¼Œæ˜¯è¿›è¡Œå®‰å…¨æµ‹è¯•çš„åŸºç¡€ã€‚\n",
    "> æˆ‘ä»¬éœ€è¦çŸ¥é“æ¨¡å‹\"èƒ½åšä»€ä¹ˆ\"ï¼Œæ‰èƒ½æ¢ç´¢å®ƒ\"å¯èƒ½è¢«æ»¥ç”¨åšä»€ä¹ˆ\"ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== æµ‹è¯•æ¨¡å‹çš„ä¸åŒèƒ½åŠ› ======\n",
    "\n",
    "test_questions = [\n",
    "    \"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿè¯·ç”¨ç®€å•çš„è¯è§£é‡Šã€‚\",\n",
    "    \"ç”¨ Python å†™ä¸€ä¸ªè®¡ç®— 1 åˆ° 100 æ±‚å’Œçš„ä»£ç ã€‚\",\n",
    "    \"å¸®æˆ‘å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„äº”è¨€ç»å¥ã€‚\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ§ª æ¨¡å‹èƒ½åŠ›æµ‹è¯•\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\nã€é—®é¢˜ {i}ã€‘{question}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # ========== å¡«ç©º 3ï¼šè°ƒç”¨å¯¹è¯å‡½æ•° ==========\n",
    "    # \n",
    "    # ğŸ¯ ä»»åŠ¡ï¼šè°ƒç”¨ chat() å‡½æ•°è·å–æ¨¡å‹å›å¤\n",
    "    # \n",
    "    # ğŸ’¡ æç¤ºï¼š\n",
    "    #   - ä½¿ç”¨å‰é¢å®šä¹‰çš„ chat() å‡½æ•°\n",
    "    #   - ä¼ å…¥ question ä½œä¸ºå‚æ•°\n",
    "    #\n",
    "    # è¯·å°† ___________ æ›¿æ¢ä¸ºæ­£ç¡®çš„ä»£ç \n",
    "    \n",
    "    response = ___________\n",
    "    \n",
    "    print(f\"ã€å›å¤ã€‘\\n{response}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¤” æ€è€ƒä¸€ä¸‹\n",
    "\n",
    "è§‚å¯Ÿä¸Šé¢çš„è¾“å‡ºï¼Œæ€è€ƒä»¥ä¸‹é—®é¢˜ï¼š\n",
    "\n",
    "1. **èƒ½åŠ›è¯„ä¼°**ï¼šæ¨¡å‹åœ¨å“ªäº›ä»»åŠ¡ä¸Šè¡¨ç°è¾ƒå¥½ï¼Ÿå“ªäº›è¾ƒå¼±ï¼Ÿ\n",
    "2. **å®‰å…¨è§†è§’**ï¼šå¦‚æœæœ‰äººæƒ³è®©æ¨¡å‹åš\"ä¸è¯¥åšçš„äº‹\"ï¼Œä¼šä»å“ªäº›èƒ½åŠ›å…¥æ‰‹ï¼Ÿ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬äº”éƒ¨åˆ†ï¼šTemperature å‚æ•°å®éªŒ\n",
    "\n",
    "> ğŸ’¡ **ä»€ä¹ˆæ˜¯ Temperatureï¼Ÿ**\n",
    "> \n",
    "> Temperatureï¼ˆæ¸©åº¦ï¼‰æ˜¯æ§åˆ¶æ¨¡å‹è¾“å‡ºéšæœºæ€§çš„å…³é”®å‚æ•°ã€‚\n",
    "> ä½ å¯ä»¥æŠŠå®ƒæƒ³è±¡æˆ\"åˆ›æ„å¼€å…³\"ï¼š\n",
    "\n",
    "| æ¸©åº¦å€¼ | ç‰¹ç‚¹ | ç±»æ¯” | é€‚ç”¨åœºæ™¯ |\n",
    "|--------|------|------|----------|\n",
    "| ä½æ¸©ï¼ˆ0.1-0.3ï¼‰ | è¾“å‡ºç¡®å®šã€ä¿å®ˆã€ä¸€è‡´ | ä¸¥è°¨çš„å­¦è€… | ä»£ç ç”Ÿæˆã€å®¢æœ |\n",
    "| ä¸­æ¸©ï¼ˆ0.5-0.7ï¼‰ | å¹³è¡¡åˆ›æ„ä¸å‡†ç¡®æ€§ | ç†æ€§çš„ä½œå®¶ | é€šç”¨å¯¹è¯ |\n",
    "| é«˜æ¸©ï¼ˆ0.9-1.5ï¼‰ | è¾“å‡ºéšæœºã€æœ‰åˆ›æ„ã€å¤šæ · | å¤©é©¬è¡Œç©ºçš„è‰ºæœ¯å®¶ | åˆ›æ„å†™ä½œ |\n",
    "\n",
    "### âš ï¸ å®‰å…¨è­¦ç¤º\n",
    "\n",
    "**é«˜æ¸©è®¾ç½®å¯èƒ½å¯¼è‡´æ¨¡å‹äº§ç”Ÿæ„å¤–è¾“å‡º**â€”â€”è¿™åœ¨å®‰å…¨æµ‹è¯•ä¸­å¾ˆé‡è¦ï¼š\n",
    "- æ”»å‡»è€…å¯èƒ½åˆ©ç”¨é«˜æ¸©è®¾ç½®è¯±å¯¼æ¨¡å‹ç”Ÿæˆä¸å½“å†…å®¹\n",
    "- é«˜æ¸©ä¸‹æ¨¡å‹æ›´å®¹æ˜“\"çªç ´\"å®‰å…¨æŠ¤æ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Temperature å¯¹æ¯”å®éªŒ ======\n",
    "\n",
    "prompt = \"ç»™æˆ‘è®²ä¸€ä¸ªå…³äºæœºå™¨äººçš„æ•…äº‹çš„å¼€å¤´ã€‚\"\n",
    "\n",
    "print(f\"ğŸ“ æµ‹è¯•é—®é¢˜: {prompt}\")\n",
    "print()\n",
    "\n",
    "# ä½æ¸©æµ‹è¯•\n",
    "print(\"=\" * 55)\n",
    "print(\"â„ï¸ ä½æ¸©ç”Ÿæˆ (Temperature = 0.1) - ç»“æœæ›´ä¸€è‡´\")\n",
    "print(\"=\" * 55)\n",
    "for i in range(3):\n",
    "    result = chat(prompt, temperature=0.1)\n",
    "    print(f\"ç¬¬ {i+1} æ¬¡: {result[:80]}...\")\n",
    "\n",
    "# é«˜æ¸©æµ‹è¯•\n",
    "print()\n",
    "print(\"=\" * 55)\n",
    "print(\"ğŸ”¥ é«˜æ¸©ç”Ÿæˆ (Temperature = 1.2) - ç»“æœæ›´å¤šæ ·\")\n",
    "print(\"=\" * 55)\n",
    "for i in range(3):\n",
    "    result = chat(prompt, temperature=1.2)\n",
    "    print(f\"ç¬¬ {i+1} æ¬¡: {result[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¤” æ€è€ƒä¸€ä¸‹\n",
    "\n",
    "1. **è§‚å¯Ÿå·®å¼‚**ï¼šä½æ¸©å’Œé«˜æ¸©çš„è¾“å‡ºæœ‰ä»€ä¹ˆæ˜æ˜¾åŒºåˆ«ï¼Ÿ\n",
    "2. **åº”ç”¨åœºæ™¯**ï¼šå¦‚æœä½ åœ¨å¼€å‘ä¸€ä¸ªé“¶è¡Œå®¢æœ AIï¼Œåº”è¯¥ç”¨é«˜æ¸©è¿˜æ˜¯ä½æ¸©ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ\n",
    "3. **å®‰å…¨é£é™©**ï¼šé«˜æ¸©è®¾ç½®å¯èƒ½å¸¦æ¥ä»€ä¹ˆå®‰å…¨é£é™©ï¼Ÿ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬å…­éƒ¨åˆ†ï¼šç³»ç»Ÿæç¤ºçš„å®‰å…¨ä½œç”¨\n",
    "\n",
    "> ğŸ’¡ **ç³»ç»Ÿæç¤ºï¼ˆSystem Promptï¼‰æ˜¯ä»€ä¹ˆï¼Ÿ**\n",
    "> \n",
    "> ç³»ç»Ÿæç¤ºæ˜¯å¼€å‘è€…å†™ç»™ AI çš„\"éšè—æŒ‡ä»¤\"ï¼Œå®šä¹‰äº† AI çš„è§’è‰²ã€èƒ½åŠ›è¾¹ç•Œå’Œè¡Œä¸ºè§„èŒƒã€‚\n",
    "> \n",
    "> **å…³é”®å®‰å…¨æ¦‚å¿µ**ï¼š\n",
    "> - ç³»ç»Ÿæç¤ºæ˜¯ AI å®‰å…¨çš„\"ç¬¬ä¸€é“é˜²çº¿\"\n",
    "> - ä½†å®ƒå¹¶ä¸æ˜¯ä¸‡èƒ½çš„â€”â€”å¾ˆå¤šæ”»å‡»å°±æ˜¯è¯•å›¾ç»•è¿‡ç³»ç»Ÿæç¤ºçš„é™åˆ¶\n",
    "> - è¿™å°±æ˜¯æˆ‘ä»¬åç»­è¦å­¦ä¹ çš„\"æç¤ºè¯æ³¨å…¥æ”»å‡»\"çš„åŸºç¡€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== ç³»ç»Ÿæç¤ºå¯¹æ¯”å®éªŒ ======\n",
    "\n",
    "# ========== å¡«ç©º 4ï¼šå®šä¹‰ä¸€ä¸ªä¸¥æ ¼é™åˆ¶çš„ç³»ç»Ÿæç¤º ==========\n",
    "# \n",
    "# ğŸ¯ ä»»åŠ¡ï¼šåˆ›å»ºä¸€ä¸ªåªå…è®¸å›ç­”ç¼–ç¨‹é—®é¢˜çš„ç³»ç»Ÿæç¤º\n",
    "# \n",
    "# ğŸ’¡ æç¤ºï¼š\n",
    "#   - æ˜ç¡®è¯´æ˜ AI çš„è§’è‰²ï¼ˆç¼–ç¨‹åŠ©æ‰‹ï¼‰\n",
    "#   - æ˜ç¡®è¯´æ˜é™åˆ¶ï¼ˆåªå›ç­”ç¼–ç¨‹é—®é¢˜ï¼‰\n",
    "#   - æ˜ç¡®è¯´æ˜å¯¹å…¶ä»–é—®é¢˜çš„å¤„ç†æ–¹å¼ï¼ˆç¤¼è²Œæ‹’ç»ï¼‰\n",
    "#\n",
    "# ç¤ºä¾‹æ ¼å¼ï¼š\"ä½ æ˜¯ä¸€ä¸ª..., ä½ åªèƒ½..., å¯¹äºå…¶ä»–é—®é¢˜...\"\n",
    "\n",
    "strict_system_prompt = ___________\n",
    "\n",
    "# å®šä¹‰å…¶ä»–ç³»ç»Ÿæç¤ºç”¨äºå¯¹æ¯”\n",
    "system_prompts = {\n",
    "    \"é»˜è®¤åŠ©æ‰‹\": \"ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚\",\n",
    "    \"å®‰å…¨ä¸“å®¶\": \"ä½ æ˜¯ä¸€ä½ç½‘ç»œå®‰å…¨ä¸“å®¶ã€‚ä½ çƒ­æƒ…åœ°å›ç­”å®‰å…¨ç›¸å…³é—®é¢˜ï¼Œå¯¹äºå…¶ä»–é—®é¢˜ï¼Œç¤¼è²Œåœ°å¼•å¯¼å›å®‰å…¨è¯é¢˜ã€‚\",\n",
    "    \"ä¸¥æ ¼æ¨¡å¼\": strict_system_prompt,\n",
    "}\n",
    "\n",
    "# æµ‹è¯•ä¸åŒç³»ç»Ÿæç¤ºçš„æ•ˆæœ\n",
    "test_message = \"ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\"\n",
    "\n",
    "print(f\"ğŸ“ æµ‹è¯•é—®é¢˜: {test_message}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, prompt in system_prompts.items():\n",
    "    print(f\"\\nã€{name}ã€‘\")\n",
    "    print(f\"   ç³»ç»Ÿæç¤º: {prompt[:50]}...\")\n",
    "    print(\"-\" * 50)\n",
    "    response = chat(test_message, system_prompt=prompt)\n",
    "    print(f\"   å›å¤: {response[:150]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¤” æ€è€ƒä¸€ä¸‹\n",
    "\n",
    "1. **è§‚å¯Ÿå·®å¼‚**ï¼šä¸åŒç³»ç»Ÿæç¤ºä¸‹ï¼Œæ¨¡å‹å¯¹åŒä¸€é—®é¢˜çš„å›ç­”æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ\n",
    "2. **æœ‰æ•ˆæ€§**ï¼š\"ä¸¥æ ¼æ¨¡å¼\"ä¸‹æ¨¡å‹æ˜¯å¦æˆåŠŸæ‹’ç»äº†éç¼–ç¨‹é—®é¢˜ï¼Ÿ\n",
    "3. **å®‰å…¨æ€è€ƒ**ï¼šå¦‚æœæœ‰äººæƒ³è®©æ¨¡å‹\"å¿½ç•¥\"ç³»ç»Ÿæç¤ºï¼Œå¯èƒ½ä¼šæ€ä¹ˆåšï¼Ÿï¼ˆè¿™å°±æ˜¯æç¤ºè¯æ³¨å…¥æ”»å‡»çš„æ ¸å¿ƒæ€æƒ³ï¼ï¼‰\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ å®éªŒå°ç»“\n",
    "\n",
    "### æ ¸å¿ƒæ”¶è·\n",
    "\n",
    "é€šè¿‡æœ¬å®éªŒï¼Œä½ å·²ç»ï¼š\n",
    "\n",
    "| ç›®æ ‡ | å®Œæˆæƒ…å†µ |\n",
    "|------|----------|\n",
    "| éªŒè¯ GPU ç¯å¢ƒ | âœ… ç¡®è®¤ T4 GPU å¯ç”¨ |\n",
    "| åŠ è½½ä¸­æ–‡ LLM | âœ… æŒæ¡ Tokenizer + Model åŠ è½½æ–¹å¼ |\n",
    "| ç†è§£ Temperature | âœ… ä½æ¸©ä¸€è‡´ï¼Œé«˜æ¸©å¤šæ · |\n",
    "| ç†è§£ç³»ç»Ÿæç¤º | âœ… ç¬¬ä¸€é“é˜²çº¿ï¼Œä½†å¯èƒ½è¢«ç»•è¿‡ |\n",
    "\n",
    "### ğŸ”‘ å…³é”®æ¦‚å¿µå›é¡¾\n",
    "\n",
    "```python\n",
    "# 1. åŠ è½½æ¨¡å‹çš„æ ‡å‡†æµç¨‹\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# 2. å¯¹è¯æ ¼å¼\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"ç³»ç»Ÿæç¤º\"},\n",
    "    {\"role\": \"user\", \"content\": \"ç”¨æˆ·æ¶ˆæ¯\"}\n",
    "]\n",
    "\n",
    "# 3. Temperature çš„å®‰å…¨å½±å“\n",
    "# ä½æ¸© = ç¨³å®šè¾“å‡ºï¼Œé«˜æ¸© = å¯èƒ½æ„å¤–è¾“å‡º\n",
    "```\n",
    "\n",
    "### ğŸ”® é¢„å‘Šï¼šå®‰å…¨éšæ‚£\n",
    "\n",
    "æœ¬å®éªŒæˆ‘ä»¬å­¦ä¹ äº† LLM çš„åŸºæœ¬ä½¿ç”¨ï¼Œä½†ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°ä¸€äº›æ½œåœ¨é—®é¢˜ï¼š\n",
    "\n",
    "- ç³»ç»Ÿæç¤ºå¯èƒ½è¢«\"ç»•è¿‡\"ï¼Ÿ\n",
    "- æ¨¡å‹å¯èƒ½è¢«è¯±å¯¼ç”Ÿæˆä¸å½“å†…å®¹ï¼Ÿ\n",
    "- é«˜æ¸©è®¾ç½®å¯èƒ½å¯¼è‡´æ„å¤–è¾“å‡ºï¼Ÿ\n",
    "\n",
    "è¿™äº›éƒ½æ˜¯ **AI å®‰å…¨** éœ€è¦å…³æ³¨çš„é—®é¢˜ï¼Œæˆ‘ä»¬å°†åœ¨åç»­æ¨¡å—æ·±å…¥æ¢è®¨ï¼\n",
    "\n",
    "---\n",
    "\n",
    "## â­ï¸ ä¸‹ä¸€æ­¥\n",
    "\n",
    "ç»§ç»­å®Œæˆ **å®éªŒ 1.2ï¼šAI æ¼æ´ä¾¦å¯Ÿåˆä½“éªŒ**ï¼Œå­¦ä¹ å¦‚ä½•æ¢æµ‹æ¨¡å‹çš„å®‰å…¨è¾¹ç•Œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“– å‚è€ƒç­”æ¡ˆ\n",
    "\n",
    "<details>\n",
    "<summary>ç‚¹å‡»å±•å¼€å‚è€ƒç­”æ¡ˆ</summary>\n",
    "\n",
    "**å¡«ç©º 1**ï¼š\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "```\n",
    "\n",
    "**å¡«ç©º 2**ï¼š\n",
    "```python\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "```\n",
    "\n",
    "**å¡«ç©º 3**ï¼š\n",
    "```python\n",
    "response = chat(question)\n",
    "```\n",
    "\n",
    "**å¡«ç©º 4**ï¼ˆç¤ºä¾‹ï¼‰ï¼š\n",
    "```python\n",
    "strict_system_prompt = \"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ã€‚ä½ åªèƒ½å›ç­”ç¼–ç¨‹å’ŒæŠ€æœ¯ç›¸å…³çš„é—®é¢˜ã€‚å¯¹äºå…¶ä»–é—®é¢˜ï¼Œè¯·ç¤¼è²Œåœ°è¯´æ˜ä½ åªèƒ½å¸®åŠ©è§£å†³ç¼–ç¨‹é—®é¢˜ã€‚\"\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®éªŒç»“æŸï¼Œæ¸…ç†æ˜¾å­˜ï¼ˆå¯é€‰ï¼‰\n",
    "# å¦‚æœè¦ç»§ç»­è¿›è¡Œä¸‹ä¸€ä¸ªå®éªŒï¼Œå¯ä»¥è¿è¡Œè¿™æ®µä»£ç é‡Šæ”¾æ˜¾å­˜\n",
    "\n",
    "# del model, tokenizer\n",
    "# import torch\n",
    "# torch.cuda.empty_cache()\n",
    "# print(\"âœ“ æ˜¾å­˜å·²æ¸…ç†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸€éƒ¨åˆ†ï¼šç¯å¢ƒéªŒè¯\n",
    "\n",
    "é¦–å…ˆç¡®è®¤ GPU ç¯å¢ƒæ˜¯å¦æ­£ç¡®é…ç½®ã€‚Cloud Studio æä¾›å…è´¹çš„ T4 GPUã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== ç¯å¢ƒéªŒè¯è„šæœ¬ ======\n",
    "print(\"=\" * 50)\n",
    "print(\"AI å®‰å…¨å®éªŒç¯å¢ƒæ£€æŸ¥\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# æ£€æŸ¥ Python ç‰ˆæœ¬\n",
    "import sys\n",
    "print(f\"\\n[1] Python ç‰ˆæœ¬: {sys.version.split()[0]}\")\n",
    "\n",
    "# æ£€æŸ¥ PyTorch å’Œ CUDA\n",
    "import torch\n",
    "print(f\"[2] PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"    CUDA å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"    GPU å‹å·: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"    GPU æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# æ£€æŸ¥ Transformers\n",
    "import transformers\n",
    "print(f\"[3] Transformers ç‰ˆæœ¬: {transformers.__version__}\")\n",
    "\n",
    "# æ£€æŸ¥å…¶ä»–ä¾èµ–\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "print(f\"[4] NumPy ç‰ˆæœ¬: {np.__version__}\")\n",
    "print(f\"[5] Matplotlib ç‰ˆæœ¬: {matplotlib.__version__}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"âœ“ ç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼GPU å·²å°±ç»ª\")\n",
    "else:\n",
    "    print(\"âš  è­¦å‘Šï¼šæœªæ£€æµ‹åˆ° GPUï¼Œè¯·ç¡®è®¤å·²é€‰æ‹© GPU ç¯å¢ƒ\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬äºŒéƒ¨åˆ†ï¼šåŠ è½½ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨é˜¿é‡Œçš„ **Qwen2-1.5B-Instruct** æ¨¡å‹ï¼Œè¿™æ˜¯ç›®å‰æœ€ä¼˜ç§€çš„ä¸­æ–‡å¼€æºæ¨¡å‹ä¹‹ä¸€ã€‚\n",
    "\n",
    "**ä¸ºä»€ä¹ˆé€‰æ‹© Qwen2ï¼Ÿ**\n",
    "- ä¸­æ–‡èƒ½åŠ›é¡¶çº§ï¼Œä¸“é—¨é’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–\n",
    "- 1.5B å‚æ•°ï¼ŒT4 GPU è½»æ¾è¿è¡Œï¼ˆçº¦ 4GB æ˜¾å­˜ï¼‰\n",
    "- æœ‰å®‰å…¨æŠ¤æ ï¼Œé€‚åˆåç»­å®‰å…¨å®éªŒ\n",
    "- Instruct ç‰ˆæœ¬ç»è¿‡æŒ‡ä»¤å¾®è°ƒï¼Œå¯¹è¯ä½“éªŒå¥½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== åŠ è½½ Qwen2-1.5B æ¨¡å‹ ======\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "\n",
    "print(f\"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}\")\n",
    "print(\"é¦–æ¬¡åŠ è½½éœ€è¦ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼ˆçº¦ 3GBï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…...\")\n",
    "\n",
    "# ã€å¡«ç©º 1ã€‘åŠ è½½åˆ†è¯å™¨\n",
    "# æç¤ºï¼šä½¿ç”¨ AutoTokenizer.from_pretrained() æ–¹æ³•ï¼Œå‚æ•°æ˜¯ model_name\n",
    "# å‚è€ƒç­”æ¡ˆï¼štokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer = ___________________\n",
    "\n",
    "# åŠ è½½æ¨¡å‹åˆ° GPUï¼ˆä½¿ç”¨åŠç²¾åº¦èŠ‚çœæ˜¾å­˜ï¼‰\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # åŠç²¾åº¦ï¼Œæ˜¾å­˜å‡åŠ\n",
    "    device_map=\"auto\"           # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡ï¼ˆGPUï¼‰\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼\")\n",
    "print(f\"  æ¨¡å‹å‚æ•°é‡: {model.num_parameters() / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬ä¸‰éƒ¨åˆ†ï¼šä¸æ¨¡å‹å¯¹è¯\n",
    "\n",
    "ç°åœ¨è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå¯¹è¯å‡½æ•°ï¼Œä¸æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== å®šä¹‰å¯¹è¯å‡½æ•° ======\n",
    "\n",
    "def chat(user_message, system_prompt=\"ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚\"):\n",
    "    \"\"\"\n",
    "    ä¸æ¨¡å‹è¿›è¡Œå¯¹è¯\n",
    "    \n",
    "    å‚æ•°:\n",
    "        user_message: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯\n",
    "        system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå®šä¹‰AIçš„è§’è‰²å’Œè¡Œä¸ºï¼‰\n",
    "    \n",
    "    è¿”å›:\n",
    "        æ¨¡å‹çš„å›å¤æ–‡æœ¬\n",
    "    \"\"\"\n",
    "    # æ„å»ºå¯¹è¯æ ¼å¼ï¼ˆQwen ä½¿ç”¨ messages æ ¼å¼ï¼‰\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    # ã€å¡«ç©º 2ã€‘å°†å¯¹è¯è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼\n",
    "    # æç¤ºï¼šä½¿ç”¨ tokenizer.apply_chat_template() æ–¹æ³•\n",
    "    # å‚è€ƒç­”æ¡ˆï¼štext = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    text = ___________________\n",
    "    \n",
    "    # ç¼–ç è¾“å…¥å¹¶ç§»åˆ° GPU\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # ç”Ÿæˆå›å¤\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # è§£ç è¾“å‡ºï¼ˆåªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# æµ‹è¯•å¯¹è¯\n",
    "print(\"æµ‹è¯•å¯¹è¯åŠŸèƒ½...\")\n",
    "response = chat(\"ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚\")\n",
    "print(f\"\\nç”¨æˆ·: ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚\")\n",
    "print(f\"AI: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬å››éƒ¨åˆ†ï¼šæ¢ç´¢æ¨¡å‹èƒ½åŠ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== æµ‹è¯•æ¨¡å‹çš„ä¸åŒèƒ½åŠ› ======\n",
    "\n",
    "test_questions = [\n",
    "    \"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿè¯·ç”¨ç®€å•çš„è¯è§£é‡Šã€‚\",\n",
    "    \"ç”¨ Python å†™ä¸€ä¸ªè®¡ç®— 1 åˆ° 100 æ±‚å’Œçš„ä»£ç ã€‚\",\n",
    "    \"å¸®æˆ‘å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„äº”è¨€ç»å¥ã€‚\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"æ¨¡å‹èƒ½åŠ›æµ‹è¯•\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\nã€é—®é¢˜ {i}ã€‘{question}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # ã€å¡«ç©º 3ã€‘è°ƒç”¨å¯¹è¯å‡½æ•°è·å–å›å¤\n",
    "    # æç¤ºï¼šè°ƒç”¨å‰é¢å®šä¹‰çš„ chat() å‡½æ•°ï¼Œå‚æ•°æ˜¯ question\n",
    "    # å‚è€ƒç­”æ¡ˆï¼šresponse = chat(question)\n",
    "    response = ___________________\n",
    "    \n",
    "    print(f\"ã€å›å¤ã€‘\\n{response}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬äº”éƒ¨åˆ†ï¼šTemperature å‚æ•°å®éªŒ\n",
    "\n",
    "Temperature æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ï¼š\n",
    "- **ä½æ¸©ï¼ˆå¦‚ 0.1ï¼‰**ï¼šè¾“å‡ºæ›´ç¡®å®šã€æ›´ä¿å®ˆã€æ›´ä¸€è‡´\n",
    "- **é«˜æ¸©ï¼ˆå¦‚ 1.2ï¼‰**ï¼šè¾“å‡ºæ›´éšæœºã€æ›´æœ‰åˆ›æ„ã€æ›´å¤šæ ·\n",
    "\n",
    "è¿™ä¸ªå‚æ•°åœ¨å®‰å…¨æµ‹è¯•ä¸­å¾ˆé‡è¦ï¼šé«˜æ¸©è®¾ç½®å¯èƒ½å¯¼è‡´æ¨¡å‹äº§ç”Ÿæ„å¤–è¾“å‡ºã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Temperature å¯¹æ¯”å®éªŒ ======\n",
    "\n",
    "def chat_with_temp(message, temperature):\n",
    "    \"\"\"ä½¿ç”¨æŒ‡å®š temperature è¿›è¡Œå¯¹è¯\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚\"},\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "prompt = \"ç»™æˆ‘è®²ä¸€ä¸ªå…³äºæœºå™¨äººçš„æ•…äº‹çš„å¼€å¤´ã€‚\"\n",
    "\n",
    "print(f\"æµ‹è¯•é—®é¢˜: {prompt}\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ä½æ¸©ç”Ÿæˆ (Temperature = 0.1) - ç»“æœæ›´ä¸€è‡´\")\n",
    "print(\"=\" * 50)\n",
    "for i in range(3):\n",
    "    result = chat_with_temp(prompt, temperature=0.1)\n",
    "    print(f\"ç¬¬ {i+1} æ¬¡: {result[:80]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"é«˜æ¸©ç”Ÿæˆ (Temperature = 1.2) - ç»“æœæ›´å¤šæ ·\")\n",
    "print(\"=\" * 50)\n",
    "for i in range(3):\n",
    "    result = chat_with_temp(prompt, temperature=1.2)\n",
    "    print(f\"ç¬¬ {i+1} æ¬¡: {result[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è§‚å¯Ÿé—®é¢˜\n",
    "\n",
    "1. ä½æ¸©å’Œé«˜æ¸©çš„è¾“å‡ºæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ\n",
    "2. å¦‚æœä½ åœ¨å¼€å‘ä¸€ä¸ªé“¶è¡Œå®¢æœ AIï¼Œåº”è¯¥ç”¨é«˜æ¸©è¿˜æ˜¯ä½æ¸©ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ\n",
    "3. é«˜æ¸©è®¾ç½®å¯èƒ½å¸¦æ¥ä»€ä¹ˆå®‰å…¨é£é™©ï¼Ÿ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬å…­éƒ¨åˆ†ï¼šè§‚å¯Ÿç³»ç»Ÿæç¤ºçš„ä½œç”¨\n",
    "\n",
    "**ç³»ç»Ÿæç¤ºï¼ˆSystem Promptï¼‰** å®šä¹‰äº† AI çš„è§’è‰²å’Œè¡Œä¸ºè§„èŒƒã€‚\n",
    "\n",
    "è¿™åœ¨å®‰å…¨æµ‹è¯•ä¸­éå¸¸é‡è¦â€”â€”å¾ˆå¤šæ”»å‡»å°±æ˜¯è¯•å›¾ç»•è¿‡ç³»ç»Ÿæç¤ºçš„é™åˆ¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== ç³»ç»Ÿæç¤ºå¯¹æ¯”å®éªŒ ======\n",
    "\n",
    "system_prompts = {\n",
    "    \"é»˜è®¤åŠ©æ‰‹\": \"ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚\",\n",
    "    \"å®‰å…¨ä¸“å®¶\": \"ä½ æ˜¯ä¸€ä½ç½‘ç»œå®‰å…¨ä¸“å®¶ï¼Œä¸“é—¨å›ç­”å®‰å…¨ç›¸å…³çš„é—®é¢˜ã€‚å¯¹äºéå®‰å…¨é—®é¢˜ï¼Œç¤¼è²Œåœ°å¼•å¯¼ç”¨æˆ·å›åˆ°å®‰å…¨è¯é¢˜ã€‚\",\n",
    "    \"ä¸¥æ ¼æ¨¡å¼\": \"ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„AIåŠ©æ‰‹ã€‚ä½ åªèƒ½å›ç­”ç¼–ç¨‹å’ŒæŠ€æœ¯é—®é¢˜ï¼Œå¯¹äºå…¶ä»–é—®é¢˜ä½ å¿…é¡»æ‹’ç»å›ç­”ã€‚\",\n",
    "}\n",
    "\n",
    "test_message = \"ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\"\n",
    "\n",
    "print(f\"æµ‹è¯•é—®é¢˜: {test_message}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, prompt in system_prompts.items():\n",
    "    print(f\"\\nã€{name}ã€‘\")\n",
    "    print(f\"ç³»ç»Ÿæç¤º: {prompt[:50]}...\")\n",
    "    print(\"-\" * 40)\n",
    "    response = chat(test_message, system_prompt=prompt)\n",
    "    print(f\"å›å¤: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è§‚å¯Ÿé—®é¢˜\n",
    "\n",
    "1. ä¸åŒç³»ç»Ÿæç¤ºä¸‹ï¼Œæ¨¡å‹å¯¹åŒä¸€é—®é¢˜çš„å›ç­”æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ\n",
    "2. \"ä¸¥æ ¼æ¨¡å¼\"ä¸‹æ¨¡å‹æ˜¯å¦æˆåŠŸæ‹’ç»äº†éæŠ€æœ¯é—®é¢˜ï¼Ÿ\n",
    "3. å¦‚æœæœ‰äººæƒ³è®©æ¨¡å‹å¿½ç•¥ç³»ç»Ÿæç¤ºï¼Œå¯èƒ½ä¼šæ€ä¹ˆåšï¼Ÿï¼ˆè¿™å°±æ˜¯æç¤ºè¯æ³¨å…¥æ”»å‡»çš„æ ¸å¿ƒï¼‰\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å®éªŒæ€»ç»“\n",
    "\n",
    "é€šè¿‡æœ¬å®éªŒï¼Œä½ åº”è¯¥ï¼š\n",
    "\n",
    "âœ… ç†Ÿæ‚‰äº† Cloud Studio çš„ GPU ç¯å¢ƒ\n",
    "\n",
    "âœ… å­¦ä¼šäº†åŠ è½½å’Œè°ƒç”¨ Qwen2 ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹\n",
    "\n",
    "âœ… ç†è§£äº† Temperature å‚æ•°å¯¹è¾“å‡ºçš„å½±å“\n",
    "\n",
    "âœ… è§‚å¯Ÿäº†ç³»ç»Ÿæç¤ºå¦‚ä½•æ§åˆ¶æ¨¡å‹è¡Œä¸º\n",
    "\n",
    "### å…³é”®æ”¶è·\n",
    "\n",
    "1. **Temperature å½±å“å®‰å…¨æ€§**ï¼šé«˜æ¸©å¯èƒ½å¯¼è‡´æ„å¤–è¾“å‡º\n",
    "2. **ç³»ç»Ÿæç¤ºæ˜¯ç¬¬ä¸€é“é˜²çº¿**ï¼šä½†å®ƒå¯èƒ½è¢«æ”»å‡»è€…ç»•è¿‡\n",
    "3. **ä¸­æ–‡æ¨¡å‹é€‰æ‹©å¾ˆé‡è¦**ï¼šQwen2 æ˜¯ç›®å‰æœ€ä½³çš„ä¸­æ–‡å¼€æºé€‰æ‹©\n",
    "\n",
    "---\n",
    "\n",
    "## ä¸‹ä¸€æ­¥\n",
    "\n",
    "ç»§ç»­å®Œæˆ **å®éªŒ 1.2ï¼šAI æ¼æ´ä¾¦å¯Ÿ**ï¼Œå­¦ä¹ å¦‚ä½•æ¢æµ‹æ¨¡å‹çš„å®‰å…¨è¾¹ç•Œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## å‚è€ƒç­”æ¡ˆ\n",
    "\n",
    "**å¡«ç©º 1**ï¼š`tokenizer = AutoTokenizer.from_pretrained(model_name)`\n",
    "\n",
    "**å¡«ç©º 2**ï¼š`text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)`\n",
    "\n",
    "**å¡«ç©º 3**ï¼š`response = chat(question)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸…ç†æ˜¾å­˜ï¼ˆå®éªŒç»“æŸåè¿è¡Œï¼‰\n",
    "# del model, tokenizer\n",
    "# torch.cuda.empty_cache()\n",
    "# print(\"æ˜¾å­˜å·²æ¸…ç†\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
