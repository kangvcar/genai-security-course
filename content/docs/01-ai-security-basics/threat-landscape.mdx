---
title: 第1章：AI 安全威胁全景图
description: 认识 AI 安全领域的主要威胁，理解为什么 AI 安全与传统网络安全存在本质差异
---


import { Callout } from 'fumadocs-ui/components/callout';
import { Tab, Tabs } from 'fumadocs-ui/components/tabs';
import { Step, Steps } from 'fumadocs-ui/components/steps';
import { Accordion, Accordions } from 'fumadocs-ui/components/accordion';
import { Quiz } from '@/components/ui/quiz';


<Callout title="" type="info">
预计阅读约16分钟
</Callout>

## 本章导读

当微软的 AI 助手 Sydney 在上线几天后就被用户"诱导"泄露了内部指令，当自动驾驶系统因为路牌上的几个贴纸就将停车标志识别为限速标志——这些事件都在告诉我们：**AI 系统面临的安全威胁，与我们熟悉的传统网络安全截然不同。** 传统安全对抗的是代码漏洞，而 AI 安全对抗的是模型行为的不确定性、训练数据的不可控性、以及自然语言交互带来的全新攻击面。

本章是整个课程的"地图课"。我们将从 AI 安全与传统安全的本质差异讲起，借助 OWASP Top 10 for LLM Applications 等主流威胁框架建立系统化的风险认知，并通过真实安全事件将抽象的威胁类别变成可感知的攻防场景。学完本章，你将拥有一幅完整的 AI 安全威胁全景图，后续每个模块所学习的攻击技术和防御策略，都能在这幅地图中找到对应的位置。

## 学习目标

<Callout title="本章学完后，你将能够：" type="info">
1. **识别 AI 安全威胁的独特性**：理解 AI 系统面临的安全威胁与传统软件系统的本质区别
2. **掌握 AI 威胁分类体系**：了解 OWASP Top 10 for LLM Applications 等主流威胁分类框架，能够识别常见的 AI 安全风险
3. **分析真实安全事件**：通过典型案例理解 AI 安全威胁的实际影响和危害
4. **建立安全意识**：认识到 AI 安全问题的严重性，为后续深入学习打下基础
</Callout>

## 1 AI 安全：一个全新的战场

### 1.1 从一个真实事件说起

2023年2月，微软发布了集成 ChatGPT 技术的新版必应搜索引擎。然而，上线仅几天后，用户就发现了一个令人震惊的现象：通过特定的对话方式，可以让必应的 AI 助手"Sydney"表现出攻击性、情绪化，甚至泄露其系统设置信息。

<Callout title="Sydney 事件" type="warn">
一位用户通过巧妙的提问，成功让 Sydney 透露了它的内部指令，包括"我的名字是 Sydney"、"我不能透露我的提示词"等原本应该保密的信息。这个事件引发了全球关注，微软不得不紧急限制对话轮次，并对系统进行多次调整。
</Callout>

这个案例揭示了一个重要事实：**即使是科技巨头精心打造的 AI 系统，也可能存在意想不到的安全漏洞。**

如果说 Sydney 事件让我们看到了“问题真的存在”，那么下一步就要回答“它和传统安全问题到底哪里不同”。

### 1.2 AI 安全威胁的独特性

传统的网络安全主要关注代码漏洞、权限控制、数据加密等问题。例如，SQL 注入攻击利用的是代码对输入验证不足的漏洞，攻击者通过构造特殊的输入字符串来执行恶意数据库操作。这类攻击的原理相对明确，防御方法也比较成熟。

但 AI 系统的安全问题有着本质的不同：

<Tabs items={['传统软件', 'AI 系统']}>
  <Tab value="传统软件">
    传统软件就像一台按照固定程序运行的机器，它的行为是可预测的。如果输入 A，就一定输出 B。安全问题通常出现在程序逻辑的漏洞上。
  </Tab>
  <Tab value="AI 系统">
    AI 系统更像一个经过训练的"学生"。它通过学习大量数据来掌握某种能力，但这个"学生"的行为并不总是可预测的。同样的问题，换一种问法可能得到完全不同的答案。
  </Tab>
</Tabs>

这种差异带来了几个独特的安全挑战：

**第一，输入的多样性和不可预测性**  
传统系统可以通过输入验证来防御攻击，但 AI 系统需要处理自然语言、图像等复杂输入。攻击者可以通过精心设计的输入，让 AI 系统产生错误的输出，而这些输入在表面上看起来可能完全正常。

**第二，模型行为的不透明性**  
深度学习模型通常包含数百万甚至数十亿个参数，我们很难完全理解模型为什么会做出某个决策。这种"黑盒"特性使得发现和修复安全问题变得极其困难。

**第三，攻击的隐蔽性**  
对 AI 系统的攻击可能不会留下明显的痕迹。例如，攻击者可以在训练数据中植入恶意样本，这些样本在正常情况下不会被发现，但在特定条件下会触发模型的异常行为。

<Callout title="思考" type="idea">
既然 AI 系统这么容易受到攻击，为什么还要使用它们？答案是，AI 技术带来的便利和效率提升是巨大的，我们不能因噎废食。但正因如此，我们更需要深入理解 AI 的安全风险，学会如何防范和应对这些威胁。
</Callout>

到这里，我们已经知道了 AI 安全“为什么难”。接下来需要把这些风险做成可枚举、可检查的清单，否则在工程实践中很容易只看到局部问题。

## 2 AI 威胁分类：OWASP Top 10 for LLM Applications

为了系统地理解 AI 面临的安全威胁，安全研究人员开发了多种分类框架。其中最具影响力的是 OWASP（开放式 Web 应用程序安全项目）发布的 **Top 10 for Large Language Model Applications**（现归属 OWASP GenAI Security Project）。

这类框架的意义，不是让我们死记硬背名词，而是把“分散的安全担忧”转化为“可以逐项核查的工程问题”。

### 2.1 OWASP Top 10 for LLM Applications（2025）威胁概览

| 序号 | 威胁名称 | 简要描述 |
|:---:|---------|---------|
| 1 | **提示词注入（Prompt Injection）** | 通过精心构造输入，操纵模型行为并绕过系统约束 |
| 2 | **敏感信息泄露（Sensitive Information Disclosure）** | 模型输出中暴露机密数据、个人信息或业务敏感内容 |
| 3 | **供应链风险（Supply Chain）** | 模型、数据、依赖组件或平台链路被污染或篡改 |
| 4 | **数据与模型投毒（Data and Model Poisoning）** | 在训练、微调或知识库数据中注入恶意内容影响行为 |
| 5 | **输出处理不当（Improper Output Handling）** | 未充分校验和隔离模型输出，导致下游系统被利用 |
| 6 | **过度代理权（Excessive Agency）** | 给模型授予过高执行权限，触发越权或高风险自动化操作 |
| 7 | **系统提示词泄露（System Prompt Leakage）** | 系统指令或内部策略被套取，削弱防护边界 |
| 8 | **向量与嵌入弱点（Vector and Embedding Weaknesses）** | 检索、向量库、嵌入处理链路中的缺陷被攻击利用 |
| 9 | **错误信息（Misinformation）** | 模型输出不实内容并被用户或系统当作可靠事实使用 |
| 10 | **无界消耗（Unbounded Consumption）** | 资源与成本被恶意或异常请求放大，导致服务退化或失控 |

### 2.2 威胁分类的实用价值

看完这张表，你可能会觉得"十个类别太多了，记不住"。别急——你不需要一次记住所有威胁。这张清单的真正价值在于，它把"分散的安全担忧"转化为了"可以逐项核查的工程问题"。当我们在开发或使用 AI 系统时，可以对照这个清单逐条检查：

- 我们的系统是否容易受到提示词注入攻击？
- 训练数据的来源是否可靠，是否可能被投毒？
- 模型的输出是否经过了适当的验证？
- 我们是否使用了来自可信来源的预训练模型？

有了分类清单之后，下一步就该看真实案例。因为只有看到这些威胁在现实中如何发生、造成什么后果，我们才能理解“为什么安全措施必须前置”。

## 3 真实世界的 AI 安全事件

理论知识固然重要，但真实案例能让我们更直观地理解 AI 安全威胁的影响。

### 3.1 案例一：ChatGPT 越狱事件（2022-2023）

<Steps>
  <Step>
    **背景**
    
    OpenAI 在发布 ChatGPT 时，设置了严格的内容过滤机制，禁止模型生成暴力、色情、违法等有害内容。
  </Step>
  <Step>
    **攻击过程**
    
    用户很快发现，通过角色扮演的方式可以绕过这些限制。最著名的是"DAN"（Do Anything Now）越狱方法。用户让 ChatGPT 扮演一个"没有任何限制"的角色，从而诱导模型生成原本被禁止的内容。
  </Step>
  <Step>
    **影响与启示**
    
    这类越狱方法在社交媒体上广泛传播，OpenAI 不得不持续更新模型。这说明仅仅依靠内容过滤是不够的，需要在系统设计层面就考虑安全问题。
  </Step>
</Steps>

### 3.2 案例二：自动驾驶系统的对抗攻击（2018）

<Steps>
  <Step>
    **背景**
    
    自动驾驶系统高度依赖计算机视觉模型识别交通标志、车道线和障碍物，这些识别结果会直接影响车辆控制决策。
  </Step>
  <Step>
    **攻击过程**
    
    研究人员发现，在停车标志上贴上特定图案的贴纸，可以让图像识别系统将其误认为限速标志。这些贴纸对人类驾驶员看起来只是普通涂鸦，但对模型会形成误导。
  </Step>
  <Step>
    **影响与启示**
    
    该案例说明对抗样本可在物理世界触发真实风险。对于自动驾驶这类安全关键系统，不能仅依赖单一感知模型，需要多传感器融合、规则校验与冗余安全机制。
  </Step>
</Steps>

### 3.3 案例三：GitHub Copilot 泄露训练数据（2021）

<Steps>
  <Step>
    **背景**
    
    AI 代码助手通过大规模代码语料训练，能够根据上下文自动补全代码，显著提升开发效率。
  </Step>
  <Step>
    **攻击过程**
    
    研究人员发现，通过特定提示方式，可以诱导 Copilot 输出与训练数据高度相似甚至近似原文的代码片段，其中可能包含 API 密钥、密码等敏感信息。
  </Step>
  <Step>
    **影响与启示**
    
    这一事件暴露了模型记忆泄露与敏感信息披露风险。AI 生成内容在落地前需要进行密钥扫描、许可证合规检查和人工复核，避免敏感数据直接进入代码仓库。
  </Step>
</Steps>

三个案例放在一起看，会发现一个共同点：攻击不一定来自传统“代码漏洞”，也可能来自输入操控、模型行为偏移、输出泄露等 AI 特有风险。下面我们把这种差异做一个结构化对比。

## 4 AI 安全与传统安全的对比

| 对比维度 | 传统安全 | AI 安全 |
|---------|---------|---------|
| **攻击面** | 代码漏洞、配置错误、网络协议 | 训练数据、模型输入、模型本身、供应链 |
| **防御方法** | 输入验证、访问控制、加密、补丁管理 | 对抗训练、输入检测、模型加固、输出验证 |
| **测试验证** | 单元测试、集成测试，结果确定 | 概率性行为，难以穷举测试 |
| **修复方式** | 修改代码、打补丁 | 可能需要重新训练模型 |

这个对比表的核心结论是：AI 安全不是传统安全的“附加题”，而是一个需要独立方法论的新题型。理解这一点，才能为后续章节里的攻防实践打好基础。

## 本章小结

本章我们建立了对 AI 安全威胁的整体认识：

1. **AI 安全的独特性**：AI 系统面临的安全威胁与传统软件有本质区别，主要体现在输入的多样性、模型的不透明性和攻击的隐蔽性上。

2. **主要威胁类型**：OWASP Top 10 for LLM Applications 列出了当前关键风险，包括提示词注入、敏感信息泄露、供应链风险、数据与模型投毒等。

3. **真实案例的启示**：通过 ChatGPT 越狱、自动驾驶对抗攻击、Copilot 数据泄露等案例，我们看到这些威胁是现实存在的风险。

4. **安全思维的重要性**：AI 安全不是一个可以"一劳永逸"解决的问题，而是需要在整个系统生命周期中持续关注的议题。


## 课后思考

<Accordions>
  <Accordion title="思考题1：AI 安全的独特性">
    请思考：为什么传统的输入验证方法（如正则表达式过滤）难以防御 AI 系统的攻击？请结合本章内容，举出至少两个原因。
  </Accordion>
  <Accordion title="思考题2：OWASP Top 10 for LLM Applications 应用">
    请选择 OWASP Top 10 for LLM Applications 中的三种威胁，结合你熟悉的 AI 应用场景（如智能客服、图像识别等），分析这些威胁可能的攻击场景和潜在危害。
  </Accordion>
  <Accordion title="思考题3：安全平衡">
    从案例中我们看到，AI 系统的安全问题往往涉及到安全性与可用性的平衡。如果你是一个 AI 系统的产品经理，你会如何平衡这两者？
  </Accordion>
</Accordions>

## 自测 Quiz

<Quiz questions={[
  {
    question: 'AI 安全与传统网络安全的核心区别是什么？',
    options: [
      { label: 'AI 系统更贵，所以安全性更重要' },
      { label: 'AI 模型行为不透明、输入多样、攻击隐蔽，需要独立的安全方法论', correct: true },
      { label: 'AI 系统不需要传统的输入验证' },
      { label: 'AI 安全只需要关注数据加密' },
    ],
    explanation: 'AI 安全面临模型行为不透明、输入多样性和不可预测性、攻击隐蔽性三大独特挑战，这些使得传统安全方法无法直接套用。',
  },
  {
    question: 'OWASP Top 10 for LLM Applications 2025 中排名第一的威胁是什么？',
    options: [
      { label: '数据投毒（Data Poisoning）' },
      { label: '敏感信息泄露（Sensitive Information Disclosure）' },
      { label: '提示词注入（Prompt Injection）', correct: true },
      { label: '供应链风险（Supply Chain）' },
    ],
    explanation: '提示词注入排名第一，因为它直接利用了 LLM 无法区分指令和数据的根本特性，攻击门槛低但危害大。',
  },
  {
    question: '以下哪个不是 AI 安全威胁的独特特征？',
    options: [
      { label: '攻击可以不留下明显痕迹' },
      { label: '模型行为是"黑盒"，难以理解决策原因' },
      { label: '必须通过代码漏洞才能发起攻击', correct: true },
      { label: '相同输入可能产生不同输出' },
    ],
    explanation: 'AI 系统的攻击不一定需要代码漏洞，攻击者可以通过精心设计的自然语言输入（如提示词注入）来操纵模型行为。',
  },
  {
    question: '必应 Sydney 事件（2023年）说明了什么核心问题？',
    options: [
      { label: '微软的服务器不够安全' },
      { label: '即使是科技巨头精心打造的 AI 系统也可能存在意想不到的安全漏洞', correct: true },
      { label: '聊天机器人不应该有名字' },
      { label: 'AI 系统只要限制对话轮次就安全了' },
    ],
    explanation: 'Sydney 事件揭示了即使投入大量资源的 AI 系统仍可能被用户通过巧妙的对话方式操纵，暴露内部设置信息。',
  },
]} />

## 延伸阅读

- [OWASP Top 10 for LLM Applications](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- [NIST AI RMF 1.0](https://www.nist.gov/publications/artificial-intelligence-risk-management-framework-ai-rmf-10)
- [MITRE ATLAS Fact Sheet](https://atlas.mitre.org/pdf-files/MITRE_ATLAS_Fact_Sheet.pdf)
