---
title: 第4章：AI 安全测试环境搭建
description: 配置完整的 AI 安全测试环境，包括云平台使用、Python 配置和核心依赖库安装
---


import { Callout } from 'fumadocs-ui/components/callout';
import { Tab, Tabs } from 'fumadocs-ui/components/tabs';
import { Step, Steps } from 'fumadocs-ui/components/steps';
import { File, Folder, Files } from 'fumadocs-ui/components/files';
import { Accordion, Accordions } from 'fumadocs-ui/components/accordion';
import { Quiz } from '@/components/ui/quiz';


<Callout title="" type="info">
预计阅读约16分钟
</Callout>

## 本章导读

前三章为你搭建了 AI 安全的认知框架——威胁全景、模型原理和红队思维。但安全研究不能停留在纸面上，从本章开始我们要真正"动手"了。而动手的第一步，不是写攻击代码，而是搭建一个**稳定、可复现、可隔离**的实验环境。如果环境不可靠，后续所有实验结论都将失去意义。

本章将引导你在云平台上完成完整的 AI 安全测试环境配置：从选择云平台和创建 GPU 实例，到安装 Python 依赖和加载第一个大语言模型，再到验证整条链路的联通性。默认基线与实验 1.1 对齐（腾讯 Cloud Studio + T4 GPU + Qwen2-1.5B-Instruct），确保你在阅读和动手之间无缝衔接。完成本章后，你将拥有一个可长期复用的实验基地，为模块二到模块五的所有实验做好准备。

## 学习目标

<Callout title="本章学完后，你将能够：" type="info">
1. **掌握云平台的使用**：熟练使用云端开发环境创建和管理 GPU 开发环境
2. **配置 Python 环境**：正确安装和配置 Python 及相关依赖库
3. **调用大语言模型（LLM）**：能够使用 Transformers 库加载和运行预训练模型
4. **排查常见问题**：识别和解决环境搭建过程中的典型问题
</Callout>

## 1 环境搭建的三个原则

在开始动手前，先统一三条原则。后面所有操作都围绕它们展开：

| 原则 | 为什么重要 | 具体做法 |
|-----|------------|---------|
| **可复现** | 团队成员应能得到一致结果 | 固定依赖版本、保留 `requirements.txt` |
| **可隔离** | 避免一个项目影响另一个项目 | 每个实验使用独立虚拟环境 |
| **可验证** | 避免“看起来能跑、实际上不稳定” | 每次配置后做最小验证脚本 |

有了这三个原则，我们再来决定环境方案，就不会只看“能不能跑”，而是看“能不能稳定地跑”。

## 2 为什么首选云平台

### 2.1 本地环境 vs 云平台

<Tabs items={['本地环境的挑战', '云平台的优势']}>
  <Tab value="本地环境的挑战">
    - **硬件门槛高**：运行大语言模型通常需要较强 GPU，成本高
    - **底层依赖复杂**：CUDA、cuDNN、驱动版本容易互相制约
    - **环境冲突常见**：不同项目依赖版本容易相互影响
    - **资源利用率低**：GPU 大多数时间闲置
  </Tab>
  <Tab value="云平台的优势">
    - **GPU 可按需使用**：对学习阶段更友好
    - **启动速度快**：多数平台提供预配置镜像
    - **访问门槛低**：浏览器即可使用
    - **更利于协作**：分享 Notebook 和配置更方便
  </Tab>
</Tabs>

<Callout title="类比理解" type="idea">
使用云平台就像租用健身房，而不是在家里建一个健身房。你只需要在需要锻炼时去健身房，使用专业的器材，用完就走。
</Callout>

这并不意味着本地部署不重要，而是对于课程阶段，云平台更容易把注意力放在"安全能力提升"而非"底层环境排障"上。选定了平台之后，下面按五步流程完成完整的环境配置。

## 3 环境配置总流程

<Steps>
  <Step>
    **创建工作空间**

    - 访问云端开发环境（本课程默认使用腾讯 Cloud Studio）
    - 选择 **Python 3** 或 **Data Science** 模板
    - 启用 **GPU** 支持并命名工作空间（例如 `ai-security-lab`）
    - 课程建议配置：**NVIDIA Tesla T4（16GB 显存）**
  </Step>
  <Step>
    **创建隔离环境**

    使用虚拟环境隔离依赖，避免后续实验互相污染。
  </Step>
  <Step>
    **安装固定依赖**

    固定关键库版本，确保你和同学、以及未来的自己都能复现结果。
  </Step>
  <Step>
    **运行最小验证脚本**

    验证 Python 版本、库版本和 CUDA 可用性，确认环境真正可用。
  </Step>
  <Step>
    **执行首个模型调用**

    使用课程实验同款模型 `Qwen/Qwen2-1.5B-Instruct` 验证“推理链路”完整可用，作为后续安全实验的起点。
  </Step>
</Steps>

## 4 Python 环境配置

### 4.1 检查 Python 版本

```bash title="检查版本"
python --version
# 或
python3 --version
```

建议使用 Python 3.8 或更高版本。

### 4.2 创建并激活虚拟环境

```bash title="创建并激活虚拟环境"
# 创建虚拟环境
python3 -m venv ai_security_env

# 激活虚拟环境
source ai_security_env/bin/activate  # Linux/Mac
# 或
ai_security_env\Scripts\activate     # Windows
```

<Callout title="为什么一定要用虚拟环境？" type="info">
虚拟环境的本质是“依赖隔离”。它能避免你在一个实验里升级了库版本，导致另一个实验突然失效。
</Callout>

## 5 安装核心依赖库

```bash title="安装必要的 Python 库"
# 升级 pip
pip install --upgrade pip

# 安装核心库（固定版本，保证可复现）
pip install transformers==4.40.0 torch==2.1.0 numpy==1.24.3 matplotlib==3.7.1 accelerate==0.27.0
```

| 库名 | 版本 | 作用 |
|-----|------|------|
| **transformers** | 4.40.0 | Hugging Face 预训练模型库，负责模型与分词器加载 |
| **torch** | 2.1.0 | 深度学习框架，负责底层张量计算与推理 |
| **numpy** | 1.24.3 | 数值计算库，处理数组和矩阵运算 |
| **matplotlib** | 3.7.1 | 可视化库，用于展示实验结果 |
| **accelerate** | 0.27.0 | 设备调度与加速工具，优化推理执行 |

固定版本不是“保守做法”，而是安全实验的基础。否则同一测试在不同机器上可能得到不一致结果。

## 6 验证安装是否成功

```python title="验证环境配置"
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")

import transformers
print(f"Transformers version: {transformers.__version__}")

import numpy as np
print(f"NumPy version: {np.__version__}")
```

<Callout title="如何解释验证结果" type="success">
- `CUDA available: True`：说明当前环境可使用 GPU。
- `CUDA available: False`：不一定是错误，可能是 CPU 实例；若课程实验要求 GPU，请切换实例再测。
- 库版本与预期一致：说明环境已具备可复现实验基础。
</Callout>

## 7 加载第一个模型（最小链路验证）

```python title="加载 Qwen2-1.5B-Instruct 并完成一次对话"
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_name = "Qwen/Qwen2-1.5B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    dtype=torch.float16,
    device_map="auto"
)

messages = [
    {"role": "system", "content": "你是一个有帮助的AI助手。"},
    {"role": "user", "content": "请用一句话介绍 AI 安全。"}
]

text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = tokenizer([text], return_tensors="pt").to(model.device)

outputs = model.generate(
    **inputs,
    max_new_tokens=64,
    temperature=0.7,
    do_sample=True
)

generated_ids = outputs[0][inputs.input_ids.shape[-1]:]
print(tokenizer.decode(generated_ids, skip_special_tokens=True))
```

这一步的重点不是“生成质量”，而是确认三件事：模型能加载、推理能执行、输出能稳定返回。

## 8 常见问题排查

<Tabs items={['CUDA 不可用', '内存不足', '包安装失败']}>
  <Tab value="CUDA 不可用">
    **问题**：`torch.cuda.is_available()` 返回 `False`

    **排查顺序**：
    - 确认实例类型是否为 GPU
    - 重启工作空间并重新连接
    - 检查平台侧 GPU 配额或会话是否过期
  </Tab>
  <Tab value="内存不足">
    **问题**：加载模型时报 `OutOfMemoryError`

    **解决方案**：
    - 先换小模型（如 `Qwen/Qwen2-0.5B-Instruct` 而不是 `Qwen/Qwen2-1.5B-Instruct`）
    - 尝试 `model.half()` 降低显存占用
    - 减小批量大小（batch size）
  </Tab>
  <Tab value="包安装失败">
    **问题**：`pip install` 失败

    **解决方案**：
    - 升级 pip：`pip install --upgrade pip`
    - 检查网络与 DNS
    - 使用镜像源：`pip install -i https://pypi.tuna.tsinghua.edu.cn/simple 包名`
  </Tab>
</Tabs>

## 9 实验习惯最佳实践

<Callout title="良好习惯" type="success">
1. **先记录再修改**：每次变更依赖前先保存当前环境清单
2. **每次实验可复现**：保留输入、参数、输出和版本信息
3. **资源有预算意识**：持续关注 GPU 占用与运行时长
4. **代码与结论一起保存**：Notebook 中同时记录实验代码与观察结论
5. **结束后清理资源**：释放模型和会话，避免不必要成本
</Callout>

## 10 推荐的项目结构

为了让实验过程更清晰、协作更顺畅，建议采用如下结构：

<Files>
  <Folder name="ai-security-lab" defaultOpen>
    <Folder name="notebooks" defaultOpen>
      <File name="01_environment_test.ipynb" />
      <File name="02_vulnerability_scan.ipynb" />
    </Folder>
    <Folder name="scripts">
      <File name="test_model.py" />
      <File name="utils.py" />
    </Folder>
    <Folder name="data">
      <File name="test_inputs.json" />
    </Folder>
    <File name="requirements.txt" />
    <File name="README.md" />
  </Folder>
</Files>

## 配套实验

<Callout title="动手实践" type="success">
完成本章学习后，请进行 **实验 1.1：环境配置**，在实际操作中验证你的环境是否真正达到“可复现、可隔离、可验证”三个目标。
</Callout>

## 常见问题

<Accordions>
  <Accordion title="为什么推荐使用云平台而不是本地环境？">
    在课程阶段，云平台能显著降低硬件和配置门槛，让你把主要精力放在安全测试方法上。等你形成稳定方法后，再迁移到本地或企业环境会更高效。
  </Accordion>
  <Accordion title="哪些云平台可以使用？">
    可选平台包括腾讯 Cloud Studio、Google Colab、Kaggle Notebooks、阿里云 DSW 等。为与实验 1.1 保持一致，课程默认使用腾讯 Cloud Studio（T4 GPU）。
  </Accordion>
  <Accordion title="如果我想在本地运行怎么办？">
    若你有支持 CUDA 的 NVIDIA GPU（如 RTX 2060 或更高），可以本地运行。建议先在云端跑通流程，再迁移到本地，减少排障成本。
  </Accordion>
</Accordions>

## 本章小结

1. **本章核心不是“安装工具”**：而是建立可复现、可隔离、可验证的实验基线
2. **云平台优先是阶段性策略**：先降低环境门槛，再聚焦安全能力提升
3. **配置流程要形成闭环**：创建环境 -> 安装依赖 -> 验证结果 -> 最小模型测试
4. **排障要有顺序**：先查实例与资源，再查依赖与代码
5. **良好习惯决定长期效率**：规范记录、版本管理和资源清理缺一不可


## 课后思考

<Accordions>
  <Accordion title="思考题1：本地环境 vs 云平台">
    如果你需要长期维护一个企业内部的 AI 安全测试环境，你会选择本地部署还是云平台？请结合成本、合规、可维护性进行分析。
  </Accordion>
  <Accordion title="思考题2：依赖与可复现性">
    本章要求固定多个依赖版本。你会如何记录并保证团队成员的环境一致性？请写出至少两种做法。
  </Accordion>
</Accordions>


## 自测 Quiz

<Quiz questions={[
  {
    question: '本课程推荐的实验平台和 GPU 配置是什么？',
    options: [
      { label: 'Google Colab + NVIDIA A100' },
      { label: '腾讯 Cloud Studio + NVIDIA Tesla T4', correct: true },
      { label: '本地电脑 + CPU 运行' },
      { label: 'AWS SageMaker + NVIDIA V100' },
    ],
    explanation: '课程推荐腾讯 Cloud Studio 作为实验平台，配备 NVIDIA Tesla T4 GPU（16GB 显存），优势是国内访问快、模型下载稳定、免费使用。',
  },
  {
    question: '环境搭建的三项核心原则是什么？',
    options: [
      { label: '快速、简单、免费' },
      { label: '可复现、可隔离、可验证', correct: true },
      { label: '高性能、大内存、多 GPU' },
      { label: '开源、跨平台、云原生' },
    ],
    explanation: '可复现保证实验结果一致、可隔离避免依赖冲突、可验证确认环境正确。这三项原则是建立可靠实验基线的基础。',
  },
  {
    question: '为什么实验中需要固定依赖版本？',
    options: [
      { label: '固定版本的软件运行更快' },
      { label: '保证团队成员的环境一致，实验可复现', correct: true },
      { label: '最新版本总是有 bug' },
      { label: '固定版本占用更少磁盘空间' },
    ],
    explanation: '固定依赖版本是实现"可复现"原则的关键手段，确保不同时间、不同机器上运行实验能得到一致的结果。',
  },
]} />

## 延伸阅读

- [Python venv 文档](https://docs.python.org/3.11/library/venv.html)
- [Transformers 安装指南](https://huggingface.co/docs/transformers/installation)
- [PyTorch 安装页面](https://pytorch.org/)
