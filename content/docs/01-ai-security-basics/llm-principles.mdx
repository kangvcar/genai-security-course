---
title: 第2章：大语言模型的工作原理
description: 揭开大语言模型的神秘面纱，理解 Transformer 架构、文本分词和关键参数
---


import { Callout } from 'fumadocs-ui/components/callout';
import { Tab, Tabs } from 'fumadocs-ui/components/tabs';
import { Step, Steps } from 'fumadocs-ui/components/steps';
import { Accordion, Accordions } from 'fumadocs-ui/components/accordion';


<Callout title="" type="info">
预计阅读约17分钟
</Callout>

## 本章导读

当我们与 ChatGPT 对话时，它能够理解我们的问题，生成流畅的回答，甚至展现出一定的"创造力"。这背后的技术原理是什么？为什么大语言模型有时会"一本正经地胡说八道"？本章将揭开大语言模型的神秘面纱，帮助大家理解这些强大 AI 系统的工作机制。

## 学习目标

<Callout title="本章学完后，你将能够：" type="info">
1. **理解大语言模型的核心机制**：掌握 Transformer 架构的基本思想和文本分词原理
2. **认识关键参数的作用**：理解 Temperature、Top-p、Max Tokens 等参数如何影响模型输出
3. **把握模型的能力与局限**：了解大语言模型擅长什么、不擅长什么
4. **建立安全视角**：从工作原理的角度理解大语言模型的安全风险来源
</Callout>

## 从"自动补全"说起：理解语言模型的本质

### 一个熟悉的场景

当你在手机上打字时，输入法会自动推荐下一个可能的词语。例如，当你输入"今天天气"，输入法可能会建议"真好"、"不错"、"很热"等选项。

<Callout title="核心洞察" type="idea">
大语言模型本质上就是一个超级强大的"自动补全系统"。它通过学习海量的文本数据，掌握了语言的统计规律和语义关系。
</Callout>

### 语言模型的训练过程

<Steps>
  <Step>
    **准备训练数据**
    
    收集海量文本，包括网页、书籍、新闻、对话等。例如，GPT-3 的训练数据包含了约 45TB 的文本。
  </Step>
  <Step>
    **学习预测**
    
    模型会看到一个句子的前半部分，然后尝试预测下一个词。如果预测错误，模型会调整内部参数。
  </Step>
  <Step>
    **反复迭代**
    
    这个过程会重复数百万次，模型逐渐学会了语法规则、常见搭配、语义关系，甚至一些常识知识。
  </Step>
</Steps>

<Callout title="重要理解" type="warn">
模型真的"理解"了语言吗？从技术角度看，模型并没有像人类那样理解语言的含义，它只是学会了文本的统计规律。这种"不理解但能做得很好"的特性，正是大语言模型安全问题的根源之一。
</Callout>

## Transformer：大语言模型的核心架构

### 为什么需要 Transformer

在 Transformer 出现之前，处理文本的主流方法是循环神经网络（RNN）。RNN 的工作方式类似于逐字阅读：从左到右依次处理每个词。

<Tabs items={['RNN 的问题', 'Transformer 的优势']}>
  <Tab value="RNN 的问题">
    - 处理长文本时效率很低
    - 难以捕捉距离较远的词之间的关系
    - 需要"记住"前面的内容，容易遗忘
  </Tab>
  <Tab value="Transformer 的优势">
    - 可以同时关注整个句子中所有词之间的关系
    - 从"逐字阅读"升级到"一眼看全文"
    - 支持并行处理，训练和推理速度更快
  </Tab>
</Tabs>

### 注意力机制：Transformer 的核心

**注意力机制**（Attention Mechanism）让模型能够判断句子中哪些词之间的关系更重要。

```text title="注意力机制示例"
句子："银行的利率上涨了"

当模型处理"利率"这个词时，注意力机制会让它重点关注"银行"这个词，
因为这两个词在语义上密切相关。而"的"、"了"这些虚词的重要性就相对较低。
```

注意力机制的优势：

| 特性 | 说明 |
|-----|------|
| **并行处理** | 可以同时计算所有词之间的关系，大大提高速度 |
| **长距离依赖** | 能够轻松捕捉距离很远的词之间的关系 |
| **可解释性** | 可以可视化注意力权重，了解模型关注了哪些词 |

## 文本分词：将语言转化为数字

### 计算机如何"看懂"文字

计算机只能处理数字，不能直接理解文字。因此，在将文本输入模型之前，需要先将其转换为数字形式。这个过程称为**分词**（Tokenization）。

### 分词的不同策略

<Tabs items={['按字符分词', '按词分词', '子词分词']}>
  <Tab value="按字符分词">
    将每个字符作为一个 Token。例如，"你好"会被分为"你"和"好"两个 Token。
    
    - ✅ 词表很小，可以处理任何文本
    - ❌ 序列会很长，增加学习难度
  </Tab>
  <Tab value="按词分词">
    将每个完整的词作为一个 Token。例如，"今天天气很好"会被分为"今天"、"天气"、"很"、"好"。
    
    - ✅ 序列较短，语义单元更清晰
    - ❌ 词表会非常大，无法处理新词
  </Tab>
  <Tab value="子词分词">
    介于字符和词之间，将常见的词保持完整，将罕见的词拆分成更小的单元。
    
    - ✅ 词表大小合理，能处理新词
    - ✅ GPT 系列模型使用的 BPE 就是这种方法
  </Tab>
</Tabs>

### 分词对安全的影响

<Callout title="安全警示" type="error">
分词方式会影响模型的安全性。例如，如果"炸弹"被分为一个 Token，模型可以学会识别并拒绝。但如果攻击者使用"炸 弹"（中间加空格）或"zhàdàn"（拼音），分词结果就会不同，可能绕过过滤机制。
</Callout>

## 关键参数：控制模型的行为

### Temperature：控制"创造力"

Temperature（温度）参数控制模型输出的随机性，取值范围通常是 0 到 2。

| Temperature 值 | 效果 | 适用场景 |
|---------------|------|---------|
| **= 0** | 总是选择概率最高的词，输出确定、保守 | 需要一致性的场景 |
| **= 1** | 按概率分布采样，输出较为平衡 | 通用场景 |
| **> 1** | 增加低概率词被选中的机会，输出更"创造性" | 创意写作 |

<Callout title="安全影响" type="warn">
- 过低的 Temperature 可能让输出过于刻板
- 过高的 Temperature 可能让模型输出不可控的内容
- 在安全关键应用中，通常使用较低的 Temperature
</Callout>

### Top-p（Nucleus Sampling）

Top-p 是另一种控制输出随机性的方法。它只从累积概率达到 p 的最高概率词中采样。

```text title="Top-p 采样过程"
Top-p = 0.9 意味着：
1. 将所有可能的下一个词按概率从高到低排序
2. 累加概率，直到总和达到 0.9
3. 只从这些词中随机选择
```

### Max Tokens：控制输出长度

Max Tokens 参数限制模型生成的最大 Token 数量。这个参数对安全和成本控制都很重要：

- **防止资源滥用**：限制恶意用户生成极长文本
- **控制信息泄露**：降低模型输出大量信息的风险
- **避免失控输出**：作为"安全阀"，防止重复循环

## 大语言模型的能力与局限

### 模型擅长的任务

| 任务类型 | 说明 |
|---------|------|
| **文本生成和续写** | 写文章、编故事、生成代码 |
| **文本理解和分析** | 回答问题、总结文本、提取关键信息 |
| **格式转换** | 翻译、改写、风格转换 |
| **简单推理** | 常见的逻辑推理和常识问题 |

### 模型的局限性

<Callout title="幻觉问题" type="error">
这是大语言模型最严重的问题之一。模型有时会生成看似合理但完全错误的信息。例如，你问模型列举论文，它可能会编造出听起来很专业但实际不存在的论文标题和作者。

**原因**：模型的目标是生成"看起来合理"的文本，而不是"事实正确"的文本。
</Callout>

其他局限性：
- **缺乏真实世界的理解**：只学习了文本的统计规律
- **知识截止日期**：不知道训练数据截止日期之后发生的事情
- **缺乏持续学习能力**：训练完成后参数固定
- **容易受到提示词的影响**：稍微改变提问方式，可能得到完全不同的答案

### 从局限性看安全风险

| 局限性 | 安全风险 |
|-------|---------|
| 幻觉问题 | 错误信息传播，在医疗、法律等关键领域尤其危险 |
| 对提示词的敏感性 | 使得提示词注入攻击成为可能 |
| 缺乏真实理解 | 可能被诱导生成有害内容 |
| 知识截止日期 | 可能给出过时的建议 |

## 提示工程：与模型有效交互的艺术

### 提示词的基本结构

一个有效的提示词通常包含：

1. **角色设定**："你是一个专业的 Python 程序员"
2. **任务描述**："请帮我写一个函数，计算列表中所有偶数的和"
3. **输入数据**："列表是 [1, 2, 3, 4, 5, 6]"
4. **输出格式**："请用 Python 代码回答，并添加注释"
5. **约束条件**："不要使用第三方库"

### 提示工程与安全的关系

<Callout title="双刃剑" type="warn">
提示工程是一把双刃剑：
- **正面**：可以引导模型遵守安全规则，检测和过滤不当输入
- **负面**：攻击者也可以使用提示工程技巧来绕过安全机制

这就是为什么我们会专门学习提示词注入攻击和防御技术。
</Callout>

## 本章小结

1. **语言模型的本质**：大语言模型本质上是一个超级强大的"自动补全系统"
2. **Transformer 架构**：注意力机制让模型能够同时关注文本中所有词之间的关系
3. **文本分词**：将文字转换为数字的过程，分词策略会影响安全性
4. **关键参数**：Temperature、Top-p、Max Tokens 等参数控制模型的输出行为
5. **能力与局限**：模型存在幻觉、缺乏真实理解等严重局限
6. **提示工程**：有效的提示词设计可以提升表现，但也可能被攻击者利用


## 课后思考

<Accordions>
  <Accordion title="思考题1：超级自动补全系统">
    请用自己的话解释，为什么说大语言模型是“超级自动补全系统”？这种工作方式会带来哪些安全隐患？
  </Accordion>
  <Accordion title="思考题2：参数应用场景">
    **Temperature** 和 **Top-p** 参数都能控制输出的随机性，但工作原理不同。分析在什么场景下应该使用较低的值？
  </Accordion>
  <Accordion title="思考题3：安全风险设计">
    假设你正在开发一个 AI 写作助手，你会如何设置模型参数？请说明理由，并考虑可能的安全风险。
  </Accordion>
</Accordions>

## 延伸阅读

- [Attention Is All You Need (Hugging Face Papers)](https://huggingface.co/papers/1706.03762)
- [Attention Is All You Need (CORE)](https://core.ac.uk/outputs/83868954/)
