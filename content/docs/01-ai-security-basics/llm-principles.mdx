---
title: 第2章：大语言模型的工作原理
description: 揭开大语言模型的神秘面纱，理解 Transformer 架构、文本分词和关键参数
---


import { Callout } from 'fumadocs-ui/components/callout';
import { Tab, Tabs } from 'fumadocs-ui/components/tabs';
import { Step, Steps } from 'fumadocs-ui/components/steps';
import { Accordion, Accordions } from 'fumadocs-ui/components/accordion';


<Callout title="" type="info">
预计阅读约17分钟
</Callout>

## 本章导读

当我们与 ChatGPT 对话时，它能够理解我们的问题，生成流畅的回答，甚至展现出一定的"创造力"。这背后的技术原理是什么？为什么大语言模型有时会"一本正经地胡说八道"？本章将揭开大语言模型（Large Language Model, LLM）的神秘面纱，帮助大家理解这些强大 AI 系统的工作机制。

为了避免把知识点学成“零散名词”，本章会按一条清晰主线展开：先理解语言模型的本质，再看 Transformer 如何处理上下文，接着理解分词和参数如何影响输出，最后回到能力边界与安全风险。

## 学习目标

<Callout title="本章学完后，你将能够：" type="info">
1. **理解大语言模型的核心机制**：掌握 Transformer 架构的基本思想和文本分词原理
2. **认识关键参数的作用**：理解 Temperature、Top-p、Max Tokens 等参数如何影响模型输出
3. **把握模型的能力与局限**：了解大语言模型擅长什么、不擅长什么
4. **建立安全视角**：从工作原理的角度理解大语言模型的安全风险来源
</Callout>

## 1 从"自动补全"说起：理解语言模型的本质

### 1.1 一个熟悉的场景

当你在手机上打字时，输入法会自动推荐下一个可能的词语。例如，当你输入"今天天气"，输入法可能会建议"真好"、"不错"、"很热"等选项。

这个日常体验非常关键，因为它提供了理解大语言模型的最好入口：模型并不是先"想明白"再说话，而是基于上下文不断预测下一个最可能出现的 Token。只不过，相比手机输入法，大语言模型见过的数据更多、参数更大、上下文窗口更长，所以表现得更自然、更像在"理解"你。

<Callout title="核心洞察" type="idea">
大语言模型本质上就是一个超级强大的"自动补全系统"。它通过学习海量的文本数据，掌握了语言的统计规律和语义关系。
</Callout>

如果你只记住这一节的一句话，那就是：模型表现出的“聪明”，本质上来自大规模统计学习，而不是人类意义上的理解与意识。

### 1.2 语言模型的训练过程

<Steps>
  <Step>
    **准备训练数据**
    
    收集海量文本，包括网页、书籍、新闻、对话等。例如，GPT-3 的训练数据包含了约 45TB 的文本。
  </Step>
  <Step>
    **学习预测**
    
    模型会看到一个句子的前半部分，然后尝试预测下一个词。如果预测错误，模型会调整内部参数。
  </Step>
  <Step>
    **反复迭代**
    
    这个过程会重复数百万次，模型逐渐学会了语法规则、常见搭配、语义关系，甚至一些常识知识。
  </Step>
</Steps>

<Callout title="重要理解" type="warn">
模型真的"理解"了语言吗？从技术角度看，模型并没有像人类那样理解语言的含义，它只是学会了文本的统计规律。这种"不理解但能做得很好"的特性，正是大语言模型安全问题的根源之一。
</Callout>

理解了这一点，我们就可以顺着一个自然问题继续往下看：既然模型要在长文本中持续做预测，它内部到底靠什么机制处理上下文关系？这就引出了下一节的核心架构。

## 2 Transformer：大语言模型的核心架构

### 2.1 为什么需要 Transformer

在 Transformer 出现之前，处理文本的主流方法是循环神经网络（RNN）。RNN 的工作方式类似于逐字阅读：从左到右依次处理每个词。

这种"按顺序读"的方法在短句子里还可以工作，但一旦文本变长，模型就很难稳定记住远距离信息，训练效率也会明显下降。大语言模型要处理的是跨段落、跨主题的复杂上下文，因此需要一种更适合长文本和并行计算的架构。

<Tabs items={['RNN 的问题', 'Transformer 的优势']}>
  <Tab value="RNN 的问题">
    - 处理长文本时效率很低
    - 难以捕捉距离较远的词之间的关系
    - 需要"记住"前面的内容，容易遗忘
  </Tab>
  <Tab value="Transformer 的优势">
    - 可以同时关注整个句子中所有词之间的关系
    - 从"逐字阅读"升级到"一眼看全文"
    - 支持并行处理，训练和推理速度更快
  </Tab>
</Tabs>

### 2.2 注意力机制：Transformer 的核心

**注意力机制**（Attention Mechanism）让模型能够判断句子中哪些词之间的关系更重要。

```text title="注意力机制示例"
句子："银行的利率上涨了"

当模型处理"利率"这个词时，注意力机制会让它重点关注"银行"这个词，
因为这两个词在语义上密切相关。而"的"、"了"这些虚词的重要性就相对较低。
```

注意力机制的优势：

| 特性 | 说明 |
|-----|------|
| **并行处理** | 可以同时计算所有词之间的关系，大大提高速度 |
| **长距离依赖** | 能够轻松捕捉距离很远的词之间的关系 |
| **可解释性** | 可以可视化注意力权重，了解模型关注了哪些词 |

到这里我们知道了模型"怎么处理关系"，但还缺一块关键拼图：模型处理的一切最终都必须是数字。也就是说，在进入 Transformer 之前，文本必须先被转换成可计算的表示形式。

## 3 文本分词：将语言转化为数字

### 3.1 计算机如何"看懂"文字

计算机只能处理数字，不能直接理解文字。因此，在将文本输入模型之前，需要先将其转换为数字形式。这个过程称为**分词**（Tokenization）。

分词并不是一个纯工程细节，它会直接影响模型看到世界的方式：同一句话被切成不同 Token，模型感知到的模式就会不同，后续的预测行为也会随之变化。

### 3.2 分词的不同策略

<Tabs items={['按字符分词', '按词分词', '子词分词']}>
  <Tab value="按字符分词">
    将每个字符作为一个 Token。例如，"你好"会被分为"你"和"好"两个 Token。
    
    - ✅ 词表很小，可以处理任何文本
    - ❌ 序列会很长，增加学习难度
  </Tab>
  <Tab value="按词分词">
    将每个完整的词作为一个 Token。例如，"今天天气很好"会被分为"今天"、"天气"、"很"、"好"。
    
    - ✅ 序列较短，语义单元更清晰
    - ❌ 词表会非常大，无法处理新词
  </Tab>
  <Tab value="子词分词">
    介于字符和词之间，将常见的词保持完整，将罕见的词拆分成更小的单元。
    
    - ✅ 词表大小合理，能处理新词
    - ✅ GPT 系列模型使用的 BPE 就是这种方法
  </Tab>
</Tabs>

### 3.3 分词对安全的影响

<Callout title="安全警示" type="error">
分词方式会影响模型的安全性。例如，如果"炸弹"被分为一个 Token，模型可以学会识别并拒绝。但如果攻击者使用"炸 弹"（中间加空格）或"zhàdàn"（拼音），分词结果就会不同，可能绕过过滤机制。
</Callout>

当我们理解了"架构"与"分词"这两层基础后，就可以进一步回答一个更贴近实践的问题：同一个模型，为什么有时回答稳定，有时又很发散？这通常和推理阶段的采样参数设置直接相关。

## 4 关键参数：控制模型的行为

### 4.1 Temperature：控制"创造力"

Temperature（温度）参数控制模型输出的随机性，取值范围通常是 0 到 2。

| Temperature 值 | 效果 | 适用场景 |
|---------------|------|---------|
| **= 0** | 总是选择概率最高的词，输出确定、保守 | 需要一致性的场景 |
| **= 1** | 按概率分布采样，输出较为平衡 | 通用场景 |
| **> 1** | 增加低概率词被选中的机会，输出更"创造性" | 创意写作 |

<Callout title="安全影响" type="warn">
- 过低的 Temperature 可能让输出过于刻板
- 过高的 Temperature 可能让模型输出不可控的内容
- 在安全关键应用中，通常使用较低的 Temperature
</Callout>

从工程角度看，参数不是“调口味”的小选项，而是“控制风险暴露面”的关键开关。

### 4.2 Top-p（Nucleus Sampling）

Top-p 是另一种控制输出随机性的方法。它只从累积概率达到 p 的最高概率词中采样。

```text title="Top-p 采样过程"
Top-p = 0.9 意味着：
1. 将所有可能的下一个词按概率从高到低排序
2. 累加概率，直到总和达到 0.9
3. 只从这些词中随机选择
```

可以把 Temperature 和 Top-p 理解为两种互补的"随机性控制旋钮"：前者调整概率分布的平滑程度，后者限制可选词的候选范围。二者组合决定了输出是在"稳健可控"和"多样创造"之间的具体位置。

### 4.3 Max Tokens：控制输出长度

Max Tokens 参数限制模型生成的最大 Token 数量。这个参数对安全和成本控制都很重要：

- **防止资源滥用**：限制恶意用户生成极长文本
- **控制信息泄露**：降低模型输出大量信息的风险
- **避免失控输出**：作为"安全阀"，防止重复循环

到这里我们已经知道了模型能如何生成文本。下一步自然要问的是：这些机制在现实任务中到底带来了哪些能力，又在哪些地方会失效？只有把能力和边界同时看清，才能建立可靠的安全判断。

## 5 大语言模型的能力与局限

### 5.1 模型擅长的任务

| 任务类型 | 说明 |
|---------|------|
| **文本生成和续写** | 写文章、编故事、生成代码 |
| **文本理解和分析** | 回答问题、总结文本、提取关键信息 |
| **格式转换** | 翻译、改写、风格转换 |
| **简单推理** | 常见的逻辑推理和常识问题 |

### 5.2 模型的局限性

<Callout title="幻觉问题" type="error">
这是大语言模型最严重的问题之一。模型有时会生成看似合理但完全错误的信息。例如，你问模型列举论文，它可能会编造出听起来很专业但实际不存在的论文标题和作者。

**原因**：模型的目标是生成"看起来合理"的文本，而不是"事实正确"的文本。
</Callout>

其他局限性：
- **缺乏真实世界的理解**：只学习了文本的统计规律
- **知识截止日期**：不知道训练数据截止日期之后发生的事情
- **缺乏持续学习能力**：训练完成后参数固定
- **容易受到提示词的影响**：稍微改变提问方式，可能得到完全不同的答案

### 5.3 从局限性看安全风险

| 局限性 | 安全风险 |
|-------|---------|
| 幻觉问题 | 错误信息传播，在医疗、法律等关键领域尤其危险 |
| 对提示词的敏感性 | 使得提示词注入攻击成为可能 |
| 缺乏真实理解 | 可能被诱导生成有害内容 |
| 知识截止日期 | 可能给出过时的建议 |

当我们明确了这些局限，就会发现一个现实结论：与其假设模型"天然可靠"，不如通过更精确的交互方式主动约束它的行为。这就是提示词工程在工程实践和安全防护中都很重要的原因。

## 6 提示词工程：与模型有效交互的艺术

### 6.1 提示词的基本结构

一个有效的提示词通常包含：

1. **角色设定**："你是一个专业的 Python 程序员"
2. **任务描述**："请帮我写一个函数，计算列表中所有偶数的和"
3. **输入数据**："列表是 [1, 2, 3, 4, 5, 6]"
4. **输出格式**："请用 Python 代码回答，并添加注释"
5. **约束条件**："不要使用第三方库"

### 6.2 提示词工程与安全的关系

<Callout title="双刃剑" type="warn">
提示词工程是一把双刃剑：
- **正面**：可以引导模型遵守安全规则，检测和过滤不当输入
- **负面**：攻击者也可以使用提示词工程技巧来绕过安全机制

这就是为什么我们会专门学习提示词注入攻击和防御技术。
</Callout>

换句话说，本章前半部分讲的是"模型为什么会这样表现"，而提示词工程回答的是"我们如何在这个机制之上更可控地使用它"。两者结合，才构成后续学习 AI 安全攻防的共同基础。

也就是说，理解原理不是为了“会背概念”，而是为了在真实应用中做出更稳健、更安全的工程决策。

## 7 本章小结

1. **语言模型的本质**：大语言模型本质上是一个超级强大的"自动补全系统"
2. **Transformer 架构**：注意力机制让模型能够同时关注文本中所有词之间的关系
3. **文本分词**：将文字转换为数字的过程，分词策略会影响安全性
4. **关键参数**：Temperature、Top-p、Max Tokens 等参数控制模型的输出行为
5. **能力与局限**：模型存在幻觉、缺乏真实理解等严重局限
6. **提示词工程**：有效的提示词设计可以提升表现，但也可能被攻击者利用


## 课后思考

<Accordions>
  <Accordion title="思考题1：超级自动补全系统">
    请用自己的话解释，为什么说大语言模型是“超级自动补全系统”？这种工作方式会带来哪些安全隐患？
  </Accordion>
  <Accordion title="思考题2：参数应用场景">
    **Temperature** 和 **Top-p** 参数都能控制输出的随机性，但工作原理不同。分析在什么场景下应该使用较低的值？
  </Accordion>
  <Accordion title="思考题3：安全风险设计">
    假设你正在开发一个 AI 写作助手，你会如何设置模型参数？请说明理由，并考虑可能的安全风险。
  </Accordion>
</Accordions>

## 延伸阅读

- [Attention Is All You Need (Hugging Face Papers)](https://huggingface.co/papers/1706.03762)
- [Attention Is All You Need (CORE)](https://core.ac.uk/outputs/83868954/)
