---
title: 第3章：黑盒攻击技术
description: 掌握迁移攻击和基于查询的攻击策略
---

import { Callout } from 'fumadocs-ui/components/callout';
import { Steps, Step } from 'fumadocs-ui/components/steps';
import { Tabs, Tab } from 'fumadocs-ui/components/tabs';

在上一章中，我们学习了白盒攻击技术，了解了如何在完全掌握模型信息的情况下生成对抗样本。然而，现实世界中的大多数 AI 系统并不会向用户公开其内部结构和参数。当我们面对一个"看不见内部"的模型时，还能否发起有效的攻击？

## 章节目标

<Callout title="学完本章后，你将能够：" type="info">
1. **理解黑盒攻击与白盒攻击的本质区别**：认识黑盒场景的现实意义
2. **掌握迁移攻击的核心原理**：能够解释对抗样本为何具有跨模型迁移性
3. **了解基于查询的攻击策略**：理解其工作机制和效率权衡
4. **认识实际威胁**：理解黑盒攻击在商业 AI 服务中的安全风险
</Callout>

## 1. 从白盒到黑盒：现实世界的攻击挑战

### 1.1 什么是黑盒攻击

<Tabs items={['白盒攻击', '黑盒攻击']}>
  <Tab value="白盒攻击">
    **White-box Attack**
    
    攻击者能够获取模型的完整信息：
    - 网络结构
    - 参数权重
    - 可以计算梯度
    
    模型对攻击者来说是**完全透明**的，就像一个透明的白色盒子。
  </Tab>
  <Tab value="黑盒攻击">
    **Black-box Attack**
    
    攻击者面临以下限制：
    - 无法获知模型的网络结构
    - 无法获取模型的参数权重
    - 无法计算损失函数对输入的梯度
    - 只能向模型发送查询请求
    - 只能获得模型的输出结果
    
    模型就像一个**不透明的黑色盒子**，只能观察输入和输出。
  </Tab>
</Tabs>

### 1.2 黑盒场景的现实意义

<Callout title="为什么黑盒攻击更重要？" type="warn">
我们日常接触的绑大多数 AI 服务都是黑盒形式的：
- 商业图像识别 API（如百度、腾讯、阿里云）
- 人脸识别门禁系统
- 自动驾驶的感知模块
- 医疗影像诊断系统

用户只能上传数据、接收结果，无法获知模型内部细节。
</Callout>

如果我们只研究白盒攻击，研究成果的实际应用价值将非常有限。**真正具有现实威胁的，恰恰是那些能够在黑盒条件下发起的攻击**。

### 1.3 黑盒攻击的两大策略

| 策略 | 核心思想 | 特点 |
|------|---------|------|
| **迁移攻击** | 在本地替代模型上生成对抗样本，利用迁移性攻击目标模型 | 不需要查询目标模型，但成功率相对较低 |
| **基于查询的攻击** | 反复查询目标模型，根据返回结果逐步调整扰动 | 成功率较高，但需要大量查询，成本高 |

## 2. 迁移攻击：借助替代模型的间接攻击

### 2.1 对抗样本迁移性的发现

2014年，Szegedy 等人发现了一个令人惊讶的现象：

<Callout title="迁移性发现" type="info">
针对某个特定模型生成的对抗样本，不仅能欺骗该模型，还能以**一定概率**欺骗其他结构完全不同的模型！
</Callout>

**为什么会有迁移性？**

尽管不同的深度学习模型在结构上存在差异，但它们在学习同一任务时，往往会学到**相似的特征表示**：
- 浅层网络：学习边缘、纹理等低级特征
- 深层网络：学习更抽象的语义特征

这种特征学习的相似性，导致不同模型的**决策边界**在某些方向上具有相似的脆弱性。

<Callout title="类比理解">
想象不同的模型就像不同的老师，他们各自出题考试。虽然每位老师的出题风格不同，但如果他们教的是同一门课程，那么他们关注的重点知识往往是相似的。一个学生如果找到了某种"投机取巧"的答题方式能骗过一位老师，这种方式很可能对其他老师也有一定效果。
</Callout>

### 2.2 迁移攻击的实施流程

<Steps>
  <Step>
    **收集训练数据**：收集与目标任务相关的数据（如攻击人脸识别系统，需收集人脸图像数据）
  </Step>
  <Step>
    **训练替代模型**：在本地训练一个或多个模型（称为**替代模型**或**代理模型**）
  </Step>
  <Step>
    **生成对抗样本**：使用白盒攻击方法（如 FGSM 或 PGD）在替代模型上生成对抗样本
  </Step>
  <Step>
    **测试迁移效果**：将生成的对抗样本发送给目标模型，观察攻击是否成功
  </Step>
</Steps>

### 2.3 提高迁移成功率的方法

单一替代模型生成的对抗样本迁移成功率往往不够理想。研究者提出了多种方法来提高迁移性：

<Tabs items={['集成多个模型', '输入变换增强', '攻击中间层特征']}>
  <Tab value="集成多个模型">
    **方法**：同时使用多个不同架构的模型，要求扰动能够同时欺骗所有这些模型。
    
    **原理**：如果一个扰动能够欺骗多个不同的模型，它更可能捕捉到了深度学习模型的某种**共性弱点**。
    
    **实现**：将多个模型的损失函数加权求和，然后对综合损失计算梯度生成扰动。
  </Tab>
  <Tab value="输入变换增强">
    **方法**：在生成过程中对输入图像施加各种随机变换（随机缩放、裁剪、旋转等）。
    
    **原理**：生成的扰动需要在各种变换下都保持有效，因此更加"鲁棒"，更容易迁移。
  </Tab>
  <Tab value="攻击中间层特征">
    **方法**：除了攻击模型的最终输出，还攻击模型的中间层特征表示。
    
    **原理**：不同模型的中间层特征往往比最终输出更加相似，针对中间层的攻击可能具有更好的迁移性。
  </Tab>
</Tabs>

<Callout title="实际效果">
结合多种技术的迁移攻击成功率可以达到 **50% 以上**，这对于商业 AI 系统来说是一个不可忽视的威胁。
</Callout>

## 3. 基于查询的攻击：通过试错逼近目标

### 3.1 查询攻击的基本思想

如果攻击者能够反复查询目标模型，是否能找到更可靠的攻击方法？

<Callout title="猜数字类比">
假设有人心里想了一个 1 到 100 之间的数字，你需要猜出这个数字。每次猜测后，对方会告诉你"大了"或"小了"。虽然你不知道答案是什么，但通过不断猜测和获取反馈，你可以逐步缩小范围，最终找到正确答案。

在对抗攻击中：
- **猜测** → 尝试不同的扰动
- **反馈** → 模型的输出结果
</Callout>

### 3.2 基于评分的查询攻击

适用于模型返回**置信度分数**的情况（如"猫: 0.85, 狗: 0.10, 兔子: 0.05"）。

**梯度估计方法**：

| 方法 | 原理 |
|------|------|
| **有限差分法** | 在当前点的某个方向上施加微小扰动，观察输出变化，估计该方向上的梯度 |
| **进化算法（NES）** | 维护一个扰动的"种群"，通过模拟自然选择不断淘汰效果差的扰动，保留和变异效果好的扰动 |

### 3.3 基于决策的查询攻击

适用于模型**只返回最终类别标签**的情况（更具挑战性）。

<Callout title="HopSkipJump 算法" type="info">
一个著名的算法，思想非常巧妙：
1. 首先找到一个已经能够欺骗模型的起点（如一张随机噪声图片被错误分类）
2. 逐步向原始图片靠近，同时保持"攻击成功"的状态
3. 最终找到决策边界上距离原始图片最近的点

**类比**：你站在一个房间里，房间被一堵看不见的墙分成两半。你知道自己在墙的哪一侧，但不知道墙的具体位置。通过不断尝试向前走并检查是否越过了墙，你可以逐步找到墙的位置。
</Callout>

### 3.4 查询攻击的效率与成本

查询攻击面临的实际成本问题：

| 攻击类型 | 典型查询次数 | 挑战 |
|---------|-------------|------|
| 基于评分的攻击 | 1,000 - 5,000 次 | 按查询收费的 API 成本高 |
| 基于决策的攻击 | 5,000 - 20,000 次 | 可能触发安全机制，被识别为攻击行为 |

<Callout title="防御机会" type="info">
这些限制使得查询攻击在实际应用中面临诸多挑战，但也为防御方提供了**检测和阻止攻击的机会**。
</Callout>

## 4. 真实案例：商业 AI 服务的黑盒攻击

### 4.1 案例背景

2020年，来自清华大学和加州大学伯克利分校的研究团队展示了对多个商业人脸识别 API 的黑盒攻击，包括：
- Amazon Rekognition
- Microsoft Azure Face API
- 国内某知名云服务商

### 4.2 攻击过程

<Steps>
  <Step>
    **部署替代模型**：在本地部署多个公开的人脸识别模型（FaceNet、ArcFace 等）
  </Step>
  <Step>
    **集成攻击**：同时针对多个替代模型生成对抗样本，使用输入变换增强技术提高迁移性
  </Step>
  <Step>
    **测试攻击效果**：将对抗样本上传到各个商业 API 进行测试
  </Step>
</Steps>

### 4.3 攻击结果

| 攻击场景 | 迁移成功率 |
|---------|-----------|
| **拒绝服务攻击**（让系统无法识别人脸） | 47% - 67% |
| **身份冒充攻击**（让系统将一个人误识别为另一个人） | 12% - 22% |

<Callout title="安全警示" type="error">
这意味着攻击者只需要生成几个对抗样本进行尝试，就有相当大的概率成功。考虑到人脸识别技术已广泛应用于支付验证、门禁系统、身份认证等安全敏感场景，这一发现具有重要的安全警示意义。
</Callout>

### 4.4 案例启示

| 启示 | 说明 |
|------|------|
| **商业服务并非坚不可摧** | 即使是技术实力雄厚的公司提供的服务，也容易受到对抗攻击 |
| **迁移性是真实风险** | 攻击者不需要知道目标系统的任何内部信息 |
| **需考虑对抗鲁棒性** | AI 服务提供商在部署模型时不能仅关注正常情况下的准确率 |

## 5. 本章小结

<Steps>
  <Step>
    **黑盒攻击的现实意义**：大多数商业 AI 服务都以黑盒形式提供，黑盒攻击比白盒攻击更具现实威胁
  </Step>
  <Step>
    **迁移攻击**：利用对抗样本的跨模型迁移性，在本地替代模型上生成对抗样本攻击目标模型
  </Step>
  <Step>
    **提高迁移性的方法**：集成多个替代模型、使用输入变换增强、攻击中间层特征
  </Step>
  <Step>
    **基于查询的攻击**：通过反复查询目标模型，利用梯度估计或进化算法逐步优化扰动
  </Step>
  <Step>
    **真实威胁**：商业人脸识别 API 的攻击案例证明黑盒攻击是实际存在的安全威胁
  </Step>
</Steps>

## 术语对照表

| 中文术语 | 英文术语 | 简要解释 |
|---------|---------|---------|
| 黑盒攻击 | Black-box Attack | 攻击者不知道模型内部信息，只能通过输入输出进行攻击 |
| 白盒攻击 | White-box Attack | 攻击者完全掌握模型的结构、参数等信息 |
| 迁移攻击 | Transfer Attack | 利用对抗样本的迁移性，在替代模型上生成样本攻击目标模型 |
| 替代模型 | Surrogate Model | 攻击者在本地训练的、用于生成对抗样本的模型 |
| 决策边界 | Decision Boundary | 模型区分不同类别的分界线 |
| 梯度估计 | Gradient Estimation | 在无法直接计算梯度时，通过查询来近似估计梯度的方法 |

## 课后思考题

1. **理解性问题**：对抗样本为什么会具有跨模型的迁移性？请从深度学习模型学习特征的角度进行解释。

2. **分析性问题**：某商业图像识别 API 为了增加攻击难度，决定只返回置信度最高的类别标签，而不返回具体的置信度分数。这种做法能否有效防御查询攻击？攻击者还有什么应对策略？

3. **应用性问题**：假设你是一家提供人脸识别服务的公司的安全工程师，基于本章学习的黑盒攻击知识，你会建议采取哪些措施来提高系统的安全性？

<Callout title="法律与伦理提醒" type="warn">
对商业 AI 服务进行未经授权的对抗攻击测试可能违反服务条款和相关法律法规。根据《中华人民共和国网络安全法》和《中华人民共和国刑法》的相关规定，未经授权入侵计算机信息系统或破坏计算机信息系统功能的行为可能构成犯罪。本章内容仅用于安全教育和防御研究，读者在进行任何安全测试前，必须获得系统所有者的明确授权。
</Callout>
