---
title: 第2章：白盒攻击技术
description: 深入学习 FGSM 和 PGD 算法，掌握基于梯度的攻击方法
---

import { Callout } from 'fumadocs-ui/components/callout';
import { Steps, Step } from 'fumadocs-ui/components/steps';
import { Tabs, Tab } from 'fumadocs-ui/components/tabs';
import { Accordion, Accordions } from 'fumadocs-ui/components/accordion';

在上一章中，我们了解了对抗样本的基本概念，知道了通过添加微小的扰动可以欺骗 AI 模型。但是，这些扰动是如何计算出来的呢？为什么攻击者能够精确地找到模型的弱点？本章将带领大家深入理解白盒攻击的工作原理。

## 章节目标

<Callout title="学完本章后，你将能够：" type="info">
1. **理解白盒攻击的基本原理**：掌握攻击者需要的信息和攻击思路
2. **掌握 FGSM 算法**：能解释其核心思想和工作机制
3. **理解 PGD 算法**：了解如何通过迭代优化提升攻击成功率
4. **分析算法优缺点**：能够判断不同攻击算法的适用场景
5. **认识实际威胁**：理解白盒攻击对 AI 系统安全性的影响
</Callout>

## 1. 白盒攻击的基本概念

### 1.1 什么是白盒攻击

**白盒攻击（White-box Attack）**是指攻击者完全了解目标模型内部结构的攻击方式。

<Callout title="生活类比">
假设你要破解一个保险箱，白盒攻击就像是你不仅知道保险箱的品牌型号，还拿到了它的设计图纸、内部结构说明书，甚至知道锁芯的工作原理。有了这些信息，你就能更精确地找到破解方法。
</Callout>

### 1.2 攻击者掌握的信息

在 AI 安全的场景中，白盒攻击者通常掌握以下信息：

| 信息类型 | 具体内容 |
|---------|---------|
| **模型架构** | 模型有多少层、每层神经元数量、激活函数类型 |
| **模型参数** | 所有权重和偏置参数 |
| **训练数据** | 了解模型的训练数据分布 |
| **损失函数** | 模型训练时使用的优化目标 |

### 1.3 白盒攻击的现实可能性

<Tabs items={['开源模型', '模型窃取', '内部威胁', '学术研究']}>
  <Tab value="开源模型">
    许多研究机构和公司会公开发布模型架构和参数，如 BERT、ResNet 等。攻击者可以直接对这些模型进行白盒攻击。
  </Tab>
  <Tab value="模型窃取">
    攻击者通过大量查询 API，可以训练一个近似的替代模型，然后对替代模型进行白盒攻击。
  </Tab>
  <Tab value="内部威胁">
    公司内部人员可能直接访问模型，存在内部攻击风险。
  </Tab>
  <Tab value="学术研究">
    研究人员需要在白盒条件下测试模型的鲁棒性，这也是评估安全性的标准方法。
  </Tab>
</Tabs>

### 1.4 白盒攻击的核心思想

<Callout title="核心思想" type="warn">
**利用模型的梯度信息，找到让模型犯错的最优扰动方向。**
</Callout>

**梯度的理解**：
- 梯度表示函数值变化最快的方向
- 类比：爬山时的坡度——梯度大的地方坡度陡，梯度小的地方坡度缓
- 训练时：沿着梯度的**反方向**调整参数，让预测越来越准确
- 攻击时：沿着梯度的**方向**修改输入，让预测越来越错误

### 1.5 白盒攻击的优势与局限

| 优势 | 局限 |
|------|------|
| 攻击效率高：直接计算梯度，快速生成对抗样本 | 信息要求高：需要完全了解目标模型 |
| 成功率高：利用完整模型信息，攻击更精准 | 迁移性问题：针对特定模型生成的样本可能对其他模型无效 |
| 扰动更小：可以优化扰动大小，更难被察觉 | 防御相对容易：攻击原理清晰，可针对性防御 |

## 2. 快速梯度符号法（FGSM）

### 2.1 FGSM 的诞生背景

2014年，Google 的研究员 Ian Goodfellow 等人发表了开创性论文《Explaining and Harnessing Adversarial Examples》，首次提出了**快速梯度符号法（Fast Gradient Sign Method，FGSM）**。

<Callout title="历史意义" type="info">
这个算法的提出震惊了整个 AI 安全领域，因为它证明了：即使是最先进的深度学习模型，也可以被简单的数学方法轻易欺骗。生成对抗样本**只需要一步计算**就能完成。
</Callout>

### 2.2 FGSM 的核心原理

FGSM 的核心思想：**沿着损失函数梯度的方向，给输入添加一个固定大小的扰动**。

<Steps>
  <Step>
    **计算梯度**：计算损失函数相对于输入图片的梯度，这告诉我们改变哪些像素会让模型的预测发生最大变化
  </Step>
  <Step>
    **提取符号**：不关心梯度的具体数值，只关心方向（正或负），这就是"符号"的含义
  </Step>
  <Step>
    **添加扰动**：在梯度为正的地方增加像素值，在梯度为负的地方减少像素值，扰动大小由参数 ε 控制
  </Step>
</Steps>

### 2.3 FGSM 的数学公式

用数学语言描述，FGSM 生成对抗样本的公式是：

$$
x_{adv} = x + \epsilon \cdot \text{sign}(\nabla_x J(\theta, x, y))
$$

其中：
- $x$：原始图片
- $x_{adv}$：对抗样本
- $\epsilon$：扰动强度参数
- $\text{sign}$：符号函数，将梯度转换为 +1 或 -1
- $\nabla_x J$：损失函数对输入的梯度

<Callout title="为什么只用符号？">
使用符号而非实际梯度值有两个好处：
1. 计算非常快，只需一步就能完成
2. 扰动大小可控，不会让图片变化太大而被人眼察觉
</Callout>

### 2.4 FGSM 的实际效果

<Tabs items={['攻击前', '攻击过程', '攻击后']}>
  <Tab value="攻击前">
    **原始图片**：一张熊猫的照片
    
    **模型预测**：熊猫（置信度 99.8%）
  </Tab>
  <Tab value="攻击过程">
    1. 设置扰动强度 $\epsilon = 0.007$（每个像素最多改变 0.7%）
    2. 计算损失函数相对于输入图片的梯度
    3. 使用 FGSM 公式生成对抗样本
  </Tab>
  <Tab value="攻击后">
    **对抗样本**：人眼看来仍然是熊猫
    
    **模型预测**：长臂猿（置信度 99.3%）
    
    **关键发现**：只改变了 0.7% 的像素值，模型预测完全改变！
  </Tab>
</Tabs>

### 2.5 FGSM 的变体

| 变体 | 说明 |
|------|------|
| **目标攻击版本** | 指定让模型预测为某个特定类别，而非任意错误类别 |
| **迭代版本（I-FGSM）** | 多次添加小扰动，每次重新计算梯度，成功率更高但计算时间更长 |
| **随机启动版本** | 先添加小的随机噪声再执行 FGSM，避免陷入局部最优 |

## 3. 投影梯度下降法（PGD）

### 3.1 PGD 的改进思路

虽然 FGSM 简单高效，但它只执行**一步攻击**，可能无法找到最优的对抗样本。

2017年，Aleksander Madry 等人提出了**投影梯度下降法（Projected Gradient Descent，PGD）**。PGD 是 FGSM 的**迭代增强版本**。

<Callout title="FGSM vs PGD 类比">
假设你要在一个黑暗的房间里找到出口：
- **FGSM**：打开手电筒看一眼，然后朝着看起来最有希望的方向走**一大步**
- **PGD**：打开手电筒，朝着有希望的方向走**一小步**，再次观察，调整方向，再走一小步，如此反复
</Callout>

### 3.2 PGD 的工作流程

<Steps>
  <Step>
    **随机初始化**：不从原始图片开始，而是在原始图片附近随机选择一个起点，避免陷入局部最优
  </Step>
  <Step>
    **迭代攻击**：重复执行（通常 10-100 次）：
    - 计算当前图片的梯度
    - 沿着梯度方向移动一小步（步长 α，通常比 FGSM 的 ε 小）
    - 检查是否超出允许的扰动范围
  </Step>
  <Step>
    **投影操作**：如果超出了扰动范围 ε，将其"投影"回来（如某像素扰动达到 0.05，规定最大 0.03，则限制回 0.03）
  </Step>
  <Step>
    **输出对抗样本**：经过多次迭代后，输出最终的对抗样本
  </Step>
</Steps>

### 3.3 PGD 的数学直觉

PGD 本质上是在解决一个优化问题：**在扰动大小受限的条件下，找到让模型预测错误程度最大的扰动**。

<Callout title="寻宝类比">
想象在一个有限区域内寻宝：
- **区域边界**：由扰动大小 ε 决定
- **宝藏位置**：由模型的弱点决定
- **随机起点**：避免遗漏宝藏
- **梯度指引**：朝着宝藏方向前进
- **边界约束**：如果走出区域，退回到边界上
</Callout>

### 3.4 PGD 与 FGSM 的对比

| 特性 | FGSM | PGD |
|------|------|-----|
| 迭代次数 | 1次 | 多次（10-100次） |
| 计算时间 | 快（毫秒级） | 慢（秒级） |
| 攻击成功率 | 较高 | 很高 |
| 扰动优化程度 | 低 | 高 |
| 初始化方式 | 原始图片 | 随机扰动 |
| 适用场景 | 快速测试 | 严格评估 |

<Callout title="何时使用哪种方法？">
- **FGSM**：适合快速测试模型的基本鲁棒性
- **PGD**：适合进行更严格的安全评估，是目前评估模型鲁棒性的**黄金标准**
</Callout>

### 3.5 PGD 在实际中的应用

**案例：ImageNet 鲁棒性测试（2018）**

研究人员使用 PGD 对 ImageNet 上的多个顶级模型进行了鲁棒性测试：

| 模型 | 干净图片准确率 | PGD 攻击下准确率 |
|------|--------------|-----------------|
| ResNet-50 | 76% | **0%** |
| Inception-v3 | 78% | **0%** |
| DenseNet-121 | 75% | **0%** |

<Callout title="启示" type="error">
即使是在大规模数据集上训练的先进模型，在面对精心设计的对抗攻击时也**非常脆弱**。
</Callout>

## 4. 白盒攻击的实战考虑

### 4.1 攻击强度的选择

在实际应用中，攻击者需要在**攻击成功率**和**隐蔽性**之间做权衡：

| 扰动强度 ε | 可见性 | 攻击成功率 |
|-----------|--------|-----------|
| 0.001 | 几乎不可见 | ~30% |
| 0.01 | 仔细观察可能被发现 | ~70% |
| 0.03 | 某些情况下可见 | ~95% |
| 0.1 | 明显可见 | ~100% |

<Callout title="实践建议">
实际攻击场景中，攻击者通常选择 **ε = 0.01 到 0.03** 之间的值，既能保证较高成功率，又不容易被人眼察觉。
</Callout>

### 4.2 不同模型架构的鲁棒性差异

| 模型类型 | 对白盒攻击的抵抗能力 |
|---------|-------------------|
| **卷积神经网络（CNN）** | 较为脆弱，决策边界不够平滑 |
| **Vision Transformer（ViT）** | 表现更好，但仍可被攻击 |
| **对抗训练模型** | 鲁棒性强，但正常准确率可能下降 2-5% |

### 4.3 白盒攻击的实际威胁

<Tabs items={['开源模型风险', '模型窃取攻击', '供应链攻击']}>
  <Tab value="开源模型风险">
    许多公司使用公开发布的模型（如 BERT、GPT 等），攻击者可以直接对这些模型进行白盒攻击。
  </Tab>
  <Tab value="模型窃取攻击">
    即使模型不公开，攻击者可以通过大量查询 API，训练一个近似的替代模型，然后进行白盒攻击。研究表明，这种攻击生成的对抗样本有很高的迁移性。
  </Tab>
  <Tab value="供应链攻击">
    在模型开发和部署过程中，内部人员可能接触到模型的完整信息，从而进行白盒攻击。
  </Tab>
</Tabs>

**案例：特斯拉自动驾驶系统（2019）**

腾讯科恩实验室的研究人员在道路上放置了经过精心设计的贴纸：
- 对人眼：只是普通的图案
- 对特斯拉：能够欺骗车道识别系统，导致车辆偏离车道

### 4.4 白盒攻击的防御挑战

| 防御方法 | 说明 | 优缺点 |
|---------|------|--------|
| **对抗训练** | 训练时加入对抗样本 | 最有效，但增加训练成本，可能降低正常准确率 |
| **输入预处理** | 处理前降噪、压缩 | 容易被自适应攻击破解 |
| **模型集成** | 多模型综合预测 | 提高鲁棒性，但计算成本高 |
| **认证防御** | 数学证明在一定范围内预测不变 | 最可靠，但只适用于小规模模型 |

<Callout title="攻防博弈" type="warn">
攻防是一个持续演化的过程。每当出现新的防御方法，攻击者就会设计新的攻击来破解它。这种"猫鼠游戏"推动了 AI 安全领域的不断发展。
</Callout>

## 本章小结

<Steps>
  <Step>
    **白盒攻击的本质**：利用模型的梯度信息，找到让模型犯错的最优扰动方向
  </Step>
  <Step>
    **FGSM 算法**：最简单高效的白盒攻击方法，只需一步计算，沿着梯度符号方向添加固定大小的扰动
  </Step>
  <Step>
    **PGD 算法**：FGSM 的迭代增强版本，通过多次迭代和投影操作生成更强的对抗样本，是评估鲁棒性的黄金标准
  </Step>
  <Step>
    **实战考虑**：需要在攻击成功率和隐蔽性之间权衡，不同模型架构的抵抗能力有所不同
  </Step>
  <Step>
    **防御挑战**：防御白盒攻击是一个持续演化的过程，对抗训练是目前最有效的方法
  </Step>
</Steps>

## 课后思考题

<Accordions>
  <Accordion title="思考题1：FGSM 的符号函数">
    FGSM 为什么只使用梯度的符号而不是实际的梯度值？这样做有什么好处？
  </Accordion>
  <Accordion title="思考题2：PGD vs FGSM">
    为什么 PGD 的攻击成功率通常高于 FGSM？它付出了什么代价？
  </Accordion>
  <Accordion title="思考题3：安全性评估">
    如果你需要评估一个图像分类模型的安全性，你会选择 FGSM 还是 PGD？请说明你的选择理由。
  </Accordion>
</Accordions>
