---
title: 第2章：白盒攻击技术
description: 深入学习 FGSM 和 PGD 算法，掌握基于梯度的攻击方法
---


import { Callout } from 'fumadocs-ui/components/callout';
import { Steps, Step } from 'fumadocs-ui/components/steps';
import { Tabs, Tab } from 'fumadocs-ui/components/tabs';
import { Accordion, Accordions } from 'fumadocs-ui/components/accordion';


<Callout title="" type="info">
预计阅读约24分钟
</Callout>

## 本章导读

在上一章中，我们了解了对抗样本的基本概念，知道了通过添加微小的扰动可以欺骗AI模型。但是，这些扰动是如何计算出来的呢？为什么攻击者能够精确地找到模型的弱点？本章将带领大家深入理解白盒攻击的工作原理。我们将学习两种经典的白盒攻击算法：快速梯度符号法（FGSM）和投影梯度下降法（PGD），并通过实验观察它们的攻击效果。学习这些内容不仅能帮助我们理解AI模型的脆弱性，更重要的是为后续学习防御技术打下基础。

## 章节目标

学完本章后，你将能够：

- 理解白盒攻击的基本原理和攻击者需要掌握的信息
- 掌握FGSM算法的核心思想，并能解释其工作机制
- 理解PGD算法如何通过迭代优化提升攻击成功率
- 能够分析不同攻击算法的优缺点和适用场景
- 认识到白盒攻击对AI系统安全性的实际威胁

## 1. 白盒攻击的基本概念

### 1.1 什么是白盒攻击

在网络安全领域，我们经常听到"白盒测试"和"黑盒测试"这两个术语。这个分类方法同样适用于对抗样本攻击。**白盒攻击（White-box Attack）**是指攻击者完全了解目标模型内部结构的攻击方式。

让我们用一个生活化的类比来理解这个概念。假设你要破解一个保险箱，白盒攻击就像是你不仅知道保险箱的品牌型号，还拿到了它的设计图纸、内部结构说明书，甚至知道锁芯的工作原理。有了这些信息，你就能更精确地找到破解方法。

在AI安全的场景中，白盒攻击者通常掌握以下信息：

1. **模型架构**：知道模型有多少层、每层有多少个神经元、使用了什么激活函数
2. **模型参数**：能够访问模型的所有权重和偏置参数
3. **训练数据**：了解模型是用什么数据训练的
4. **损失函数**：知道模型训练时使用的优化目标

有同学可能会问：现实中攻击者怎么可能获得这么多信息？这是一个很好的问题。确实，在实际应用中，完全的白盒场景并不常见。但是，以下几种情况下白盒攻击是可能的：

- **开源模型**：许多研究机构和公司会公开发布模型架构和参数，比如BERT、ResNet等
- **模型窃取**：攻击者通过大量查询，可以训练一个近似的替代模型
- **内部威胁**：公司内部人员可能直接访问模型
- **学术研究**：研究人员需要在白盒条件下测试模型的鲁棒性

### 1.2 白盒攻击的核心思想

白盒攻击的核心思想可以用一句话概括：**利用模型的梯度信息，找到让模型犯错的最优扰动方向**。

这里需要解释一下"梯度"这个概念。在机器学习中，梯度表示的是函数值变化最快的方向。我们可以把它想象成爬山时的坡度：梯度大的地方坡度陡，梯度小的地方坡度缓。

在模型训练时，我们使用梯度下降算法，沿着梯度的反方向调整参数，让模型的预测越来越准确。而白盒攻击恰恰相反：攻击者沿着梯度的方向修改输入，让模型的预测越来越错误。

这就像是在玩一个"找茬"游戏。训练模型时，我们努力让模型学会正确识别图片；而攻击时，我们故意在图片上做一些微小的改动，让模型"看走眼"。由于我们知道模型的内部结构，就能精确计算出应该在哪些像素上做改动，以及改动多少。

### 1.3 白盒攻击的优势与局限

白盒攻击的主要优势在于：

1. **攻击效率高**：由于能够直接计算梯度，攻击者可以快速生成对抗样本
2. **成功率高**：利用完整的模型信息，攻击更加精准
3. **扰动更小**：可以优化扰动的大小，使对抗样本更难被察觉

但白盒攻击也有明显的局限性：

1. **信息要求高**：需要完全了解目标模型，这在实际场景中很难满足
2. **迁移性问题**：针对特定模型生成的对抗样本，可能对其他模型无效
3. **防御相对容易**：由于攻击原理清晰，防御者可以针对性地设计防御措施

了解了白盒攻击的基本概念后，接下来我们将学习两种最经典的白盒攻击算法。这两种算法虽然提出时间较早，但至今仍然是研究和测试AI模型鲁棒性的重要工具。

## 2. 快速梯度符号法（FGSM）

### 2.1 FGSM的诞生背景

2014年，Google的研究员Ian Goodfellow等人发表了一篇开创性的论文《Explaining and Harnessing Adversarial Examples》，首次提出了快速梯度符号法（Fast Gradient Sign Method，FGSM）。这个算法的提出震惊了整个AI安全领域，因为它证明了即使是最先进的深度学习模型，也可以被简单的数学方法轻易欺骗。

Goodfellow的研究动机很有意思。当时，研究人员已经发现了对抗样本现象，但普遍认为生成对抗样本需要复杂的优化过程。Goodfellow想要证明：生成对抗样本其实非常简单，只需要一步计算就能完成。这个发现让人们意识到，对抗样本不是偶然现象，而是深度学习模型的固有特性。

### 2.2 FGSM的核心原理

FGSM的核心思想非常直观：**沿着损失函数梯度的方向，给输入添加一个固定大小的扰动**。

让我们用通俗的语言来理解这个过程。假设我们有一张猫的图片，模型正确地将它识别为"猫"。现在，我们想要欺骗模型，让它把这张图片误认为是"狗"。FGSM的做法是：

1. **计算梯度**：首先计算损失函数相对于输入图片的梯度。这个梯度告诉我们，改变图片的哪些像素会让模型的预测发生最大变化。

2. **提取符号**：我们不关心梯度的具体数值，只关心它的方向（正或负）。这就是"符号"的含义。

3. **添加扰动**：在梯度为正的地方增加像素值，在梯度为负的地方减少像素值，扰动的大小由一个参数ε（epsilon）控制。

用数学语言描述，FGSM生成对抗样本的公式是：

**对抗样本 = 原始图片 + ε × sign(梯度)**

这里的sign函数会把梯度转换为+1或-1，表示增加或减少的方向。

有同学可能会问：为什么只用符号，不用梯度的实际大小？这是FGSM的巧妙之处。使用符号有两个好处：一是计算非常快，只需要一步就能完成；二是能够保证扰动的大小是可控的，不会让图片变化太大而被人眼察觉。

### 2.3 FGSM的实际效果

让我们通过一个具体例子来看FGSM的威力。假设我们有一个在ImageNet数据集上训练的图像分类模型，它能够识别1000种不同的物体。

**原始图片**：一张熊猫的照片，模型以99.8%的置信度预测为"熊猫"

**攻击过程**：
1. 设置扰动强度ε = 0.007（这意味着每个像素最多改变0.7%）
2. 计算损失函数相对于输入图片的梯度
3. 使用FGSM公式生成对抗样本

**攻击结果**：对抗样本在人眼看来仍然是熊猫，但模型以99.3%的置信度预测为"长臂猿"

这个例子展示了FGSM的两个重要特点：
- **扰动微小**：只改变了0.7%的像素值，人眼几乎无法察觉
- **攻击有效**：模型的预测完全改变，而且置信度很高

### 2.4 FGSM的变体

FGSM提出后，研究人员开发了多个改进版本：

**目标攻击版本**：原始的FGSM是无目标攻击，只要让模型预测错误即可。目标攻击版本可以指定让模型预测为某个特定类别。例如，不仅要让模型把熊猫识别错，还要让它识别为"长臂猿"。

**迭代版本（I-FGSM）**：不是一次性添加大的扰动，而是多次添加小的扰动。每次添加后，都重新计算梯度，然后继续添加。这种方法的攻击成功率更高，但计算时间也更长。

**随机启动版本**：在开始攻击前，先给图片添加一个小的随机噪声，然后再执行FGSM。这样可以避免陷入局部最优解，提高攻击成功率。

FGSM虽然简单，但它揭示了深度学习模型的一个根本性弱点：模型对输入的微小变化过于敏感。这个发现促使研究人员开始思考如何提高模型的鲁棒性。接下来，我们将学习一种更强大的攻击方法：投影梯度下降法。

## 3. 投影梯度下降法（PGD）

### 3.1 PGD的改进思路

虽然FGSM简单高效，但它有一个明显的缺点：只执行一步攻击，可能无法找到最优的对抗样本。就像爬山时，如果只走一步，可能无法到达山顶。

2017年，Aleksander Madry等人提出了投影梯度下降法（Projected Gradient Descent，PGD）。PGD可以看作是FGSM的迭代增强版本，它通过多次迭代来寻找更强的对抗样本。

让我们用一个类比来理解PGD和FGSM的区别。假设你要在一个黑暗的房间里找到出口：

- **FGSM**：打开手电筒看一眼，然后朝着看起来最有希望的方向走一大步
- **PGD**：打开手电筒，朝着有希望的方向走一小步，然后再次打开手电筒观察，调整方向，再走一小步，如此反复

PGD的方法虽然需要更多步骤，但能够找到更好的路径，生成更强的对抗样本。

### 3.2 PGD的工作流程

PGD的攻击过程可以分为以下几个步骤：

**第一步：随机初始化**
不是从原始图片开始，而是在原始图片附近随机选择一个起点。这就像在山脚下随机选择一个位置开始爬山，可以避免陷入局部最优。

**第二步：迭代攻击**
重复执行以下操作多次（通常10-100次）：
1. 计算当前图片的梯度
2. 沿着梯度方向移动一小步（步长α，通常比FGSM的ε小得多）
3. 检查是否超出允许的扰动范围

**第三步：投影操作**
这是PGD名称中"投影"的含义。每次移动后，我们需要确保对抗样本仍然在允许的范围内。如果超出了范围，就把它"投影"回来。

举个例子，假设我们规定扰动不能超过ε=0.03。如果某个像素的扰动达到了0.05，我们就把它限制回0.03。这就像是在一个围栏内活动，如果走出了围栏，就把自己拉回围栏边界。

**第四步：输出对抗样本**
经过多次迭代后，输出最终的对抗样本。

### 3.3 PGD的数学直觉

虽然我们尽量避免复杂的数学公式，但理解PGD的数学直觉对掌握这个算法很有帮助。

PGD本质上是在解决一个优化问题：在扰动大小受限的条件下，找到让模型预测错误程度最大的扰动。

我们可以把这个问题想象成在一个有限的区域内寻宝。这个区域的边界由扰动大小ε决定，宝藏的位置由模型的弱点决定。PGD就是一个智能的寻宝算法：

1. **随机起点**：在区域内随机选择一个起点，避免遗漏宝藏
2. **梯度指引**：利用梯度信息，朝着宝藏的方向前进
3. **边界约束**：如果走出了区域，就退回到边界上
4. **多次尝试**：通过多次迭代，逐步接近宝藏

这个过程保证了两点：一是生成的对抗样本足够强（找到了模型的弱点），二是扰动不会太大（始终在允许的区域内）。

### 3.4 PGD与FGSM的对比

让我们通过一个对比表格来总结PGD和FGSM的区别：

| 特性 | FGSM | PGD |
|------|------|-----|
| 迭代次数 | 1次 | 多次（10-100次） |
| 计算时间 | 快（毫秒级） | 慢（秒级） |
| 攻击成功率 | 较高 | 很高 |
| 扰动优化程度 | 低 | 高 |
| 初始化方式 | 原始图片 | 随机扰动 |
| 适用场景 | 快速测试 | 严格评估 |

有同学可能会问：既然PGD更强，为什么还要使用FGSM？这是因为在实际应用中，我们需要在攻击强度和计算成本之间做权衡。FGSM适合快速测试模型的基本鲁棒性，而PGD适合进行更严格的安全评估。

### 3.5 PGD在实际中的应用

PGD不仅是一种攻击方法，也是一种重要的防御工具。在对抗训练（Adversarial Training）中，研究人员使用PGD生成对抗样本，然后用这些样本来训练模型，提高模型的鲁棒性。

**案例：ImageNet鲁棒性测试**

2018年，研究人员使用PGD对ImageNet上的多个顶级模型进行了鲁棒性测试。结果令人震惊：

- **ResNet-50**：在干净图片上准确率76%，在PGD攻击下准确率降至0%
- **Inception-v3**：在干净图片上准确率78%，在PGD攻击下准确率降至0%
- **DenseNet-121**：在干净图片上准确率75%，在PGD攻击下准确率降至0%

这个案例说明，即使是在大规模数据集上训练的先进模型，在面对精心设计的对抗攻击时也非常脆弱。这促使研究人员开始重视模型的鲁棒性问题，而不仅仅关注准确率。

了解了FGSM和PGD这两种经典的白盒攻击算法后，我们对对抗样本的生成机制有了更深入的理解。接下来，我们将探讨白盒攻击在实际应用中面临的挑战和局限性。

## 4. 白盒攻击的实战考虑

### 4.1 攻击强度的选择

在实际应用中，攻击者需要在攻击成功率和隐蔽性之间做权衡。扰动越大，攻击成功率越高，但也越容易被察觉。

**扰动大小的影响**：

- **ε = 0.001**：扰动几乎不可见，但攻击成功率较低（约30%）
- **ε = 0.01**：扰动在仔细观察下可能被发现，攻击成功率中等（约70%）
- **ε = 0.03**：扰动在某些情况下可见，但攻击成功率很高（约95%）
- **ε = 0.1**：扰动明显可见，攻击成功率接近100%

在实际攻击场景中，攻击者通常会选择ε = 0.01到0.03之间的值，这样既能保证较高的成功率，又不容易被人眼察觉。

### 4.2 不同模型架构的鲁棒性差异

研究发现，不同的模型架构对白盒攻击的抵抗能力有所不同：

**卷积神经网络（CNN）**：传统的CNN模型（如ResNet、VGG）对白盒攻击较为脆弱，主要原因是它们的决策边界不够平滑。

**Vision Transformer（ViT）**：基于注意力机制的模型表现出更好的鲁棒性，但仍然可以被攻击。研究表明，ViT需要更大的扰动才能被成功攻击。

**对抗训练模型**：专门使用对抗样本训练的模型具有更强的鲁棒性，但代价是在干净数据上的准确率会下降2-5个百分点。

### 4.3 白盒攻击的实际威胁

有同学可能会想：白盒攻击需要完全了解模型，这在现实中很难实现，那它的实际威胁有多大？

实际上，白盒攻击的威胁不容小觑：

**开源模型的风险**：许多公司和研究机构会公开发布模型，如BERT、GPT等。攻击者可以直接对这些模型进行白盒攻击。

**模型窃取攻击**：即使模型不公开，攻击者也可以通过大量查询API，训练一个近似的替代模型，然后对替代模型进行白盒攻击。研究表明，这种攻击生成的对抗样本有很高的迁移性。

**供应链攻击**：在模型开发和部署的过程中，内部人员可能接触到模型的完整信息，从而进行白盒攻击。

**案例：特斯拉自动驾驶系统**

2019年，腾讯科恩实验室的研究人员对特斯拉的自动驾驶系统进行了安全测试。他们使用白盒攻击方法，在道路上放置了经过精心设计的贴纸。这些贴纸对人眼来说只是普通的图案，但能够欺骗特斯拉的车道识别系统，导致车辆偏离车道。

虽然研究人员没有完全掌握特斯拉模型的内部结构，但他们通过分析类似的开源模型，成功生成了有效的对抗样本。这个案例说明，即使在不完全白盒的情况下，攻击者仍然可以利用白盒攻击的原理来威胁实际系统的安全。

### 4.4 白盒攻击的防御挑战

防御白盒攻击是一个极具挑战性的问题。目前主要的防御方法包括：

**对抗训练**：在训练过程中加入对抗样本，让模型学会抵抗攻击。这是目前最有效的防御方法，但会增加训练成本，并可能降低模型在干净数据上的性能。

**输入预处理**：在输入进入模型前，先进行降噪、压缩等处理，试图去除对抗扰动。但这种方法容易被自适应攻击破解。

**模型集成**：使用多个不同的模型进行预测，然后综合它们的结果。这可以提高鲁棒性，但计算成本较高。

**认证防御**：通过数学证明的方式，保证在一定扰动范围内模型的预测不会改变。这是最可靠的防御方法，但目前只能应用于小规模模型。

需要注意的是，攻防是一个持续演化的过程。每当出现新的防御方法，攻击者就会设计新的攻击来破解它。这种"猫鼠游戏"推动了AI安全领域的不断发展。

## 本章小结

本章我们深入学习了白盒攻击的原理和方法。让我们回顾一下关键要点：

**白盒攻击的本质**：利用模型的梯度信息，找到让模型犯错的最优扰动方向。攻击者需要完全了解模型的内部结构和参数。

**FGSM算法**：最简单高效的白盒攻击方法，只需一步计算就能生成对抗样本。它的核心思想是沿着损失函数梯度的符号方向添加固定大小的扰动。

**PGD算法**：FGSM的迭代增强版本，通过多次迭代和投影操作，生成更强的对抗样本。PGD是目前评估模型鲁棒性的黄金标准。

**实战考虑**：在实际应用中，需要在攻击成功率和隐蔽性之间做权衡。不同的模型架构对白盒攻击的抵抗能力有所不同。

**防御挑战**：防御白盒攻击是一个持续演化的过程，目前没有完美的解决方案。对抗训练是最有效的防御方法，但会带来额外的成本。

通过本章的学习，我们不仅掌握了白盒攻击的技术细节，更重要的是理解了AI模型的脆弱性来源。这些知识将帮助我们在后续章节中更好地理解黑盒攻击和防御技术。

## 教学资源

**图表与示意图**：
- 图2-1：白盒攻击与黑盒攻击的信息对比图
- 图2-2：FGSM算法的工作流程示意图
- 图2-3：PGD算法的迭代过程可视化
- 图2-4：不同扰动强度下的对抗样本对比
- 图2-5：FGSM与PGD攻击成功率对比曲线

**配套实验**：
- 实验3.1：FGSM白盒攻击实验
  - 使用FGSM对ResNet-18模型进行攻击
  - 观察不同扰动强度下的攻击效果
  - 可视化对抗样本和扰动
- 实验3.2：PGD迭代攻击实验
  - 实现PGD算法并对比FGSM的效果
  - 分析迭代次数对攻击成功率的影响
  - 观察投影操作的作用

**延伸阅读**：
- Goodfellow et al., "Explaining and Harnessing Adversarial Examples", ICLR 2015
- Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks", ICLR 2018
- Carlini & Wagner, "Towards Evaluating the Robustness of Neural Networks", IEEE S&P 2017

## 课后思考题

1. **理解性问题**：为什么FGSM只使用梯度的符号，而不使用梯度的实际大小？这样做有什么好处和局限性？

2. **分析性问题**：假设你是一个AI系统的防御者，面对白盒攻击威胁，你会采取哪些措施来保护模型？请分析每种措施的优缺点。

3. **应用性问题**：在自动驾驶场景中，攻击者可能使用白盒攻击来欺骗车辆的视觉系统。请设计一个实验方案，测试自动驾驶模型对白盒攻击的鲁棒性，并提出可能的防御策略。