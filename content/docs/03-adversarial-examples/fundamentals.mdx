---
title: 第1章：对抗样本基础原理
description: 理解对抗样本的概念、有效性原因和真实世界威胁
---


import { Callout } from 'fumadocs-ui/components/callout';
import { Steps, Step } from 'fumadocs-ui/components/steps';
import { Tabs, Tab } from 'fumadocs-ui/components/tabs';
import { Accordion, Accordions } from 'fumadocs-ui/components/accordion';


<Callout title="" type="info">
预计阅读约20分钟
</Callout>

## 本章导读

在开始学习对抗样本之前，我们先思考一个问题：如果在一张熊猫的照片上添加一些人眼几乎看不见的噪点，能否让AI模型将它误认为是长臂猿？答案是肯定的，而且这种攻击方法已经在真实世界中被验证。这种通过精心设计的微小扰动来欺骗AI模型的技术，就是我们本章要学习的对抗样本攻击。

对抗样本的存在揭示了AI模型的一个根本性脆弱点：它们对输入数据的微小变化异常敏感。这种脆弱性不仅存在于图像识别系统中，也广泛存在于语音识别、自然语言处理等各类AI应用中。理解对抗样本的原理，是我们学习AI安全的重要一步。

本章将带领大家从直觉理解开始，逐步深入对抗样本的工作机制，并通过真实案例了解其在实际场景中的危害。学习本章后，你将能够理解为什么AI模型会被这种看似简单的攻击所欺骗，以及这种攻击在现实世界中可能带来的安全风险。

## 章节目标

学完本章后，你应该能够：

- 理解对抗样本的基本概念，能够用自己的话解释什么是对抗样本
- 掌握对抗样本有效的核心原因，理解AI模型为什么会被微小扰动欺骗
- 认识对抗样本在真实世界中的危害，能够分析具体场景中的安全风险
- 建立对AI模型脆弱性的整体认知，为后续学习攻击和防御技术打下基础

## 1. 什么是对抗样本

### 1.1 从一个神奇的现象说起

让我们从一个令人惊讶的实验开始。2014年，研究人员发现了一个奇怪的现象：他们拍摄了一张熊猫的照片，这张照片被图像识别模型正确识别为"熊猫"，置信度高达57.7%。然后，他们在这张照片上添加了一些精心计算的噪点。这些噪点非常微小，人眼几乎无法察觉，但神奇的事情发生了——添加噪点后的图片被模型识别为"长臂猿"，置信度竟然达到了99.3%。

这个实验揭示了一个重要事实：AI模型的"视觉"与人类的视觉存在本质差异。对于人类来说，添加噪点前后的两张图片看起来几乎一模一样，都是熊猫。但对于AI模型来说，这两张图片却有着天壤之别。这种经过精心设计、能够欺骗AI模型的输入数据，就是我们所说的对抗样本（Adversarial Examples）。

### 1.2 对抗样本的核心特征

在深入理解对抗样本之前，我们需要明确它的几个关键特征：

**人类不可感知性**：对抗样本与原始样本在人类看来几乎没有区别。这是对抗样本最重要的特征之一。如果扰动过大，人类也能察觉异常，那么这种攻击就失去了隐蔽性。研究人员通常会严格控制扰动的大小，确保人眼无法分辨。

**模型误判性**：虽然人类无法察觉差异，但AI模型会对对抗样本产生完全错误的判断。更重要的是，模型往往会以很高的置信度给出错误答案，这意味着模型"非常确信"自己的判断是正确的。

**针对性设计**：对抗样本不是随机产生的噪声，而是经过精心计算的扰动。攻击者需要了解模型的工作机制，才能设计出有效的对抗样本。这种扰动看似随机，实际上包含了大量针对特定模型的信息。

### 1.3 用生活化的例子理解对抗样本

为了更好地理解对抗样本，我们可以用一个生活中的类比。想象你在参加一个视力测试，测试表上有一行字母"E"。现在，有人在这个"E"上用非常浅的颜色画了几笔，这几笔对你来说几乎看不见，你仍然能够准确地认出这是字母"E"。但是，如果有一台机器在读取这个字母，而这几笔恰好干扰了机器的识别算法，机器可能会把它误认为是字母"F"或其他字母。

这个类比虽然简化了问题，但它抓住了对抗样本的核心特点：微小的、人类难以察觉的变化，却能够对机器的判断产生巨大影响。这种现象的根源在于，人类和机器处理信息的方式存在本质差异。

### 1.4 学生可能的疑问

在这里，有些同学可能会产生疑问：既然AI模型这么容易被欺骗，为什么我们还要使用它们？这是一个很好的问题。事实上，AI模型在正常情况下的表现非常出色，它们能够处理海量数据，完成人类难以完成的任务。对抗样本是一种特殊的、精心设计的攻击方式，在日常应用中并不常见。但是，在安全敏感的场景中（如自动驾驶、人脸识别门禁等），我们必须认真对待这种威胁。

另一个常见的疑问是：对抗样本是否只存在于图像识别中？答案是否定的。对抗样本的概念适用于几乎所有类型的AI模型，包括语音识别、自然语言处理、恶意软件检测等。只要是基于机器学习的系统，都可能存在对抗样本的威胁。

## 2. 对抗样本为什么有效

### 2.1 AI模型的工作方式

要理解对抗样本为什么有效，我们首先需要了解AI模型是如何工作的。以图像识别为例，当我们给模型输入一张图片时，模型会将这张图片转换为一系列数字（像素值），然后通过多层神经网络进行处理，最终输出一个分类结果。

在这个过程中，模型学习的是输入数据与输出标签之间的统计规律。例如，模型可能学到"如果图片中有黑白相间的圆形区域，并且有竹子的纹理，那么这很可能是熊猫"。但是，模型并不真正"理解"什么是熊猫，它只是在高维空间中找到了一个能够区分不同类别的决策边界。

### 2.2 决策边界的脆弱性

让我们用一个简化的例子来理解决策边界的概念。假设我们要训练一个模型来区分猫和狗，模型会在特征空间中画一条线（或者更复杂的曲面），线的一边是猫，另一边是狗。理想情况下，所有的猫都应该在线的一边，所有的狗都应该在线的另一边。

但问题在于，这条决策边界往往离某些样本非常近。如果我们能够找到一个靠近边界的样本，然后给它添加一个精心设计的微小扰动，就可能把它"推"到边界的另一边，从而导致模型误判。这就是对抗样本攻击的基本原理。

### 2.3 高维空间的反直觉特性

这里需要理解一个重要的数学概念：高维空间的特性与我们在三维空间中的直觉经验大不相同。一张普通的图片可能包含数十万个像素，每个像素都是一个维度。在这样的高维空间中，即使每个维度上的扰动都非常小（人眼无法察觉），但当这些微小扰动累加起来时，在高维空间中的总距离可能会变得很大，足以跨越决策边界。

我们可以用一个简单的数学例子来说明这一点。假设一张图片有10000个像素，我们在每个像素上添加0.01的扰动。对于单个像素来说，0.01的变化几乎不可见。但是，如果我们计算所有像素扰动的总和，就会发现总的扰动量达到了100（10000 × 0.01 = 100）。这个累积效应在高维空间中非常显著，这也是为什么微小的扰动能够产生巨大影响的原因。

### 2.4 模型的线性特性

深度学习模型虽然被称为"非线性"模型，但在局部范围内，它们的行为往往接近线性。这意味着，如果我们沿着某个特定方向对输入进行微小改变，模型的输出会以一个相对稳定的速率变化。攻击者可以利用这个特性，计算出一个最优的扰动方向，使得模型的输出朝着错误的方向移动。

这种线性特性可以用一个简单的类比来理解：想象你在一个斜坡上推一个球，如果你知道斜坡的倾斜方向，你就可以用很小的力量让球沿着斜坡滚下去。对抗样本攻击就是在寻找模型"决策空间"中的这个"斜坡方向"，然后沿着这个方向施加微小的扰动。

### 2.5 学生可能的疑问

有同学可能会问：既然模型这么容易被欺骗，为什么不能通过改进训练方法来彻底消除这个问题？这是一个非常深刻的问题。事实上，研究人员已经尝试了许多方法来提高模型的鲁棒性，但到目前为止，还没有找到一个完美的解决方案。对抗样本的存在似乎是深度学习模型的一个固有特性，而不是某个具体算法的缺陷。

另一个常见的疑问是：对抗样本是否只对特定的模型有效？答案是，虽然对抗样本通常是针对特定模型设计的，但研究发现，对抗样本具有一定的"可迁移性"。也就是说，针对模型A生成的对抗样本，往往也能欺骗模型B，即使这两个模型的结构完全不同。这种可迁移性使得对抗样本攻击变得更加危险，因为攻击者不需要完全了解目标模型的内部结构。

## 3. 真实世界中的对抗样本攻击

### 3.1 自动驾驶系统的安全威胁

对抗样本攻击不仅仅是实验室中的理论问题，它在真实世界中已经造成了实际威胁。最引人关注的案例之一是针对自动驾驶系统的攻击。

2018年，研究人员展示了一种针对交通标志识别系统的攻击方法。他们在停车标志（STOP sign）上贴了几张看似普通的贴纸，这些贴纸的颜色和图案经过精心设计。对于人类驾驶员来说，这仍然是一个清晰的停车标志。但是，自动驾驶系统的摄像头却将它识别为"限速45英里"的标志。

这种攻击的危险性在于，它可以在物理世界中实施，而不仅仅是在数字图像上。攻击者只需要在真实的交通标志上做一些看似无害的涂鸦或贴纸，就可能导致自动驾驶汽车做出错误的决策。想象一下，如果一辆自动驾驶汽车在应该停车的路口没有停车，可能会造成严重的交通事故。

### 3.2 人脸识别系统的绕过

人脸识别技术已经广泛应用于门禁系统、支付验证、安全监控等场景。然而，研究人员发现，通过佩戴特殊设计的眼镜或帽子，攻击者可以欺骗人脸识别系统。

2016年，卡内基梅隆大学的研究人员展示了一种"对抗眼镜"。这种眼镜看起来与普通眼镜没有太大区别，但镜框上的图案经过精心设计。佩戴这种眼镜后，攻击者可以让人脸识别系统将自己识别为另一个人，甚至可以让系统完全无法识别自己的身份。

这种攻击在安全领域引起了广泛关注。如果攻击者能够绕过人脸识别门禁系统，就可能进入受限区域；如果能够冒充他人身份，就可能进行欺诈活动。更令人担忧的是，这种攻击方法相对简单，不需要高深的技术知识，普通人也可能实施。

### 3.3 语音识别系统的隐藏命令

对抗样本攻击不仅限于视觉系统，语音识别系统同样面临威胁。研究人员发现，可以在音频中嵌入人类听不到但机器能够识别的命令。

2017年，加州大学伯克利分校的研究人员展示了一种"海豚攻击"（DolphinAttack）。他们使用超声波频率（人类听不到的高频声音）向智能音箱发送命令。对于房间里的人来说，一切都很安静，但智能音箱却"听到"了命令，并执行了相应的操作，比如打开门锁、拨打电话、进行网上购物等。

另一个更隐蔽的攻击方法是在正常的音频中嵌入对抗扰动。例如，研究人员可以在一段音乐或播客中嵌入隐藏的命令。人类听众只会听到正常的音乐，但语音助手却会"听到"并执行隐藏的命令。这种攻击可以通过广播、视频网站等渠道传播，影响范围非常广。

### 3.4 恶意软件检测系统的绕过

在网络安全领域，许多杀毒软件和恶意软件检测系统使用机器学习技术来识别威胁。然而，这些系统同样容易受到对抗样本攻击。

攻击者可以对恶意软件进行微小的修改，使其能够绕过检测系统。这些修改可能包括添加无用的代码、改变代码的排列顺序、修改文件的元数据等。这些改变不会影响恶意软件的功能，但可以让检测系统将其误判为正常文件。

2019年的一项研究表明，通过对恶意软件进行对抗性修改，可以使检测率从99%降低到10%以下。这意味着，原本能够被有效拦截的恶意软件，经过对抗性修改后，有90%的概率能够绕过检测系统。这对网络安全构成了严重威胁。

### 3.5 案例总结与启示

通过以上案例，我们可以看到对抗样本攻击的几个共同特点：

**普遍性**：对抗样本攻击不仅存在于图像识别中，还广泛存在于语音识别、自然语言处理、恶意软件检测等各个领域。只要是基于机器学习的系统，都可能面临这种威胁。

**隐蔽性**：对抗样本攻击往往难以被人类察觉。无论是交通标志上的贴纸、特殊设计的眼镜，还是音频中的隐藏命令，它们在人类看来都很正常，但却能够欺骗AI系统。

**实用性**：对抗样本攻击不仅仅是理论上的威胁，它们可以在真实世界中实施。攻击者不需要直接访问模型的内部参数，只需要能够与系统交互，就可能发起攻击。

**危害性**：在安全敏感的应用场景中，对抗样本攻击可能造成严重后果。自动驾驶系统的误判可能导致交通事故，人脸识别系统的绕过可能导致安全漏洞，恶意软件检测系统的失效可能导致网络攻击。

这些案例提醒我们，在部署AI系统时，必须充分考虑对抗样本攻击的风险，并采取相应的防御措施。在后续章节中，我们将学习具体的攻击技术和防御方法。

## 4. 对抗样本的分类与特点

### 4.1 按攻击目标分类

对抗样本攻击可以根据攻击者的目标分为两大类：

**无目标攻击（Untargeted Attack）**：攻击者的目标是让模型产生任何错误的输出，而不关心具体的错误类型。例如，将一张猫的图片误判为任何非猫的类别（狗、鸟、汽车等）都算攻击成功。这种攻击相对容易实现，因为攻击者只需要让样本跨越决策边界即可。

**有目标攻击（Targeted Attack）**：攻击者的目标是让模型输出特定的错误结果。例如，将一张猫的图片误判为"狗"，而不是其他类别。这种攻击更加困难，因为攻击者不仅要让样本跨越决策边界，还要让它落在特定的目标类别区域内。但是，有目标攻击在某些场景中更加危险，比如让人脸识别系统将攻击者识别为特定的目标人物。

### 4.2 按攻击者知识分类

根据攻击者对目标模型的了解程度，对抗样本攻击可以分为：

**白盒攻击（White-box Attack）**：攻击者完全了解目标模型的结构、参数和训练数据。在这种情况下，攻击者可以直接计算最优的扰动方向，生成高效的对抗样本。白盒攻击通常用于研究和测试模型的鲁棒性。

**黑盒攻击（Black-box Attack）**：攻击者不了解目标模型的内部细节，只能通过输入输出来与模型交互。这种情况更接近真实世界的攻击场景。黑盒攻击通常利用对抗样本的可迁移性，或者通过大量查询来估计模型的行为。

**灰盒攻击（Gray-box Attack）**：攻击者部分了解目标模型的信息，比如知道模型的结构但不知道具体参数，或者知道训练数据的分布但不知道模型结构。这是介于白盒和黑盒之间的一种情况。

### 4.3 按实施方式分类

根据对抗样本的实施方式，可以分为：

**数字攻击（Digital Attack）**：对抗样本只存在于数字形式中，攻击者直接修改数字图像、音频或文本文件。这种攻击在网络环境中比较常见，比如上传恶意图片到社交媒体、发送对抗性文本到聊天机器人等。

**物理攻击（Physical Attack）**：对抗样本存在于物理世界中，攻击者需要在真实物体上进行修改。例如，在交通标志上贴贴纸、佩戴对抗眼镜、播放对抗音频等。物理攻击更加困难，因为需要考虑光照变化、视角变化、环境噪声等因素，但一旦成功，危害性也更大。

### 4.4 学生可能的疑问

有同学可能会问：既然白盒攻击需要完全了解模型，那么在实际应用中是否很难实现？这个问题很有见地。确实，在大多数商业应用中，模型的内部细节是保密的，攻击者很难获得完整信息。但是，研究表明，即使在黑盒条件下，攻击者仍然可以通过多种方法生成有效的对抗样本。而且，许多开源模型的结构是公开的，攻击者可以利用这些信息进行攻击。

另一个常见的疑问是：物理攻击是否真的可行？毕竟，从数字图像到物理世界，中间经过了摄像头拍摄、光照变化等多个环节，对抗扰动是否还能保持有效？研究表明，虽然物理攻击确实更加困难，但通过特殊的设计方法（如考虑多种视角和光照条件），可以生成在物理世界中仍然有效的对抗样本。这也是为什么物理对抗样本攻击引起了广泛关注的原因。

## 本章小结

在本章中，我们从直觉理解开始，逐步深入学习了对抗样本的基础原理。让我们回顾一下本章的核心要点：

**对抗样本的本质**：对抗样本是经过精心设计的输入数据，它们在人类看来与正常样本几乎没有区别，但能够导致AI模型产生错误的输出。这种攻击揭示了AI模型的一个根本性脆弱点——对输入数据的微小变化异常敏感。

**有效性的原因**：对抗样本之所以有效，主要有三个原因：一是AI模型的决策边界往往离某些样本很近，微小的扰动就可能导致误判；二是高维空间的反直觉特性，使得微小的扰动累积起来可以产生显著影响；三是模型的局部线性特性，使得攻击者可以计算出最优的扰动方向。

**真实世界的威胁**：对抗样本攻击不仅存在于理论研究中，在真实世界中也已经造成了实际威胁。从自动驾驶系统到人脸识别，从语音助手到恶意软件检测，各类AI应用都可能面临对抗样本攻击的风险。

**攻击的多样性**：对抗样本攻击有多种形式，可以根据攻击目标、攻击者知识、实施方式等进行分类。理解这些分类有助于我们更全面地认识对抗样本的威胁，并为后续学习具体的攻击和防御技术打下基础。

通过本章的学习，我们建立了对对抗样本的整体认知。在接下来的章节中，我们将深入学习具体的攻击技术，包括白盒攻击、黑盒攻击、文本对抗攻击等，并最终学习如何防御这些攻击。理解对抗样本的原理是学习AI安全的重要一步，它提醒我们在享受AI技术带来的便利的同时，也要时刻警惕其潜在的安全风险。

## 教学资源

**图表与示意图**：
- 图1：对抗样本生成过程示意图（原始图像 → 添加扰动 → 对抗样本）
- 图2：决策边界与对抗样本的关系（二维空间中的可视化）
- 图3：高维空间中的扰动累积效应示意图
- 图4：真实世界对抗样本攻击案例图集（交通标志、对抗眼镜等）

**配套实验**：
- 实验3.1：FGSM白盒攻击（体验生成对抗样本的基本方法）
- 实验3.2：PGD迭代攻击（学习更强大的攻击技术）

**延伸阅读**：
- Goodfellow et al. (2014). "Explaining and Harnessing Adversarial Examples" - 对抗样本研究的开创性论文
- Eykholt et al. (2018). "Robust Physical-World Attacks on Deep Learning Visual Classification" - 物理世界对抗样本攻击的经典研究
- 《AI安全：对抗样本攻击与防御》（推荐阅读第1-2章）

**视频资源**：
- "对抗样本：AI的视觉盲点"（15分钟科普视频）
- "自动驾驶系统的对抗样本攻击演示"（实验视频）

## 课后思考题

1. **理解性问题**：请用自己的话解释什么是对抗样本，以及为什么人类无法察觉对抗扰动，但AI模型却会被欺骗？

2. **分析性问题**：假设你是一个自动驾驶系统的安全工程师，你认为对抗样本攻击对自动驾驶系统的哪些功能模块威胁最大？为什么？

3. **应用性问题**：在日常生活中，你能想到哪些场景可能受到对抗样本攻击的威胁？如果你是这些系统的设计者，你会采取哪些初步的防护措施？（提示：可以从输入验证、异常检测等角度思考）