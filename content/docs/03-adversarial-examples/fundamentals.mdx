---
title: 第1章：对抗样本基础原理
description: 理解对抗样本的概念、有效性原因和真实世界威胁
---


import { Callout } from 'fumadocs-ui/components/callout';
import { Steps, Step } from 'fumadocs-ui/components/steps';
import { Tabs, Tab } from 'fumadocs-ui/components/tabs';
import { Accordion, Accordions } from 'fumadocs-ui/components/accordion';


<Callout title="预计学习时间" type="info">
预计阅读约20分钟
</Callout>

## 本章导读

在开始学习对抗样本之前，我们先思考一个问题：如果在一张熊猫的照片上添加一些人眼几乎看不见的噪点，能否让 AI 模型将它误认为是长臂猿？答案是肯定的，而且这种攻击方法已经在真实世界中被验证。这种通过精心设计的微小扰动来欺骗 AI 模型的技术，就是我们本章要学习的**对抗样本攻击**。

## 学习目标

<Callout title="本章学完后，你将能够：" type="info">
1. **理解对抗样本的基本概念**：能够用自己的话解释什么是对抗样本
2. **掌握对抗样本有效的核心原因**：理解 AI 模型为什么会被微小扰动欺骗
3. **认识真实世界中的危害**：能够分析具体场景中的安全风险
4. **建立整体认知**：为后续学习攻击和防御技术打下基础
</Callout>

## 1. 什么是对抗样本

### 1.1 从一个神奇的现象说起

让我们从一个令人惊讶的实验开始。2014年，研究人员发现了一个奇怪的现象：

<Steps>
  <Step>
    **原始图片**：一张熊猫的照片被图像识别模型正确识别为"熊猫"，置信度高达 **57.7%**
  </Step>
  <Step>
    **添加扰动**：在这张照片上添加了一些精心计算的噪点，这些噪点非常微小，人眼几乎无法察觉
  </Step>
  <Step>
    **神奇结果**：添加噪点后的图片被模型识别为"长臂猿"，置信度竟然达到了 **99.3%**
  </Step>
</Steps>

这个实验揭示了一个重要事实：**AI 模型的"视觉"与人类的视觉存在本质差异**。这种经过精心设计、能够欺骗 AI 模型的输入数据，就是我们所说的**对抗样本（Adversarial Examples）**。

### 1.2 对抗样本的核心特征

<Callout title="对抗样本的三大特征" type="info">
- **人类不可感知性**：对抗样本与原始样本在人类看来几乎没有区别
- **模型误判性**：AI 模型会以很高的置信度给出错误答案
- **针对性设计**：扰动是经过精心计算的，不是随机噪声
</Callout>

### 1.3 生活化的类比

为了更好地理解对抗样本，我们可以用一个生活中的类比：

> 想象你在参加一个视力测试，测试表上有一行字母"E"。现在，有人在这个"E"上用非常浅的颜色画了几笔，这几笔对你来说几乎看不见，你仍然能够准确地认出这是字母"E"。但是，如果有一台机器在读取这个字母，而这几笔恰好干扰了机器的识别算法，机器可能会把它误认为是字母"F"或其他字母。

这个类比虽然简化了问题，但它抓住了对抗样本的核心特点：微小的、人类难以察觉的变化，却能够对机器的判断产生巨大影响。

### 1.4 常见疑问

<Tabs items={['为什么还要使用 AI？', '只存在于图像识别中吗？']}>
  <Tab value="为什么还要使用 AI？">
    AI 模型在正常情况下的表现非常出色，它们能够处理海量数据，完成人类难以完成的任务。对抗样本是一种特殊的、精心设计的攻击方式，在日常应用中并不常见。但是，在安全敏感的场景中（如自动驾驶、人脸识别门禁等），我们必须认真对待这种威胁。
  </Tab>
  <Tab value="只存在于图像识别中吗？">
    答案是**否定的**。对抗样本的概念适用于几乎所有类型的 AI 模型，包括语音识别、自然语言处理、恶意软件检测等。只要是基于机器学习的系统，都可能存在对抗样本的威胁。
  </Tab>
</Tabs>

## 2. 对抗样本为什么有效

### 2.1 AI 模型的工作方式

要理解对抗样本为什么有效，我们首先需要了解 AI 模型是如何工作的。

以图像识别为例，当我们给模型输入一张图片时：
1. 模型将图片转换为一系列数字（像素值）
2. 通过多层神经网络进行处理
3. 最终输出一个分类结果

<Callout title="关键洞察" type="warn">
模型并不真正"理解"什么是熊猫，它只是在高维空间中找到了一个能够区分不同类别的**决策边界**。
</Callout>

### 2.2 决策边界的脆弱性

假设我们要训练一个模型来区分猫和狗：

| 概念 | 解释 |
|------|------|
| **决策边界** | 模型在特征空间中画的一条线（或曲面），线的一边是猫，另一边是狗 |
| **脆弱性来源** | 这条决策边界往往离某些样本非常近 |
| **攻击原理** | 找到靠近边界的样本，添加微小扰动将其"推"到边界的另一边 |

### 2.3 高维空间的反直觉特性

这里需要理解一个重要的数学概念：**高维空间的特性与我们在三维空间中的直觉经验大不相同**。

<Callout title="高维累积效应">
假设一张图片有 10,000 个像素，我们在每个像素上添加 0.01 的扰动：
- 对于单个像素：0.01 的变化几乎不可见
- 累积效应：总的扰动量达到 $10000 \times 0.01 = 100$

这个累积效应在高维空间中非常显著，这也是为什么微小的扰动能够产生巨大影响的原因。
</Callout>

### 2.4 模型的线性特性

深度学习模型虽然被称为"非线性"模型，但在局部范围内，它们的行为往往接近线性。

> **类比理解**：想象你在一个斜坡上推一个球，如果你知道斜坡的倾斜方向，你就可以用很小的力量让球沿着斜坡滚下去。对抗样本攻击就是在寻找模型"决策空间"中的这个"斜坡方向"，然后沿着这个方向施加微小的扰动。

## 3. 真实世界中的对抗样本攻击

### 3.1 自动驾驶系统的安全威胁

<Callout title="案例：交通标志攻击（2018）" type="error">
研究人员在停车标志（STOP sign）上贴了几张看似普通的贴纸：
- **人类视角**：这仍然是一个清晰的停车标志
- **AI 视角**：自动驾驶系统将它识别为"限速45英里"的标志

**危险性**：如果一辆自动驾驶汽车在应该停车的路口没有停车，可能会造成严重的交通事故。
</Callout>

### 3.2 人脸识别系统的绕过

2016年，卡内基梅隆大学的研究人员展示了一种"对抗眼镜"：

- 这种眼镜看起来与普通眼镜没有太大区别
- 镜框上的图案经过精心设计
- 佩戴后可以让人脸识别系统将攻击者识别为另一个人

<Callout title="安全影响" type="warn">
- 绕过人脸识别门禁系统，进入受限区域
- 冒充他人身份进行欺诈活动
- 逃避安全监控系统的追踪
</Callout>

### 3.3 语音识别系统的隐藏命令

<Tabs items={['海豚攻击', '嵌入式攻击']}>
  <Tab value="海豚攻击">
    **2017年，加州大学伯克利分校的研究**
    
    使用超声波频率（人类听不到的高频声音）向智能音箱发送命令：
    - 对于房间里的人：一切都很安静
    - 对于智能音箱："听到"了命令并执行操作（打开门锁、拨打电话等）
  </Tab>
  <Tab value="嵌入式攻击">
    **在正常音频中嵌入隐藏命令**
    
    在一段音乐或播客中嵌入隐藏的命令：
    - 人类听众只听到正常的音乐
    - 语音助手"听到"并执行隐藏的命令
    - 可通过广播、视频网站等渠道传播
  </Tab>
</Tabs>

### 3.4 恶意软件检测系统的绕过

在网络安全领域，许多杀毒软件使用机器学习技术来识别威胁。

| 攻击手段 | 说明 |
|---------|------|
| 添加无用代码 | 不影响功能但改变特征 |
| 改变代码排列 | 重新组织代码顺序 |
| 修改文件元数据 | 改变文件的非执行部分 |

<Callout title="研究结果（2019）" type="error">
通过对恶意软件进行对抗性修改，可以使检测率从 **99%** 降低到 **10% 以下**。
</Callout>

### 3.5 案例总结与启示

| 特点 | 说明 |
|------|------|
| **普遍性** | 广泛存在于图像识别、语音识别、NLP、恶意软件检测等领域 |
| **隐蔽性** | 在人类看来都很正常，但能够欺骗 AI 系统 |
| **实用性** | 可以在真实世界中实施，不需要直接访问模型参数 |
| **危害性** | 在安全敏感场景中可能造成严重后果 |

## 4. 对抗样本的分类与特点

### 4.1 按攻击目标分类

<Tabs items={['无目标攻击', '有目标攻击']}>
  <Tab value="无目标攻击">
    **Untargeted Attack**
    
    攻击者的目标是让模型产生任何错误的输出，不关心具体的错误类型。
    
    **示例**：将猫的图片误判为任何非猫的类别（狗、鸟、汽车等）都算攻击成功。
    
    **特点**：相对容易实现，只需让样本跨越决策边界即可。
  </Tab>
  <Tab value="有目标攻击">
    **Targeted Attack**
    
    攻击者的目标是让模型输出特定的错误结果。
    
    **示例**：将猫的图片误判为"狗"，而不是其他类别。
    
    **特点**：更加困难，需要让样本落在特定的目标类别区域内。在某些场景中更危险（如冒充特定身份）。
  </Tab>
</Tabs>

### 4.2 按攻击者知识分类

| 类型 | 英文 | 攻击者知识 | 特点 |
|------|------|-----------|------|
| **白盒攻击** | White-box | 完全了解模型结构、参数、训练数据 | 可直接计算最优扰动 |
| **黑盒攻击** | Black-box | 不了解模型内部细节，只能通过输入输出交互 | 更接近真实场景 |
| **灰盒攻击** | Gray-box | 部分了解模型信息 | 介于白盒和黑盒之间 |

### 4.3 按实施方式分类

<Callout title="数字攻击 vs 物理攻击" type="info">
- **数字攻击（Digital Attack）**：对抗样本只存在于数字形式中，直接修改数字图像、音频或文本文件
- **物理攻击（Physical Attack）**：对抗样本存在于物理世界中，需要在真实物体上进行修改（如贴纸、眼镜等）
</Callout>

## 本章小结

<Steps>
  <Step>
    **对抗样本的本质**：经过精心设计的输入数据，在人类看来与正常样本几乎没有区别，但能够导致 AI 模型产生错误的输出
  </Step>
  <Step>
    **有效性的原因**：决策边界的脆弱性、高维空间的累积效应、模型的局部线性特性
  </Step>
  <Step>
    **真实世界的威胁**：从自动驾驶到人脸识别，从语音助手到恶意软件检测，各类 AI 应用都面临风险
  </Step>
  <Step>
    **攻击的多样性**：可按攻击目标、攻击者知识、实施方式等进行分类
  </Step>
</Steps>


## 课后思考

<Accordions>
  <Accordion title="思考题1：对抗样本的本质">
    请用自己的话解释什么是对抗样本，以及为什么人类无法察觉对抗扰动，但 AI 模型却会被欺骗？
  </Accordion>
  <Accordion title="思考题2：自动驾驶的威胁分析">
    假设你是一个自动驾驶系统的安全工程师，你认为对抗样本攻击对自动驾驶系统的哪些功能模块威胁最大？为什么？
  </Accordion>
  <Accordion title="思考题3：防护措施设计">
    在日常生活中，你能想到哪些场景可能受到对抗样本攻击的威胁？如果你是这些系统的设计者，你会采取哪些初步的防护措施？
  </Accordion>
</Accordions>

## 延伸阅读

- 本模块相关章节：[第2章：白盒攻击技术](/docs/03-adversarial-examples/white-box-attacks)、[第3章：黑盒攻击技术](/docs/03-adversarial-examples/black-box-attacks)
- 配套实验：[实验 3.1：FGSM 攻击](/docs/03-adversarial-examples/labs/fgsm-attack)、[实验 3.2：PGD 攻击](/docs/03-adversarial-examples/labs/pgd-attack)、[实验 3.3：迁移攻击](/docs/03-adversarial-examples/labs/transfer-attack)


