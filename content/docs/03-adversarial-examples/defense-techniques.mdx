---
title: 第5章：防御技术入门
description: 学习对抗训练、输入预处理和检测等防御策略
---


import { Callout } from 'fumadocs-ui/components/callout';
import { Steps, Step } from 'fumadocs-ui/components/steps';
import { Tabs, Tab } from 'fumadocs-ui/components/tabs';
import { Accordion, Accordions } from 'fumadocs-ui/components/accordion';


<Callout title="预计学习时间" type="info">
预计阅读约22分钟
</Callout>

## 本章导读

在前面的章节中，我们学习了对抗样本的基本原理以及白盒、黑盒两类攻击技术。攻击者可以通过精心设计的微小扰动欺骗 AI 模型，这对实际应用构成了严重威胁。那么，面对这些攻击，我们能做些什么来保护模型呢？

## 学习目标

<Callout title="本章学完后，你将能够：" type="info">
1. **理解防御的整体思路**：掌握主要技术路线
2. **解释对抗训练的基本原理**：说明其优势与局限
3. **描述输入预处理和检测方法**：理解其工作机制
4. **认识攻防博弈关系**：理解防御的持续性挑战
5. **选择合适的防御策略**：为实际应用场景设计防御方案
</Callout>

## 1. 防御的整体思路与挑战

### 1.1 为什么防御如此困难

<Callout title="核心挑战" type="warn">
对抗样本防御是一个极具挑战性的问题，**目前还没有完美的解决方案**。
</Callout>

**生活类比**：
> 想象你是一位银行柜员，需要识别假钞。传统的假钞可能在纸张质感、水印、荧光反应等方面存在明显缺陷，经过培训后比较容易识别。但如果造假者使用了与真钞几乎完全相同的材料和工艺，只在某个极其细微的地方做了手脚，而这个细微差异恰好能让验钞机误判，那么识别难度就会大大增加。

**根本原因**：
- 深度学习模型的决策边界极其复杂
- 存在大量人类难以察觉但攻击者可以利用的"盲区"
- 修补一个漏洞后，攻击者往往能找到新的攻击路径

### 1.2 防御的三条主要路线

| 路线 | 核心思想 | 代表技术 |
|------|---------|---------|
| **增强模型鲁棒性** | 让模型本身变得更加"强壮" | 对抗训练 |
| **输入端防护** | 在输入进入模型之前设置"安检" | 预处理、检测 |
| **输出端验证** | 通过一致性检查发现异常 | 多模型投票、一致性检验 |

<Callout title="实践建议" type="info">
在实际应用中，这三条路线往往需要**结合使用**，形成多层次的防御体系。
</Callout>

## 2. 对抗训练：让模型"见多识广"

### 2.1 对抗训练的基本思想

**对抗训练（Adversarial Training）**是目前公认**最有效**的防御方法之一。

<Callout title="疫苗类比">
疫苗的原理是向人体注入经过处理的病原体（减毒或灭活），让免疫系统提前"认识"这些威胁，从而在真正感染时能够快速响应。

对抗训练的逻辑与此类似：我们主动生成对抗样本（相当于"病原体"），将它们加入训练数据，让模型学会应对这类攻击。
</Callout>

### 2.2 对抗训练的实现过程

<Steps>
  <Step>
    **准备正常的训练数据**：与普通模型训练相同，收集带有正确标签的训练样本
  </Step>
  <Step>
    **生成对抗样本**：使用攻击算法（如 FGSM 或 PGD）为每个正常样本生成对应的对抗样本，保留原始样本的正确标签
  </Step>
  <Step>
    **混合训练**：将正常样本和对抗样本混合在一起，模型需要学会对两类样本都给出正确的分类结果
  </Step>
  <Step>
    **迭代优化**：随着模型能力提升，重新生成更强的对抗样本，继续训练（可重复多次）
  </Step>
</Steps>

<Callout title="为什么要迭代？" type="info">
对抗样本是针对特定模型的弱点设计的。当模型通过训练修补了某些弱点后，原来的对抗样本可能就不再有效了，需要寻找新的攻击方向。
</Callout>

### 2.3 对抗训练的效果与局限

<Tabs items={['效果', '局限性']}>
  <Tab value="效果">
    **显著提升鲁棒性**
    
    在标准的评估基准上：
    - 未经对抗训练：面对攻击时准确率接近 **0%**
    - 经过对抗训练：准确率可提升到 **40%-60%** 甚至更高
    
    这是一个相当可观的改进。
  </Tab>
  <Tab value="局限性">
    | 局限 | 说明 |
    |------|------|
    | **计算成本高** | 每个训练步骤都需要生成对抗样本，训练时间可能增加 **10 倍以上** |
    | **可能降低正常准确率** | 模型需要在"正常表现"和"鲁棒性"之间权衡，正常准确率可能略有下降 |
    | **防御范围有限** | 主要针对训练时使用的攻击方法有效，新型攻击可能绕过防御 |
    | **扰动范围限制** | 通常针对特定大小的扰动范围优化，更大或更小的扰动可能绕过 |
  </Tab>
</Tabs>

## 3. 输入预处理：在门口设置"安检"

### 3.1 预处理防御的基本思路

输入预处理的核心思想是：在输入进入模型之前，先对其进行某种变换处理，试图**消除或削弱**其中可能存在的对抗扰动。

<Callout title="机场安检类比">
旅客（输入）在登机（进入模型）之前，需要经过安检（预处理）。安检的目的是发现并消除潜在的危险物品（对抗扰动），确保进入飞机的都是"安全"的旅客。
</Callout>

**优势**：不需要修改模型本身，可以作为独立模块添加到现有系统中。

### 3.2 常见的预处理技术

<Tabs items={['图像压缩', '图像平滑', '随机变换', '特征压缩']}>
  <Tab value="图像压缩">
    **JPEG 压缩与重建**
    
    - JPEG 是有损压缩，会丢弃图像中的一些高频信息
    - 对抗扰动通常包含高频成分
    - JPEG 压缩可能削弱这些扰动
    - 类似地，缩小图像后再放大也可能消除部分扰动
  </Tab>
  <Tab value="图像平滑">
    **高斯滤波、中值滤波**
    
    - 对图像进行平滑处理，减少图像中的噪声成分
    - 对抗扰动在某种程度上类似于噪声
    - 平滑处理可能有助于削弱攻击效果
  </Tab>
  <Tab value="随机变换">
    **随机裁剪、缩放、旋转**
    
    - 对输入图像施加随机的小幅度变换
    - 对抗扰动是针对特定输入精心设计的
    - 随机变换可能破坏扰动的精确性
  </Tab>
  <Tab value="特征压缩">
    **像素量化、自编码器重建**
    
    - 将像素值量化到较少的离散级别
    - 或使用自编码器对输入进行压缩和重建
    - 可能消除输入中的细微扰动
  </Tab>
</Tabs>

### 3.3 预处理方法的局限性

<Callout title="自适应攻击" type="error">
**自适应攻击（Adaptive Attack）**：攻击者在设计对抗样本时，将防御方法也考虑在内。

例如：如果攻击者知道系统使用了 JPEG 压缩作为预处理，他可以在生成对抗样本时**模拟这个压缩过程**，确保生成的对抗样本在经过压缩后仍然有效。
</Callout>

**重要教训**：
- 评估防御方法时，必须考虑自适应攻击
- 如果一种防御只在攻击者"不知情"时有效，实际价值有限
- 在安全领域，通常假设攻击者了解防御机制（**Kerckhoffs 原则**）

## 4. 对抗样本检测：识别可疑输入

### 4.1 检测方法的基本思路

与预处理方法试图"消除"对抗扰动不同，检测方法的目标是"**识别**"对抗样本。

一旦检测到可疑输入，系统可以：
- 拒绝处理
- 发出警报
- 请求人工审核

**核心假设**：对抗样本与正常样本之间存在某些可以被识别的差异。

### 4.2 主要的检测技术

| 技术类型 | 原理 |
|---------|------|
| **基于统计特性** | 对抗样本在某些统计特性上可能与正常样本存在差异（如局部统计分布、频域能量分布） |
| **基于模型行为** | 观察模型处理输入时的内部行为（如中间层激活模式、置信度分布） |
| **基于一致性** | 对输入施加多种变换，检查模型输出的一致性（对抗样本在变换后可能产生不一致结果） |
| **基于重建** | 使用自编码器重建输入，对抗样本的重建误差可能更大 |

### 4.3 检测方法的挑战

<Callout title="检测率与误报率的权衡" type="warn">
- **阈值太严格**：可能将大量正常样本误判为对抗样本（高误报率）
- **阈值太宽松**：可能漏掉真正的攻击（低检测率）

在实际应用中，找到合适的平衡点是一个挑战。
</Callout>

与预处理方法类似，检测方法也面临**自适应攻击**的挑战：如果攻击者了解检测机制，可以在生成对抗样本时加入额外的约束，使样本绑过检测。

## 5. 防御的猫鼠游戏与务实策略

### 5.1 攻防博弈的本质

每当研究者提出一种新的防御方法，很快就会有人提出能够绑过这种防御的攻击方法。这种现象被形象地称为"**猫鼠游戏**"。

<Callout title="案例：梯度掩码的教训（2018）" type="error">
**背景**：2017-2018年间，研究者提出了多种基于"梯度掩码"的防御方法，思路是隐藏或扰乱梯度，使攻击者无法有效计算扰动。

**最初效果**：在初步评估中，这些方法似乎非常有效，能够大幅降低攻击成功率。

**被攻破**：2018年的一项研究系统性地分析了这些防御，发现它们大多存在"虚假的安全感"。研究者提出了多种绕过技术（使用替代模型的梯度、数值方法估计梯度等），几乎所有基于梯度掩码的防御都可以被绕过。

**启示**：防御方法的评估必须考虑自适应攻击。
</Callout>

### 5.2 务实的防御策略

<Steps>
  <Step>
    **采用多层防御**：不依赖单一防御方法，组合使用对抗训练、输入预处理和检测机制。即使某一层被突破，其他层仍然可以提供保护。
  </Step>
  <Step>
    **对抗训练作为基础**：在各种防御方法中，对抗训练是经过最充分验证的方法，应该作为防御体系的基础。
  </Step>
  <Step>
    **持续监控与更新**：安全是一个持续的过程，需要建立监控机制及时发现异常，同时关注最新的攻击技术，定期更新防御措施。
  </Step>
  <Step>
    **根据风险等级选择策略**：安全关键应用（如自动驾驶、医疗诊断）需要投入更多资源进行防御；风险较低的应用可以采用相对简单的措施。
  </Step>
  <Step>
    **保持谦逊的态度**：承认当前防御技术的局限性，不要过度依赖 AI 系统的判断。在关键决策中，应该保留人工审核的环节。
  </Step>
</Steps>

### 5.3 多层防御体系示例

```
┌─────────────────────────────────────────┐
│              用户输入                    │
└─────────────────┬───────────────────────┘
                  ▼
┌─────────────────────────────────────────┐
│      第一层：输入预处理                   │
│   （图像压缩、随机变换、特征压缩）         │
└─────────────────┬───────────────────────┘
                  ▼
┌─────────────────────────────────────────┐
│      第二层：对抗检测                     │
│   （统计特性检测、一致性检查）             │
│         ↓ 可疑？→ 拒绝/警报              │
└─────────────────┬───────────────────────┘
                  ▼
┌─────────────────────────────────────────┐
│      第三层：鲁棒模型                     │
│   （对抗训练后的模型）                    │
└─────────────────┬───────────────────────┘
                  ▼
┌─────────────────────────────────────────┐
│      第四层：输出验证                     │
│   （多模型投票、置信度检查）              │
└─────────────────┬───────────────────────┘
                  ▼
┌─────────────────────────────────────────┐
│              最终输出                    │
└─────────────────────────────────────────┘
```


## 术语对照表

| 中文术语 | 英文术语 | 简要解释 |
|---------|---------|---------|
| 对抗训练 | Adversarial Training | 在训练过程中引入对抗样本以增强模型鲁棒性的方法 |
| 自适应攻击 | Adaptive Attack | 攻击者在设计对抗样本时将防御方法考虑在内的攻击方式 |
| 梯度掩码 | Gradient Masking | 通过隐藏或扰乱模型梯度来阻止基于梯度的攻击的防御思路 |
| 输入预处理 | Input Preprocessing | 在输入进入模型前进行变换处理以削弱对抗扰动的方法 |
| 鲁棒性 | Robustness | 模型在面对对抗样本或其他干扰时保持正确输出的能力 |

## 本章小结

<Steps>
  <Step>
    **防御的挑战**：对抗样本防御是一个困难的问题，目前没有完美的解决方案
  </Step>
  <Step>
    **三条防御路线**：增强模型鲁棒性（对抗训练）、输入端防护（预处理和检测）、输出端验证（一致性检查）
  </Step>
  <Step>
    **对抗训练**：通过在训练中引入对抗样本来增强模型鲁棒性，是目前最可靠的防御方法
  </Step>
  <Step>
    **预处理方法**：通过变换输入来削弱对抗扰动，实现简单但容易被自适应攻击绕过
  </Step>
  <Step>
    **检测方法**：试图识别对抗样本并拒绝处理，面临检测率与误报率的权衡问题
  </Step>
  <Step>
    **攻防博弈**：防御与攻击之间存在持续的博弈关系，评估防御方法时必须考虑自适应攻击
  </Step>
  <Step>
    **务实策略**：采用多层防御、以对抗训练为基础、持续监控更新、根据风险等级选择策略
  </Step>
</Steps>


## 课后思考

<Accordions>
  <Accordion title="思考题1：对抗训练的原理">
    对抗训练为什么能够提升模型的鲁棒性？它的基本原理是什么？
  </Accordion>
  <Accordion title="思考题2：预处理防御的局限性">
    为什么大多数预处理防御方法在面对自适应攻击时效果会大幅下降？这对我们评估防御方法有什么启示？
  </Accordion>
  <Accordion title="思考题3：人脸识别门禁系统防御方案">
    假设你需要为一个人脸识别门禁系统设计防御方案，你会采用哪些防御技术的组合？请说明你的选择理由。
  </Accordion>
</Accordions>

## 延伸阅读

- 本模块相关章节：[第3章：黑盒攻击技术](/docs/03-adversarial-examples/black-box-attacks)、[第4章：文本对抗攻击](/docs/03-adversarial-examples/text-attacks)
- 配套实验：[实验 3.1：FGSM 攻击](/docs/03-adversarial-examples/labs/fgsm-attack)、[实验 3.2：PGD 攻击](/docs/03-adversarial-examples/labs/pgd-attack)、[实验 3.3：迁移攻击](/docs/03-adversarial-examples/labs/transfer-attack)


