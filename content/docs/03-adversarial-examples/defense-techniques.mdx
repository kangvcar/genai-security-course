---
title: 第5章：防御技术入门
description: 学习对抗训练、输入预处理和检测等防御策略
---


import { Callout } from 'fumadocs-ui/components/callout';
import { Steps, Step } from 'fumadocs-ui/components/steps';
import { Tabs, Tab } from 'fumadocs-ui/components/tabs';
import { Accordion, Accordions } from 'fumadocs-ui/components/accordion';


<Callout title="" type="info">
预计阅读约22分钟
</Callout>

在前面的章节中，我们学习了对抗样本的基本原理以及白盒、黑盒两类攻击技术。攻击者可以通过精心设计的微小扰动欺骗AI模型，这对实际应用构成了严重威胁。那么，面对这些攻击，我们能做些什么来保护模型呢？本章将介绍对抗样本防御的主要思路和技术，帮助读者理解如何构建更加鲁棒的AI系统，同时也会坦诚地讨论当前防御技术面临的挑战和局限性。

## 章节目标

学完本章后，你将能够：

- 理解对抗样本防御的整体思路和主要技术路线
- 解释对抗训练的基本原理，并说明其优势与局限
- 描述输入预处理和检测方法的工作机制
- 认识到防御与攻击之间的动态博弈关系
- 为实际应用场景选择合适的防御策略组合

---

## 1. 防御的整体思路与挑战

### 1.1 为什么防御如此困难

在讨论具体的防御技术之前，我们需要先理解一个重要的背景：对抗样本防御是一个极具挑战性的问题，目前还没有完美的解决方案。

这种困难可以用一个生活中的类比来理解。想象你是一位银行柜员，需要识别假钞。传统的假钞可能在纸张质感、水印、荧光反应等方面存在明显缺陷，经过培训后比较容易识别。但如果造假者使用了与真钞几乎完全相同的材料和工艺，只在某个极其细微的地方做了手脚，而这个细微差异恰好能让验钞机误判，那么识别难度就会大大增加。对抗样本正是这样一种"高仿"攻击——它与正常输入的差异极其微小，却能精准地利用模型的弱点。

防御困难的根本原因在于：深度学习模型的决策边界极其复杂，存在大量人类难以察觉但攻击者可以利用的"盲区"。更棘手的是，当我们修补了一个漏洞，攻击者往往能找到新的攻击路径。这就形成了一场持续的攻防博弈。

### 1.2 防御的三条主要路线

尽管防御困难，研究者们仍然发展出了多种防御策略。从整体思路来看，主要有三条技术路线：

**第一条路线：增强模型本身的鲁棒性**。这种方法的核心思想是，与其在模型外部设置防线，不如让模型本身变得更加"强壮"。最具代表性的技术是**对抗训练（Adversarial Training）**，即在训练过程中主动引入对抗样本，让模型学会正确处理这类输入。

**第二条路线：在输入端设置"安检"**。这种方法不改变模型本身，而是在输入进入模型之前进行预处理或检测。预处理方法试图消除输入中可能存在的对抗扰动；检测方法则试图识别出对抗样本并拒绝处理。

**第三条路线：在输出端进行验证**。这种方法关注模型的输出结果，通过一致性检查、多模型投票等方式来发现异常。如果多个独立的模型对同一输入给出了不一致的判断，就可能存在问题。

在实际应用中，这三条路线往往需要结合使用，形成多层次的防御体系。接下来，我们将逐一介绍这些技术的具体原理。

---

## 2. 对抗训练：让模型"见多识广"

### 2.1 对抗训练的基本思想

**对抗训练（Adversarial Training）**是目前公认最有效的防御方法之一。它的核心思想非常直观：如果模型在训练时从未见过对抗样本，那么在实际部署后遇到攻击时自然容易被欺骗；反过来，如果我们在训练阶段就让模型接触大量对抗样本，并教会它正确分类这些样本，模型就能获得一定的"免疫力"。

这个思想可以用疫苗接种来类比。疫苗的原理是向人体注入经过处理的病原体（减毒或灭活），让免疫系统提前"认识"这些威胁，从而在真正感染时能够快速响应。对抗训练的逻辑与此类似：我们主动生成对抗样本（相当于"病原体"），将它们加入训练数据，让模型学会应对这类攻击。

### 2.2 对抗训练的实现过程

对抗训练的具体实现可以分为以下步骤：

**第一步：准备正常的训练数据**。这与普通的模型训练相同，需要收集带有正确标签的训练样本。

**第二步：生成对抗样本**。对于每个正常样本，使用攻击算法（如前面章节介绍的FGSM或PGD）生成对应的对抗样本。这些对抗样本保留原始样本的正确标签。

**第三步：混合训练**。将正常样本和对抗样本混合在一起，用这个扩充后的数据集训练模型。模型需要学会对两类样本都给出正确的分类结果。

**第四步：迭代优化**。随着模型能力的提升，之前生成的对抗样本可能不再有效。因此，需要基于当前模型重新生成更强的对抗样本，然后继续训练。这个过程可以重复多次。

有同学可能会问：为什么要用当前模型重新生成对抗样本？这是因为对抗样本是针对特定模型的弱点设计的。当模型通过训练修补了某些弱点后，原来的对抗样本可能就不再有效了，需要寻找新的攻击方向。

### 2.3 对抗训练的效果与局限

对抗训练确实能够显著提升模型的鲁棒性。在标准的评估基准上，经过对抗训练的模型面对攻击时的准确率可以从接近0%提升到40%-60%甚至更高。这是一个相当可观的改进。

然而，对抗训练也存在明显的局限性：

**局限一：计算成本高**。对抗训练需要在每个训练步骤中生成对抗样本，这大大增加了训练时间。使用PGD等迭代攻击方法时，训练时间可能增加10倍以上。

**局限二：可能降低正常准确率**。研究发现，对抗训练往往会导致模型在正常样本上的准确率略有下降。这是因为模型需要在"正常表现"和"鲁棒性"之间做出权衡。

**局限三：防御范围有限**。对抗训练主要针对训练时使用的攻击方法有效。如果攻击者使用了训练时未考虑的新型攻击，防御效果可能会大打折扣。

**局限四：扰动范围的限制**。对抗训练通常针对特定大小的扰动范围进行优化。如果攻击者使用了更大或更小的扰动，模型的防御能力可能会下降。

尽管存在这些局限，对抗训练仍然是目前最可靠的防御基线。在实际应用中，它通常作为防御体系的核心组成部分。

---

## 3. 输入预处理：在门口设置"安检"

### 3.1 预处理防御的基本思路

输入预处理是另一类重要的防御方法。它的核心思想是：在输入进入模型之前，先对其进行某种变换处理，试图消除或削弱其中可能存在的对抗扰动。

这种方法可以用机场安检来类比。旅客（输入）在登机（进入模型）之前，需要经过安检（预处理）。安检的目的是发现并消除潜在的危险物品（对抗扰动），确保进入飞机的都是"安全"的旅客。

预处理方法的优势在于：它不需要修改模型本身，可以作为一个独立的模块添加到现有系统中。这对于那些无法重新训练的已部署模型特别有价值。

### 3.2 常见的预处理技术

研究者们提出了多种预处理技术，以下是几种代表性的方法：

**图像压缩与重建**。JPEG压缩是一种有损压缩方法，会丢弃图像中的一些高频信息。由于对抗扰动通常包含高频成分，JPEG压缩可能会削弱这些扰动。类似地，将图像缩小后再放大，也可能消除部分扰动信息。

**图像平滑处理**。使用高斯滤波、中值滤波等方法对图像进行平滑处理，可以减少图像中的噪声成分。对抗扰动在某种程度上类似于噪声，因此平滑处理可能有助于削弱攻击效果。

**随机变换**。在输入图像上施加随机的小幅度变换，如随机裁剪、随机缩放、随机旋转等。由于对抗扰动是针对特定输入精心设计的，随机变换可能会破坏扰动的精确性，从而降低攻击效果。

**特征压缩**。将输入的像素值量化到较少的离散级别，或者使用自编码器对输入进行压缩和重建。这些方法都可能消除输入中的细微扰动。

### 3.3 预处理方法的局限性

预处理方法看起来简单有效，但在实际中面临一个严重的问题：**自适应攻击（Adaptive Attack）**。

所谓自适应攻击，是指攻击者在设计对抗样本时，将防御方法也考虑在内。具体来说，如果攻击者知道系统使用了JPEG压缩作为预处理，他可以在生成对抗样本时模拟这个压缩过程，确保生成的对抗样本在经过压缩后仍然有效。

研究表明，大多数预处理方法在面对自适应攻击时效果会大幅下降。这是因为预处理本身是一个确定性的变换，攻击者可以将其纳入攻击优化过程中。

这里有一个重要的教训：评估防御方法时，必须考虑自适应攻击。如果一种防御方法只在攻击者"不知情"的情况下有效，那么它的实际价值就很有限。在安全领域，我们通常假设攻击者了解防御机制（这被称为Kerckhoffs原则）。

尽管如此，预处理方法并非毫无价值。在某些场景下，它可以增加攻击的难度和成本，作为多层防御体系的一部分仍然有其作用。

---

## 4. 对抗样本检测：识别可疑输入

### 4.1 检测方法的基本思路

与预处理方法试图"消除"对抗扰动不同，检测方法的目标是"识别"对抗样本。一旦检测到可疑输入，系统可以拒绝处理或发出警报，而不是给出可能错误的结果。

检测方法的核心假设是：对抗样本与正常样本之间存在某些可以被识别的差异。虽然这些差异在原始输入空间中可能很微小，但在某些特征空间或统计特性上可能会表现得更加明显。

### 4.2 主要的检测技术

**基于统计特性的检测**。研究发现，对抗样本在某些统计特性上可能与正常样本存在差异。例如，对抗扰动可能会改变图像的局部统计分布，或者在频域中表现出异常的能量分布。通过分析这些统计特性，可以尝试识别对抗样本。

**基于模型行为的检测**。这类方法关注模型处理输入时的内部行为。例如，可以观察模型中间层的激活模式，或者分析模型输出的置信度分布。对抗样本可能会导致异常的激活模式或不自然的置信度分布。

**基于一致性的检测**。这类方法对输入施加多种变换，然后检查模型输出的一致性。对于正常样本，轻微的变换通常不会改变分类结果；但对于对抗样本，由于扰动是精心设计的，变换可能会破坏攻击效果，导致分类结果发生变化。这种不一致性可以作为检测的依据。

**基于重建的检测**。使用自编码器或生成模型对输入进行重建，然后比较原始输入和重建结果。正常样本通常能够被较好地重建，而对抗样本由于包含异常的扰动，重建误差可能会更大。

### 4.3 检测方法的挑战

与预处理方法类似，检测方法也面临自适应攻击的挑战。如果攻击者了解检测机制，他可以在生成对抗样本时加入额外的约束，使生成的样本能够绑过检测。

此外，检测方法还面临一个固有的权衡：**检测率与误报率的平衡**。如果检测阈值设置得太严格，可能会将大量正常样本误判为对抗样本（高误报率）；如果阈值设置得太宽松，又可能漏掉真正的攻击（低检测率）。在实际应用中，找到合适的平衡点是一个挑战。

---

## 5. 防御的猫鼠游戏与务实策略

### 5.1 攻防博弈的本质

通过前面的讨论，我们可以看到一个清晰的模式：每当研究者提出一种新的防御方法，很快就会有人提出能够绑过这种防御的攻击方法。这种现象被形象地称为"猫鼠游戏"。

**案例：梯度掩码的教训（2018年）**

2017-2018年间，研究者们提出了多种基于"梯度掩码"的防御方法。这些方法的思路是：既然白盒攻击依赖于模型的梯度信息，那么如果我们能够隐藏或扰乱梯度，攻击者就无法有效地生成对抗样本。

一些具体的做法包括：使用不可微分的操作、在梯度中添加噪声、使用防御蒸馏等。在最初的评估中，这些方法似乎非常有效，能够大幅降低攻击成功率。

然而，2018年的一项重要研究系统性地分析了这些防御方法，发现它们大多存在"虚假的安全感"。研究者提出了多种绑过梯度掩码的技术，包括：使用替代模型的梯度、使用数值方法估计梯度、使用基于得分的攻击等。结果表明，几乎所有基于梯度掩码的防御都可以被绑过。

这个案例给我们的启示是：防御方法的评估必须考虑自适应攻击，不能仅仅因为现有攻击方法失效就认为防御是有效的。

### 5.2 务实的防御策略

面对这种攻防博弈的现实，我们应该采取怎样的防御策略呢？以下是一些务实的建议：

**策略一：采用多层防御**。不要依赖单一的防御方法，而是组合使用多种技术。例如，可以同时使用对抗训练（增强模型鲁棒性）、输入预处理（削弱扰动）和检测机制（识别可疑输入）。即使某一层防御被突破，其他层仍然可以提供保护。

**策略二：对抗训练作为基础**。在各种防御方法中，对抗训练是经过最充分验证的方法，应该作为防御体系的基础。虽然它不能提供完美的保护，但能够显著提升模型的基线鲁棒性。

**策略三：持续监控与更新**。安全是一个持续的过程，而不是一次性的工作。需要建立监控机制，及时发现异常行为；同时关注最新的攻击技术，定期更新防御措施。

**策略四：根据风险等级选择策略**。不同的应用场景有不同的安全需求。对于安全关键的应用（如自动驾驶、医疗诊断），需要投入更多资源进行防御；对于风险较低的应用，可以采用相对简单的防御措施。

**策略五：保持谦逊的态度**。承认当前防御技术的局限性，不要过度依赖AI系统的判断。在关键决策中，应该保留人工审核的环节。

---

## 本章小结

本章介绍了对抗样本防御的主要技术和策略。核心要点如下：

1. **防御的挑战**：对抗样本防御是一个困难的问题，目前没有完美的解决方案。防御困难的根本原因在于深度学习模型决策边界的复杂性。

2. **三条防御路线**：增强模型鲁棒性（对抗训练）、输入端防护（预处理和检测）、输出端验证（一致性检查）。

3. **对抗训练**：通过在训练中引入对抗样本来增强模型鲁棒性，是目前最可靠的防御方法，但存在计算成本高、可能降低正常准确率等局限。

4. **预处理方法**：通过变换输入来削弱对抗扰动，实现简单但容易被自适应攻击绑过。

5. **检测方法**：试图识别对抗样本并拒绝处理，面临检测率与误报率的权衡问题。

6. **攻防博弈**：防御与攻击之间存在持续的博弈关系，评估防御方法时必须考虑自适应攻击。

7. **务实策略**：采用多层防御、以对抗训练为基础、持续监控更新、根据风险等级选择策略。

---

## 教学资源

### 图表与示意图

- 图5-1：防御技术分类图（建议配图：展示三条防御路线及其代表技术）
- 图5-2：对抗训练流程图（建议配图：展示生成对抗样本→混合训练→迭代优化的过程）
- 图5-3：多层防御体系示意图（建议配图：展示输入预处理→模型→检测→输出的防御流程）

### 配套实验

本章内容为防御技术的概念性介绍，配套实验将在后续模块中安排。建议学生在理解本章内容后，尝试在实验3.1（FGSM攻击）的基础上，观察对抗训练对模型鲁棒性的影响。

### 延伸阅读

- Madry等人关于对抗训练的经典论文（2018）
- Carlini和Wagner关于防御评估的研究（2017）
- 关于梯度掩码失效的系统性分析（Athalye等，2018）

---

## 课后思考题

1. **理解性问题**：对抗训练为什么能够提升模型的鲁棒性？它的基本原理是什么？

2. **分析性问题**：为什么大多数预处理防御方法在面对自适应攻击时效果会大幅下降？这对我们评估防御方法有什么启示？

3. **应用性问题**：假设你需要为一个人脸识别门禁系统设计防御方案，你会采用哪些防御技术的组合？请说明你的选择理由。