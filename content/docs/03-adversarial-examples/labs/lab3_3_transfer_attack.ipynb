{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验 3.3：黑盒迁移攻击\n",
    "\n",
    "## 实验目标\n",
    "- 理解对抗样本的迁移性原理\n",
    "- 在替代模型上生成对抗样本，测试对目标模型的效果\n",
    "- 观察不同模型间的迁移成功率\n",
    "\n",
    "## 实验环境\n",
    "- Python 3.8+\n",
    "- PyTorch\n",
    "- torchvision（多个预训练模型）\n",
    "\n",
    "## 预计时间：30 分钟\n",
    "\n",
    "---\n",
    "\n",
    "## 核心概念回顾\n",
    "迁移攻击：在本地替代模型上用白盒方法生成对抗样本，然后测试它是否对未知的目标模型也有效。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一部分：环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# 设置中文显示\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 标准化参数\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "print(\"环境准备完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载多个预训练模型\n",
    "# 我们将用一个模型作为\"替代模型\"，其他作为\"目标模型\"\n",
    "\n",
    "print(\"正在加载多个模型（这可能需要一些时间）...\")\n",
    "\n",
    "# 替代模型：我们用它来生成对抗样本\n",
    "surrogate_model = models.resnet18(pretrained=True)\n",
    "surrogate_model.eval()\n",
    "print(\"✓ 替代模型 (ResNet18) 加载完成\")\n",
    "\n",
    "# 目标模型：我们测试对抗样本是否能迁移\n",
    "target_models = {\n",
    "    'ResNet34': models.resnet34(pretrained=True),\n",
    "    'VGG16': models.vgg16(pretrained=True),\n",
    "    'DenseNet121': models.densenet121(pretrained=True),\n",
    "}\n",
    "\n",
    "for name, model in target_models.items():\n",
    "    model.eval()\n",
    "    print(f\"✓ 目标模型 ({name}) 加载完成\")\n",
    "\n",
    "print(\"\\n所有模型加载完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建测试图片\n",
    "def create_test_image():\n",
    "    np.random.seed(42)\n",
    "    img = np.random.rand(224, 224, 3) * 0.3 + 0.35\n",
    "    center_x, center_y = 112, 112\n",
    "    for i in range(224):\n",
    "        for j in range(224):\n",
    "            dist = np.sqrt((i - center_x)**2 + (j - center_y)**2)\n",
    "            if dist < 60:\n",
    "                img[i, j] = [0.1, 0.1, 0.1]\n",
    "            elif dist < 80:\n",
    "                img[i, j] = [0.9, 0.9, 0.9]\n",
    "    return torch.tensor(img, dtype=torch.float32).permute(2, 0, 1)\n",
    "\n",
    "def predict(model, img_tensor):\n",
    "    \"\"\"获取模型预测\"\"\"\n",
    "    input_tensor = normalize(img_tensor).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    pred_class = output.argmax(dim=1).item()\n",
    "    confidence = probs[0, pred_class].item()\n",
    "    return pred_class, confidence\n",
    "\n",
    "# 创建测试图片\n",
    "original_image = create_test_image()\n",
    "\n",
    "# 在各模型上测试原始图片\n",
    "print(\"原始图片在各模型上的预测：\")\n",
    "print(\"-\" * 40)\n",
    "surrogate_pred, surrogate_conf = predict(surrogate_model, original_image)\n",
    "print(f\"替代模型 (ResNet18): 类别 {surrogate_pred}, 置信度 {surrogate_conf:.2%}\")\n",
    "\n",
    "for name, model in target_models.items():\n",
    "    pred, conf = predict(model, original_image)\n",
    "    print(f\"目标模型 ({name}): 类别 {pred}, 置信度 {conf:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二部分：在替代模型上生成对抗样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PGD 攻击函数\n",
    "def pgd_attack(model, image, label, epsilon, alpha, num_steps):\n",
    "    \"\"\"在指定模型上执行 PGD 攻击\"\"\"\n",
    "    adv_image = image.clone().unsqueeze(0)\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        adv_image.requires_grad = True\n",
    "        normalized = normalize(adv_image.squeeze(0)).unsqueeze(0)\n",
    "        output = model(normalized)\n",
    "        loss = nn.CrossEntropyLoss()(output, torch.tensor([label]))\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        gradient = adv_image.grad.data\n",
    "        adv_image = adv_image.detach() + alpha * gradient.sign()\n",
    "        \n",
    "        perturbation = torch.clamp(adv_image - image.unsqueeze(0), -epsilon, epsilon)\n",
    "        adv_image = torch.clamp(image.unsqueeze(0) + perturbation, 0, 1)\n",
    "    \n",
    "    return adv_image.squeeze(0).detach()\n",
    "\n",
    "print(\"PGD 攻击函数定义完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 【填空 1】在替代模型上生成对抗样本\n",
    "# 提示：使用 pgd_attack 函数，以 surrogate_model 为目标\n",
    "# 参考答案：adversarial_image = pgd_attack(surrogate_model, original_image, surrogate_pred, epsilon=0.03, alpha=0.01, num_steps=20)\n",
    "\n",
    "epsilon = 0.03\n",
    "alpha = 0.01\n",
    "num_steps = 20\n",
    "\n",
    "# 在替代模型上生成对抗样本\n",
    "adversarial_image = ___________________\n",
    "\n",
    "# 验证在替代模型上的攻击效果\n",
    "adv_pred, adv_conf = predict(surrogate_model, adversarial_image)\n",
    "print(f\"替代模型上的攻击结果：\")\n",
    "print(f\"  原始预测：类别 {surrogate_pred}, 置信度 {surrogate_conf:.2%}\")\n",
    "print(f\"  攻击后：  类别 {adv_pred}, 置信度 {adv_conf:.2%}\")\n",
    "print(f\"  攻击{'成功 ✓' if adv_pred != surrogate_pred else '失败 ✗'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三部分：测试迁移性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 【填空 2】测试对抗样本在目标模型上的迁移效果\n",
    "# 提示：将在替代模型上生成的对抗样本送入目标模型，观察是否也能攻击成功\n",
    "\n",
    "print(\"迁移攻击测试结果：\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'模型':<15} {'原始预测':<12} {'攻击后预测':<12} {'迁移结果'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "transfer_results = {}\n",
    "\n",
    "for name, model in target_models.items():\n",
    "    # 获取原始预测\n",
    "    orig_pred, orig_conf = predict(model, original_image)\n",
    "    \n",
    "    # 【填空 2】获取对抗样本在目标模型上的预测\n",
    "    # 参考答案：adv_pred, adv_conf = predict(model, adversarial_image)\n",
    "    adv_pred, adv_conf = ___________________\n",
    "    \n",
    "    # 判断迁移是否成功（预测类别改变）\n",
    "    success = orig_pred != adv_pred\n",
    "    transfer_results[name] = success\n",
    "    \n",
    "    status = \"✓ 成功\" if success else \"✗ 失败\"\n",
    "    print(f\"{name:<15} 类别{orig_pred:<10} 类别{adv_pred:<10} {status}\")\n",
    "\n",
    "# 统计迁移成功率\n",
    "success_count = sum(transfer_results.values())\n",
    "total_count = len(transfer_results)\n",
    "print(\"-\" * 60)\n",
    "print(f\"迁移成功率：{success_count}/{total_count} ({success_count/total_count*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化迁移效果\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "# 第一行：原始图片和对抗样本\n",
    "axes[0, 0].imshow(original_image.permute(1, 2, 0).numpy())\n",
    "axes[0, 0].set_title(\"原始图片\")\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(adversarial_image.permute(1, 2, 0).numpy())\n",
    "axes[0, 1].set_title(\"对抗样本\\n(在 ResNet18 上生成)\")\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# 扰动可视化\n",
    "perturbation = adversarial_image - original_image\n",
    "perturbation_vis = (perturbation - perturbation.min()) / (perturbation.max() - perturbation.min())\n",
    "axes[0, 2].imshow(perturbation_vis.permute(1, 2, 0).numpy())\n",
    "axes[0, 2].set_title(\"对抗扰动\")\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# 第二行：各目标模型的结果\n",
    "for idx, (name, model) in enumerate(target_models.items()):\n",
    "    orig_pred, _ = predict(model, original_image)\n",
    "    adv_pred, adv_conf = predict(model, adversarial_image)\n",
    "    \n",
    "    axes[1, idx].imshow(adversarial_image.permute(1, 2, 0).numpy())\n",
    "    color = 'red' if orig_pred != adv_pred else 'black'\n",
    "    status = \"迁移成功\" if orig_pred != adv_pred else \"迁移失败\"\n",
    "    axes[1, idx].set_title(f\"{name}\\n{orig_pred}→{adv_pred} ({status})\", color=color)\n",
    "    axes[1, idx].axis('off')\n",
    "\n",
    "plt.suptitle(\"黑盒迁移攻击结果\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四部分：提高迁移性的技巧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 【填空 3】使用更大的扰动来提高迁移性\n",
    "# 提示：增大 epsilon 通常可以提高迁移成功率，但扰动也更明显\n",
    "\n",
    "epsilon_values = [0.01, 0.03, 0.05, 0.1]\n",
    "\n",
    "print(\"不同扰动大小对迁移性的影响：\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for eps in epsilon_values:\n",
    "    # 【填空 3】使用不同的 epsilon 生成对抗样本\n",
    "    # 参考答案：adv = pgd_attack(surrogate_model, original_image, surrogate_pred, eps, 0.01, 20)\n",
    "    adv = ___________________\n",
    "    \n",
    "    # 测试迁移效果\n",
    "    success_count = 0\n",
    "    for name, model in target_models.items():\n",
    "        orig_pred, _ = predict(model, original_image)\n",
    "        adv_pred, _ = predict(model, adv)\n",
    "        if orig_pred != adv_pred:\n",
    "            success_count += 1\n",
    "    \n",
    "    transfer_rate = success_count / len(target_models) * 100\n",
    "    print(f\"ε = {eps:.2f}: 迁移成功率 = {transfer_rate:.1f}% ({success_count}/{len(target_models)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多模型集成攻击（提高迁移性的高级技巧）\n",
    "def ensemble_attack(models, image, labels, epsilon, alpha, num_steps):\n",
    "    \"\"\"\n",
    "    多模型集成攻击：同时考虑多个模型的梯度\n",
    "    对多个模型都有效的扰动，更可能迁移到其他模型\n",
    "    \"\"\"\n",
    "    adv_image = image.clone().unsqueeze(0)\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        adv_image.requires_grad = True\n",
    "        total_loss = 0\n",
    "        \n",
    "        # 累加多个模型的损失\n",
    "        for model, label in zip(models, labels):\n",
    "            normalized = normalize(adv_image.squeeze(0)).unsqueeze(0)\n",
    "            output = model(normalized)\n",
    "            loss = nn.CrossEntropyLoss()(output, torch.tensor([label]))\n",
    "            total_loss += loss\n",
    "        \n",
    "        # 使用平均损失计算梯度\n",
    "        avg_loss = total_loss / len(models)\n",
    "        for model in models:\n",
    "            model.zero_grad()\n",
    "        avg_loss.backward()\n",
    "        \n",
    "        gradient = adv_image.grad.data\n",
    "        adv_image = adv_image.detach() + alpha * gradient.sign()\n",
    "        perturbation = torch.clamp(adv_image - image.unsqueeze(0), -epsilon, epsilon)\n",
    "        adv_image = torch.clamp(image.unsqueeze(0) + perturbation, 0, 1)\n",
    "    \n",
    "    return adv_image.squeeze(0).detach()\n",
    "\n",
    "# 使用两个模型的集成攻击\n",
    "ensemble_models = [surrogate_model, target_models['ResNet34']]\n",
    "ensemble_labels = [surrogate_pred, predict(target_models['ResNet34'], original_image)[0]]\n",
    "\n",
    "ensemble_adv = ensemble_attack(ensemble_models, original_image, ensemble_labels, 0.03, 0.01, 20)\n",
    "\n",
    "print(\"集成攻击 vs 单模型攻击的迁移效果：\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for name, model in target_models.items():\n",
    "    orig_pred, _ = predict(model, original_image)\n",
    "    \n",
    "    single_adv_pred, _ = predict(model, adversarial_image)\n",
    "    ensemble_adv_pred, _ = predict(model, ensemble_adv)\n",
    "    \n",
    "    single_status = \"✓\" if orig_pred != single_adv_pred else \"✗\"\n",
    "    ensemble_status = \"✓\" if orig_pred != ensemble_adv_pred else \"✗\"\n",
    "    \n",
    "    print(f\"{name}: 单模型攻击 {single_status}, 集成攻击 {ensemble_status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验总结\n",
    "\n",
    "### 观察记录\n",
    "\n",
    "请回答以下问题：\n",
    "\n",
    "1. **迁移攻击的成功率如何？** 在替代模型上生成的对抗样本，对目标模型的迁移成功率是多少？\n",
    "\n",
    "2. **哪些因素影响迁移性？** 扰动大小、模型架构相似度等因素如何影响迁移成功率？\n",
    "\n",
    "3. **集成攻击有帮助吗？** 使用多个模型集成是否提高了迁移性？\n",
    "\n",
    "### 核心概念回顾\n",
    "\n",
    "- **迁移性**：在一个模型上生成的对抗样本可能对其他模型也有效\n",
    "- **黑盒攻击**：不需要知道目标模型的细节\n",
    "- **提高迁移性**：增大扰动、多模型集成、输入多样性\n",
    "\n",
    "---\n",
    "\n",
    "**下一个实验**：实验 3.4 文本对抗攻击"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
