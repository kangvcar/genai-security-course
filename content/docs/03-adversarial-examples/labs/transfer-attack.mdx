---
title: "实验 3.3：迁移攻击"
description: 体验黑盒场景下的对抗样本迁移性
---


import { Callout } from 'fumadocs-ui/components/callout';


<Callout title="" type="info">
预计实验约45分钟
</Callout>

## 实验目标

本实验将帮助你理解对抗样本的迁移性，体验如何在黑盒场景下利用替代模型发起攻击。

<Callout title="学习目标" type="info">
完成本实验后，你将能够：
- 理解对抗样本迁移性的概念和原因
- 使用一个模型生成对抗样本，测试其对其他模型的效果
- 观察不同模型架构之间的迁移成功率
- 实践集成攻击方法提高迁移性
- 认识黑盒攻击的现实威胁
</Callout>

## 实验前提

<Callout title="环境要求" type="warn">
- Python 3.8+
- PyTorch 1.10+
- torchvision（需要多个预训练模型）
- matplotlib
- numpy

建议先完成实验 3.1 和 3.2 再进行本实验。
</Callout>

## 实验内容

:::notebook{file="./lab3_3_transfer_attack.ipynb" showCellNumbers}
:::

## 实验总结

<Callout title="完成检查" type="success">
完成本实验后，你应该已经：
- 使用不同的模型架构（如 ResNet、VGG、DenseNet）进行了迁移攻击实验
- 观察了对抗样本在不同模型之间的迁移成功率
- 实现了集成攻击方法，提高了迁移成功率
- 理解了为什么对抗样本具有跨模型迁移性
- 认识到黑盒攻击在实际场景中的可行性
</Callout>

## 延伸思考

1. 为什么针对某个模型生成的对抗样本能够欺骗其他不同架构的模型？

2. 观察你的实验结果，哪些模型之间的迁移成功率更高？为什么？

3. 集成多个模型生成对抗样本为什么能够提高迁移性？

4. 在真实的黑盒攻击场景中，攻击者如何选择替代模型？

5. 从防御者的角度，了解迁移攻击后你会采取什么措施？