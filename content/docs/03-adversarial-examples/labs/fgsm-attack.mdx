---
title: "实验 3.1：FGSM 攻击"
description: 使用快速梯度符号法生成对抗样本，观察攻击效果
---


import { Callout } from 'fumadocs-ui/components/callout';


<Callout title="预计实验时间" type="info">
预计实验约50分钟
</Callout>

## 实验目标

本实验将帮助你理解 FGSM（快速梯度符号法）的原理，并通过实际操作体验白盒对抗样本攻击的过程。

<Callout title="学习目标" type="info">
完成本实验后，你将能够：
- 理解 FGSM 算法的核心原理和实现步骤
- 使用 PyTorch 实现 FGSM 攻击
- 观察不同扰动强度（ε）对攻击效果的影响
- 可视化对抗样本和扰动
- 分析模型在对抗样本上的行为变化
</Callout>

## 实验前提

<Callout title="环境要求" type="warn">
- Python 3.8+
- PyTorch 1.10+
- torchvision
- matplotlib
- numpy

确保已安装所需依赖后再开始实验。
</Callout>

## 实验内容

:::notebook{file="./lab3_1_fgsm_attack.ipynb" showCellNumbers}
:::

## 实验总结

<Callout title="完成检查" type="success">
完成本实验后，你应该已经：
- 成功实现了 FGSM 攻击算法
- 生成了对图像分类模型的对抗样本
- 观察了不同 ε 值对攻击成功率和图像质量的影响
- 理解了为什么微小的扰动能够欺骗模型
- 可视化了原始图像、扰动和对抗样本的对比
</Callout>

## 延伸思考

1. 当 ε 值增大时，攻击成功率和图像可见性如何变化？如何选择合适的 ε 值？

2. FGSM 是一步攻击方法，如果我们进行多步迭代会怎样？这就是下一个实验要探索的 PGD 攻击。

3. 如果我们想让模型输出特定的错误类别（有目标攻击），应该如何修改 FGSM 算法？