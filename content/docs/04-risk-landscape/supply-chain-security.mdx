---
title: 第4章：供应链安全
description: 认识开源模型和依赖库的安全风险，学习模型卡审计方法
---

import { Callout } from 'fumadocs-ui/components/callout';
import { Tabs, Tab } from 'fumadocs-ui/components/tabs';
import { Accordion, Accordions } from 'fumadocs-ui/components/accordion';
import { Quiz } from '@/components/ui/quiz';

<Callout title="" type="info">
预计阅读约12分钟
</Callout>

## 本章导读

在前三章中，我们讨论了针对 AI 模型的各种攻击：对抗样本欺骗模型判断、隐私泄露提取模型记忆、数据投毒修改模型行为。这些攻击都有一个前提：攻击者需要和模型直接交互。

但还有一类更隐蔽的攻击路径：**不直接攻击模型，而是在模型的来源和依赖中做手脚**。就像不直接攻击你的电脑，而是在你下载的软件安装包中植入恶意代码。

这就是 AI 供应链安全问题。

## 学习目标

<Callout title="本章学完后，你将能够：" type="info">
1. **理解 AI 供应链的概念**：知道 AI 模型的供应链包含哪些环节
2. **识别开源模型的安全风险**：了解从 Hugging Face 等平台下载模型时需要注意什么
3. **了解依赖库的安全风险**：知道 Python 依赖库可能带来的安全隐患
4. **掌握模型卡审计的基本方法**：能够阅读和评估模型卡中的安全相关信息
</Callout>

## 1 AI 供应链概述

### 1.1 什么是 AI 供应链

传统软件有供应链（操作系统→编程语言→框架→库→应用），AI 系统同样有自己的供应链：

```text title="AI 供应链"
预训练模型（如Qwen、LLaMA）
  ↓
微调数据集（如公开数据集、自有数据）
  ↓
训练框架（如PyTorch、Transformers）
  ↓
依赖库（如tokenizers、safetensors）
  ↓
部署环境（如服务器、API网关）
  ↓
最终应用（如AI客服、代码助手）
```

供应链中的每个环节都可能成为攻击目标。一个环节被攻破，整条链路都会受影响。

### 1.2 为什么供应链安全重要

AI 领域的供应链安全问题尤其突出，原因有三：

**原因一：高度依赖开源**

当今的 AI 开发高度依赖开源生态——开源模型（Qwen、LLaMA）、开源框架（PyTorch、Transformers）、开源数据集。这意味着大量组件不在开发者的直接控制之下。

**原因二：模型文件的特殊性**

传统软件依赖（如 npm 包、pip 包）主要是代码，可以通过代码审查发现恶意行为。但 AI 模型文件是二进制的权重数据，人类无法直接阅读和审查。

**原因三：信任链长**

从原始数据到最终应用，经过了多个组织、多个平台、多个工具，任何一个环节被污染都可能影响最终产品。

## 2 开源模型的安全风险

### 2.1 模型托管平台

目前最主要的开源模型托管平台是 Hugging Face，它就像 AI 领域的 GitHub。数以万计的模型被上传和共享，但这也带来了安全问题。

### 2.2 恶意模型的风险

**风险一：模型文件中的恶意代码**

某些模型格式（特别是使用 Python pickle 序列化的格式）允许在模型加载时执行任意代码。

```text title="恶意模型加载风险"
正常流程：
  下载模型 → 加载模型 → 模型推理

如果模型是恶意的：
  下载模型 → 加载模型 → [执行隐藏的恶意代码] → 模型推理
                           ↑
                      可能执行：
                      - 窃取系统文件
                      - 安装后门
                      - 连接远程服务器
```

<Callout title="安全提示" type="warn">
这就是为什么 Hugging Face 推荐使用 safetensors 格式——这种格式只包含张量数据，不能嵌入可执行代码，加载时不会运行任何代码。
</Callout>

**风险二：被篡改的模型权重**

即使模型文件格式安全，模型权重本身也可能被修改。攻击者可以：
- 在模型中植入后门（回忆上一章的内容）
- 降低模型在特定场景下的性能
- 在模型中嵌入偏见

**风险三：冒名模型**

攻击者上传与知名模型名称相似的恶意模型，诱导用户下载。例如：

```text title="冒名模型风险"
正版模型：Qwen/Qwen2-1.5B-Instruct（官方发布）
冒名模型：Qvven/Qwen2-1.5B-Instruct（注意: w→vv）
冒名模型：Qwen/Qwen2-l.5B-Instruct（注意: 1→l）
```

### 2.3 如何识别可信模型

在下载和使用开源模型时，应该检查以下几点：

```text title="模型可信度检查清单"
1. 发布者身份
   - 是否是官方组织发布（如 Qwen、meta-llama）
   - 是否有官方认证标志
   - 发布者的历史记录和声誉

2. 模型文件格式
   - 优先选择 safetensors 格式
   - 避免使用 pickle 格式的不明来源模型

3. 社区验证
   - 下载量和使用量
   - 社区评论和反馈
   - 是否有第三方安全扫描结果

4. 模型卡完整性
   - 是否提供了详细的模型卡
   - 训练数据来源是否透明
   - 已知的限制和风险是否披露
```

## 3 依赖库的安全风险

### 3.1 Python 依赖的信任问题

AI 开发大量使用 Python 生态，一个典型的 AI 项目可能依赖数十个甚至上百个包。每个包都是潜在的攻击面。

**常见攻击方式：**

| 攻击类型 | 说明 | 例子 |
|----------|------|------|
| 供应链劫持 | 攻击者接管了一个流行包的维护权 | 包的原始维护者账号被盗 |
| 名称仿冒 | 上传与流行包名称相似的恶意包 | numpy → nunpy、reqeusts → requests |
| 依赖混淆 | 利用包管理器的解析规则混淆公共包和私有包 | 同名包在 PyPI 和私有源中的优先级问题 |

### 3.2 案例：AI/ML 相关的供应链攻击

近年来已经发生了多起针对 AI/ML 生态的供应链攻击：

```text title="真实案例"
2023年：研究者发现多个PyPI上的恶意包伪装成流行的ML库
  - 这些包在安装时会下载并执行远程恶意脚本
  - 受影响的用户在不知不觉中泄露了系统信息

2024年：某知名开源模型的依赖库被发现存在安全漏洞
  - 漏洞允许攻击者在模型加载时执行任意代码
  - 大量基于该模型构建的应用受到影响
```

### 3.3 依赖安全最佳实践

```text title="依赖安全检查清单"
1. 版本固定
   - 在 requirements.txt 中使用精确版本号
   - 例：transformers==4.40.0（而非 transformers>=4.40）

2. 来源验证
   - 只从官方 PyPI 安装包
   - 检查包的发布者和维护状态

3. 定期更新
   - 关注依赖库的安全公告
   - 及时修复已知漏洞

4. 最小依赖原则
   - 只安装必要的包
   - 避免"全家桶"式安装
```

## 4 模型卡审计

### 4.1 什么是模型卡

模型卡（Model Card）是模型的"说明书"，记录了模型的关键信息。一个完善的模型卡应该包含：

| 信息类别 | 内容 | 安全相关性 |
|----------|------|-----------|
| 模型基本信息 | 名称、版本、作者、许可证 | 确认模型来源和合法性 |
| 训练信息 | 训练数据来源、训练方法 | 评估数据投毒风险 |
| 性能指标 | 各项评测分数 | 验证模型质量 |
| 使用限制 | 已知局限性和不适用场景 | 评估部署风险 |
| 伦理考量 | 偏见分析、公平性评估 | 评估社会影响 |

### 4.2 从安全角度审计模型卡

当评估一个开源模型是否值得信任时，可以从安全角度审查模型卡中的以下要素：

**要素一：训练数据的透明度**

```text title="审计问题"
- 训练数据来源是否清楚说明？
- 数据是否经过清洗和审核？
- 是否包含个人信息或敏感内容？
- 是否有数据使用许可？
```

**要素二：已知风险的披露**

```text title="审计问题"
- 是否说明了模型的已知局限性？
- 是否有关于潜在危害的警告？
- 是否描述了不适合使用的场景？
- 是否进行了对抗性测试？
```

**要素三：安全评估**

```text title="审计问题"
- 是否提供了安全评测结果？
- 是否有关于偏见和公平性的分析？
- 是否描述了减少风险的措施？
```

<Callout title="模型卡 ≠ 安全保证" type="warn">
一个完善的模型卡说明开发者重视透明度和责任，但并不能保证模型没有安全问题。模型卡缺失或信息不完整则是一个明确的风险信号。
</Callout>

### 4.3 实践：读懂模型卡

Hugging Face 上的模型卡通常以 Markdown 格式呈现（README.md），有些还提供结构化的 JSON 元数据。在实验 4.4 中，你将练习审计一个模拟的模型卡 JSON，系统性地检查其中的安全相关信息。

## 本章小结

本章介绍了 AI 供应链安全——一个容易被忽视但影响深远的安全领域。

**AI 供应链**：从预训练模型到部署应用的每个环节都可能成为攻击目标。AI 领域高度依赖开源生态，供应链安全尤为重要。

**开源模型风险**：恶意代码执行（pickle 格式）、模型权重篡改、冒名模型。选择模型时应检查发布者、文件格式、社区验证和模型卡。

**依赖安全**：供应链劫持、名称仿冒、依赖混淆。应实践版本固定、来源验证、定期更新和最小依赖原则。

**模型卡审计**：从安全角度审查模型的训练数据透明度、已知风险披露和安全评估，是评估模型可信度的重要手段。

在实验 4.4 中，你将对一个模拟的模型卡进行安全审计，练习系统性地评估模型的安全风险。

## 课后思考

<Accordions>
  <Accordion title="思考题1：信任的边界">
    你现在使用的 Qwen 模型也是从 Hugging Face 下载的开源模型。基于本章所学，你认为在使用它时应该注意什么？你为什么可以（相对）信任它？
  </Accordion>
  <Accordion title="思考题2：AI 供应链 vs 软件供应链">
    传统软件开发中也有供应链安全问题（如 npm 的 event-stream 事件、Python 的 pip 恶意包事件）。AI 供应链安全与传统软件供应链安全相比，有什么独特的挑战？
  </Accordion>
</Accordions>

## 自测 Quiz

<Quiz questions={[
  {
    question: '为什么 Hugging Face 推荐使用 safetensors 格式而非 pickle 格式？',
    options: [
      { label: 'safetensors 格式的模型更准确' },
      { label: 'safetensors 只包含张量数据，不能嵌入可执行代码，加载时不会运行任何代码', correct: true },
      { label: 'safetensors 格式文件更小' },
      { label: 'pickle 格式已被官方弃用' },
    ],
    explanation: 'pickle 格式允许在模型加载时执行任意代码，攻击者可以在模型文件中隐藏恶意代码。而 safetensors 只包含纯张量数据，从根本上消除了这个风险。',
  },
  {
    question: '以下哪项不属于依赖库安全的最佳实践？',
    options: [
      { label: '在 requirements.txt 中固定精确版本号' },
      { label: '只从官方 PyPI 安装包' },
      { label: '安装尽可能多的包以确保功能完备', correct: true },
      { label: '定期关注依赖库的安全公告' },
    ],
    explanation: '最小依赖原则要求只安装必要的包。每个额外的依赖都是潜在的攻击面，应该避免"全家桶"式安装。',
  },
  {
    question: '模型卡审计中，如果发现模型卡信息不完整或缺失，这意味着什么？',
    options: [
      { label: '模型质量肯定很差' },
      { label: '模型肯定是恶意的' },
      { label: '这是一个明确的风险信号，说明开发者可能不够重视透明度和责任', correct: true },
      { label: '可以忽略，只看性能指标' },
    ],
    explanation: '模型卡缺失不能直接证明模型有问题，但说明开发者对透明度和责任的重视不足，是一个需要警惕的风险信号。',
  },
]} />

## 延伸阅读

- [Hugging Face 模型安全扫描](https://huggingface.co/docs/hub/security)
- [Model Cards for Model Reporting（Google 原始论文）](https://arxiv.org/abs/1810.03993)
- [The Foundation Model Transparency Index](https://crfm.stanford.edu/fmti/)
