---
title: "第4章：后门检测技术"
description: 掌握激活聚类、Neural Cleanse 和 STRIP 等后门检测方法
---

import { Callout } from 'fumadocs-ui/components/callout';
import { Steps, Step } from 'fumadocs-ui/components/steps';
import { Tabs, Tab } from 'fumadocs-ui/components/tabs';
import { Accordion, Accordions } from 'fumadocs-ui/components/accordion';

<Callout title="" type="info">
预计阅读约21分钟
</Callout>

## 本章导读

在前面的章节中，我们学习了后门攻击的原理和实现方式。攻击者可以在模型中植入隐蔽的后门，使其在遇到特定触发器时产生错误行为。这引出了一个关键问题：如何发现模型中是否存在后门？本章将介绍三种主流的后门检测技术，帮助我们理解如何在模型部署前或运行时识别潜在的后门威胁。

## 章节目标

- 理解后门检测面临的核心挑战，认识到为什么这是一个困难的问题
- 掌握激活聚类检测的基本原理，理解如何通过分析模型内部行为发现异常
- 理解 Neural Cleanse 逆向重建触发器的思路，了解如何"猜测"触发器的形态
- 了解 STRIP 运行时检测的工作机制，掌握一种无需了解模型内部结构的检测方法
- 能够根据实际场景选择合适的检测方法，理解各方法的适用条件和局限性

---

## 1. 后门检测的挑战与思路

### 1.1 为什么后门难以发现

在讨论具体的检测方法之前，我们需要先理解一个问题：为什么后门模型如此难以发现？

回顾前面学习的内容，后门攻击的核心特点是"隐蔽性"。一个被植入后门的模型，在正常输入上的表现与干净模型几乎没有区别。只有当输入中包含特定的触发器时，模型才会表现出异常行为。这意味着，如果我们只是用常规的测试数据来评估模型，很可能完全发现不了问题。

我们可以用一个生活中的类比来理解这个困境。想象机场的安检系统：安检人员需要从大量正常行李中识别出可能藏有违禁品的行李。违禁品被精心隐藏，外表看起来与普通物品无异。安检人员不能打开每一件行李仔细检查（成本太高），也不知道违禁品具体长什么样（可能是各种形态）。后门检测面临的正是类似的困境：我们不知道触发器是什么样子，也无法穷举所有可能的输入来测试模型。

### 1.2 检测的基本思路

既然直接寻找触发器很困难，研究者们转换了思路：与其寻找触发器本身，不如寻找后门存在的"痕迹"。就像安检使用 X 光机透视行李内部结构一样，后门检测技术试图通过分析模型的内部行为或输出特征，找到后门存在的间接证据。

目前主流的检测方法可以分为三类：

**离线检测方法**：在模型部署前进行检测，需要访问模型的内部结构（如权重、激活值）。这类方法检测效果通常较好，但需要较高的访问权限。

**在线检测方法**：在模型运行时进行检测，通常只需要观察模型的输入输出。这类方法更加灵活，但可能会增加推理延迟。

**数据层检测方法**：在训练前检测训练数据中是否存在投毒样本。这类方法可以从源头阻止后门植入，但需要在训练阶段介入。

有同学可能会问：既然后门模型在正常测试中表现正常，那检测方法是如何发现问题的？关键在于，虽然后门模型的输出看起来正常，但其内部的计算过程可能存在异常。接下来我们将详细介绍三种代表性的检测技术，看看它们是如何利用这些"蛛丝马迹"来发现后门的。

---

## 2. 激活聚类：分析模型的内部行为

### 2.1 核心思想

**激活聚类（Activation Clustering）** 是一种离线检测方法，其核心思想是：后门样本和正常样本在模型内部会产生不同的激活模式，通过聚类分析可以将它们区分开来。

为了理解这个方法，我们需要先了解什么是"激活值"。当一张图片输入神经网络后，网络的每一层都会产生一组数值，这些数值反映了该层神经元对输入的响应程度，我们称之为**激活值（Activation）**。可以把激活值想象成模型"看到"输入后产生的"印象"——不同的输入会产生不同的印象。

激活聚类方法的假设是：同一类别的正常样本，它们在模型中产生的激活值应该是相似的；而带有触发器的后门样本，虽然被模型分类到同一类别，但其激活值可能与正常样本存在明显差异。

### 2.2 检测流程

激活聚类的检测流程可以分为以下几个步骤：

**第一步：收集激活值**。将训练数据或测试数据输入模型，记录模型某一层（通常是倒数第二层）的激活值。每个样本都会产生一个激活向量。

**第二步：降维处理**。由于激活向量的维度通常很高（可能有数百甚至数千维），直接分析比较困难。因此需要使用降维技术（如 PCA 或 t-SNE）将高维激活向量投影到低维空间，便于可视化和分析。

**第三步：聚类分析**。对降维后的激活向量进行聚类。如果模型是干净的，同一类别的样本应该聚成一个簇；如果模型存在后门，同一类别的样本可能会分成两个簇——一个是正常样本簇，另一个是后门样本簇。

**第四步：异常判断**。如果某个类别的样本明显分成了两个或多个簇，且其中一个簇的样本数量明显较少，那么这个小簇中的样本很可能就是后门样本。

### 2.3 方法的优势与局限

激活聚类方法的优势在于：它不需要知道触发器的具体形态，只需要分析模型的内部行为就能发现异常。这使得它可以检测各种类型的后门攻击。

然而，这个方法也存在一些局限性。首先，它需要访问模型的内部结构，这在某些场景下可能不可行（比如使用第三方提供的模型 API）。其次，如果攻击者精心设计后门，使得后门样本的激活值与正常样本非常接近，那么聚类方法可能无法有效区分。此外，当投毒比例很低时，后门样本簇可能太小而难以被识别。

了解了如何通过分析模型内部行为来检测后门，接下来我们将介绍一种更加主动的方法：尝试逆向重建触发器本身。

---

## 3. Neural Cleanse：逆向重建触发器

### 3.1 核心思想

**Neural Cleanse** 是 2019 年提出的一种后门检测方法，其思路非常巧妙：既然我们不知道触发器是什么样子，那就让模型"告诉"我们。

具体来说，Neural Cleanse 尝试为每个类别找到一个"最小扰动"——添加这个扰动后，任意输入都会被模型分类到该类别。如果模型是干净的，要让任意输入都被分类到某个特定类别，需要添加的扰动应该比较大；但如果模型存在后门，那么对于目标类别，只需要添加一个很小的扰动（即触发器）就能达到目的。

这个思路可以用一个比喻来理解：假设有一把锁，正常情况下需要正确的钥匙才能打开。但如果这把锁被动过手脚（植入了后门），那么可能存在一把"万能钥匙"可以轻松打开它。Neural Cleanse 的工作就是尝试为每个"锁孔"（类别）找到可能的"万能钥匙"（触发器），如果某个锁孔特别容易被打开，那就说明它可能被动过手脚。

### 3.2 检测流程

Neural Cleanse 的检测流程如下：

**第一步：定义优化目标**。对于每个类别，我们希望找到一个扰动（掩码和图案的组合），使得添加这个扰动后，任意输入都能被分类到该类别，同时扰动的大小尽可能小。

**第二步：逆向优化**。使用梯度下降等优化方法，为每个类别求解最优的扰动。这个过程类似于训练神经网络，但优化的对象不是模型参数，而是扰动本身。

**第三步：异常检测**。计算每个类别对应扰动的大小（通常用 L1 范数衡量）。如果某个类别的扰动明显小于其他类别，那么这个类别很可能是后门的目标类别，对应的扰动就是重建出的触发器。

**第四步：量化判断**。为了客观判断是否存在后门，Neural Cleanse 引入了**异常指数（Anomaly Index）** 的概念。计算所有类别扰动大小的中位数和绝对中位差，如果某个类别的扰动大小偏离中位数超过一定阈值（通常设为 2），则判定存在后门。

### 3.3 一个具体的例子

为了更好地理解 Neural Cleanse 的工作原理，我们来看一个具体的场景。

假设有一个手写数字识别模型，攻击者在其中植入了后门：当图片右下角出现一个白色小方块时，模型会将任何数字都识别为"7"。

当我们对这个模型运行 Neural Cleanse 时，会发生什么？

对于类别"0"到"6"和"8"、"9"，要让任意输入都被分类到这些类别，需要添加较大的扰动（因为需要"欺骗"模型改变其正常判断）。但对于类别"7"，由于后门的存在，只需要在右下角添加一个小白块，任意输入就会被分类为"7"。因此，类别"7"对应的扰动会明显小于其他类别。

通过比较各类别扰动的大小，Neural Cleanse 可以识别出"7"是后门目标类别，并且重建出的扰动（右下角的小白块）就是触发器的近似形态。

### 3.4 方法的优势与局限

Neural Cleanse 的优势在于：它不仅能检测后门是否存在，还能重建出触发器的大致形态，这对于理解攻击和设计防御都很有价值。

但这个方法也有局限性。首先，它假设触发器是一个固定的图案，对于更复杂的触发器（如动态触发器、分布式触发器）可能效果不佳。其次，优化过程需要大量计算，对于大型模型可能比较耗时。此外，如果攻击者知道防御方会使用 Neural Cleanse，可能会设计出能够规避检测的后门。

前面介绍的两种方法都属于离线检测，需要在模型部署前进行。接下来我们将介绍一种可以在模型运行时进行的检测方法。

---

## 4. STRIP：运行时检测

### 4.1 核心思想

**STRIP（STRong Intentional Perturbation）** 是一种运行时检测方法，其核心思想基于一个观察：正常输入和后门输入对扰动的敏感程度不同。

对于正常输入，如果我们对其添加一些随机扰动（比如叠加其他图片），模型的预测结果通常会发生变化，因为扰动改变了输入的特征。但对于后门输入，由于触发器的存在，模型会"顽固地"给出目标类别的预测，即使添加了扰动也不容易改变。

我们可以用一个比喻来理解：正常的判断是基于输入的整体特征，就像我们根据一个人的长相来判断身份，如果长相被遮挡或改变，判断就会受影响。但后门判断是基于触发器这个"暗号"，只要暗号还在，无论其他部分怎么变化，判断都不会改变。

### 4.2 检测流程

STRIP 的检测流程如下：

**第一步：准备扰动样本**。收集一组干净的图片作为扰动源。

**第二步：生成叠加样本**。对于每个待检测的输入，将其与多个扰动源图片进行叠加，生成一组叠加样本。叠加的方式通常是简单的像素混合。

**第三步：观察预测分布**。将所有叠加样本输入模型，记录模型对每个样本的预测结果。

**第四步：计算熵值**。统计预测结果的分布，计算其**熵（Entropy）**。熵是衡量不确定性的指标：如果预测结果分散在多个类别，熵值较高；如果预测结果集中在某一个类别，熵值较低。

**第五步：判断是否为后门输入**。如果熵值低于设定的阈值，说明模型对该输入的预测非常"顽固"，不受扰动影响，很可能是后门输入；如果熵值较高，说明是正常输入。

### 4.3 为什么这个方法有效

有同学可能会问：为什么后门输入叠加扰动后，预测结果还是集中在目标类别？

这与后门的工作机制有关。在后门攻击中，模型学会了一个"捷径"：只要看到触发器，就输出目标类别，而忽略输入的其他部分。当我们将后门输入与其他图片叠加时，触发器仍然存在于叠加后的图片中，因此模型仍然会被触发器"劫持"，输出目标类别。

相比之下，正常输入的分类依赖于输入的整体特征。当与其他图片叠加后，原有的特征被干扰，模型的预测自然会发生变化。

### 4.4 方法的优势与局限

STRIP 的优势在于：它不需要访问模型的内部结构，只需要观察模型的输入输出，因此可以用于检测黑盒模型。此外，它可以在运行时实时检测每个输入，适合部署在生产环境中。

但 STRIP 也有局限性。首先，它会增加推理延迟，因为每个输入都需要生成多个叠加样本并进行预测。其次，阈值的设定需要根据具体场景调整，设置不当可能导致误报或漏报。此外，如果攻击者设计的触发器在叠加后容易被破坏，STRIP 可能无法有效检测。

---

## 5. 检测方法的对比与选择

### 5.1 三种方法的对比

我们已经学习了三种后门检测方法，下面通过一个表格来对比它们的特点：

| 特性 | 激活聚类 | Neural Cleanse | STRIP |
|------|----------|----------------|-------|
| 检测时机 | 离线（部署前） | 离线（部署前） | 在线（运行时） |
| 访问需求 | 需要模型内部结构 | 需要模型内部结构 | 仅需输入输出 |
| 能否重建触发器 | 否 | 是 | 否 |
| 计算开销 | 中等 | 较高 | 每次推理增加开销 |
| 适用场景 | 有训练数据访问权限 | 需要了解触发器形态 | 生产环境实时检测 |

### 5.2 如何选择检测方法

在实际应用中，选择哪种检测方法取决于具体的场景和需求：

**场景一：部署前审查第三方模型**。如果你从外部获取了一个预训练模型，想在部署前检查是否存在后门，可以优先考虑 Neural Cleanse。它不仅能检测后门，还能重建触发器，帮助你了解潜在的攻击方式。

**场景二：检测训练数据是否被投毒**。如果你怀疑训练数据中混入了后门样本，可以使用激活聚类方法。通过分析训练样本的激活值分布，可以识别出异常的样本簇。

**场景三：生产环境实时防护**。如果模型已经部署，需要在运行时检测恶意输入，STRIP 是一个合适的选择。它可以在不了解模型内部结构的情况下，实时判断每个输入是否可能是后门触发。

**场景四：综合防护**。在安全要求较高的场景下，建议组合使用多种方法：部署前使用 Neural Cleanse 和激活聚类进行审查，部署后使用 STRIP 进行运行时监控，形成多层防护体系。

### 5.3 检测技术的局限性

需要指出的是，目前的后门检测技术仍然存在局限性，无法保证检测出所有类型的后门：

**自适应攻击**：如果攻击者了解防御方使用的检测方法，可能会设计出能够规避检测的后门。例如，针对 Neural Cleanse，攻击者可以设计需要较大扰动才能触发的后门；针对 STRIP，攻击者可以设计对叠加敏感的触发器。

**新型后门**：随着后门攻击技术的发展，出现了许多新型后门（如干净标签后门、动态触发器后门），现有的检测方法可能无法有效应对。

**检测与攻击的博弈**：后门检测与后门攻击是一个持续博弈的过程。每当出现新的检测方法，攻击者就会尝试设计能够绕过检测的新攻击；而新攻击的出现又会推动检测技术的进步。

---

## 本章小结

本章介绍了三种主流的后门检测技术：

1. **激活聚类**通过分析模型内部的激活值分布，识别出行为异常的样本簇，从而发现后门样本。这种方法不需要知道触发器的形态，但需要访问模型内部结构。

2. **Neural Cleanse**通过逆向优化的方式，尝试为每个类别重建可能的触发器。如果某个类别对应的触发器明显小于其他类别，则说明该类别可能是后门目标。这种方法不仅能检测后门，还能重建触发器形态。

3. **STRIP**利用后门输入对扰动不敏感的特点，通过观察叠加样本的预测分布来判断输入是否为后门触发。这种方法可以在运行时实时检测，适合部署在生产环境。

每种方法都有其适用场景和局限性，在实际应用中需要根据具体需求选择合适的方法，或组合使用多种方法形成多层防护。同时，我们也需要认识到，后门检测与后门攻击是一个持续博弈的过程，没有一种方法能够保证检测出所有类型的后门。

---

## 教学资源

### 图表与示意图

1. **激活聚类可视化图**：展示正常样本和后门样本在激活空间中的分布差异
2. **Neural Cleanse 工作流程图**：展示逆向优化重建触发器的过程
3. **STRIP 检测流程图**：展示输入叠加、预测分布统计、熵值计算的完整流程
4. **三种方法对比图**：用表格或流程图对比三种方法的特点和适用场景

### 配套实验

本章内容对应实验 5.3：后门检测实验。在实验中，学生将：
- 使用激活聚类方法分析后门模型的激活值分布
- 观察正常样本和后门样本在降维后的空间分布差异
- 体验 STRIP 方法的检测过程

### 延伸阅读

- Wang et al., "Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks", IEEE S&P 2019
- Gao et al., "STRIP: A Defence Against Trojan Attacks on Deep Neural Networks", ACSAC 2019
- Chen et al., "Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering", AAAI Workshop 2019

---

## 课后思考题

1. **理解性问题**：激活聚类方法为什么能够区分正常样本和后门样本？如果攻击者想要规避这种检测，可能会采取什么策略？

2. **分析性问题**：Neural Cleanse 假设触发器是一个固定的小图案。如果攻击者使用的触发器是整张图片的颜色滤镜（比如将图片整体变成偏蓝色），Neural Cleanse 还能有效检测吗？为什么？

3. **应用性问题**：假设你负责一个人脸识别门禁系统的安全审计，需要检查系统使用的模型是否存在后门。你会选择哪种或哪几种检测方法？请说明你的选择理由和具体的检测方案。