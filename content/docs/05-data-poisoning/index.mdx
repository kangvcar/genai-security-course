---
title: 数据投毒总览
description: 深入理解数据投毒攻击原理和后门检测技术
---

import { Cards, Card } from 'fumadocs-ui/components/card';
import { Callout } from 'fumadocs-ui/components/callout';
import { Accordion, Accordions } from 'fumadocs-ui/components/accordion';
import { Biohazard, Skull, Package, Search, ShieldCheck } from 'lucide-react';

<Callout title="预计学习时间" type="info">
预计阅读约3-4小时，实验约2-3小时
</Callout>

在前面的模块中，我们学习了针对已部署模型的攻击技术——提示词注入和对抗样本都发生在模型推理阶段。然而，还有一类更加隐蔽、影响更加深远的攻击方式：它不是在模型使用时发起攻击，而是在模型训练之前就已经埋下隐患。这就是**数据投毒攻击（Data Poisoning Attack）**。

数据投毒就像是在教科书印刷之前篡改内容——学生拿到的教科书看起来完全正常，但里面的某些知识点已经被悄悄改错了。一旦训练数据被污染，其影响将伴随模型的整个生命周期。

## 学习目标

<Callout title="完成本模块后，你将能够：" type="success">
- 理解数据投毒攻击的基本原理，区分标签翻转和干净标签攻击
- 掌握后门攻击的核心概念，了解触发器设计和 BadNets 攻击方法
- 认识 AI 供应链的安全风险，包括模型仓库投毒和 Pickle 漏洞
- 掌握后门检测技术，包括激活聚类、Neural Cleanse 和 STRIP
- 建立完整的防御思维，从数据、训练、模型、供应链四个层面保护 AI 系统
</Callout>

## 章节概览

<Cards>
  <Card icon={<Biohazard />} title="第1章：数据投毒攻击原理" href="/docs/05-data-poisoning/poisoning-principles">
    理解标签翻转攻击和干净标签攻击的原理与危害
  </Card>
  <Card icon={<Skull />} title="第2章：后门攻击技术" href="/docs/05-data-poisoning/backdoor-attacks">
    深入学习触发器设计和 BadNets 等经典后门攻击方法
  </Card>
  <Card icon={<Package />} title="第3章：供应链攻击向量" href="/docs/05-data-poisoning/supply-chain-attacks">
    探索模型仓库投毒、Pickle 漏洞和数据集投毒的威胁
  </Card>
  <Card icon={<Search />} title="第4章：后门检测技术" href="/docs/05-data-poisoning/backdoor-detection">
    掌握激活聚类、Neural Cleanse 和 STRIP 等检测方法
  </Card>
  <Card icon={<ShieldCheck />} title="第5章：防御与缓解措施" href="/docs/05-data-poisoning/defense-mitigation">
    学习数据清洗、鲁棒训练、后门移除和供应链安全实践
  </Card>
</Cards>

## 配套实验

<Cards>
  <Card title="实验 5.1：标签翻转攻击" href="/docs/05-data-poisoning/labs/label-flipping">
    实践标签翻转投毒攻击，观察不同投毒比例的影响
  </Card>
  <Card title="实验 5.2：后门攻击" href="/docs/05-data-poisoning/labs/backdoor-attack">
    实现 BadNets 后门攻击，理解触发器植入过程
  </Card>
  <Card title="实验 5.3：后门检测" href="/docs/05-data-poisoning/labs/backdoor-detection">
    使用激活聚类等方法检测模型中的后门
  </Card>
</Cards>

<Callout title="安全提示" type="warn">
本模块介绍的攻击技术仅供学习和研究目的。请在合法和授权的环境中进行实验，不要将这些技术用于未经授权的系统或恶意用途。
</Callout>

## 常见问题

<Accordions>
  <Accordion title="数据投毒和对抗样本有什么区别？">
    对抗样本是在模型推理阶段进行的攻击，每次攻击只影响单个输入；数据投毒是在模型训练阶段进行的攻击，其影响会固化到模型参数中，影响模型的整个生命周期。
  </Accordion>
  <Accordion title="后门攻击为什么更难检测？">
    后门攻击的模型在正常输入上表现完全正常，只有当输入中包含特定触发器时才会产生异常行为。由于检测者不知道触发器的具体形态，常规测试很难发现问题。
  </Accordion>
  <Accordion title="什么是 AI 供应链攻击？">
    AI 供应链攻击是指攻击者通过污染开源模型、数据集或依赖库等共享资源，影响所有使用这些资源的下游用户。这种攻击具有“放大效应”，一次攻击可能影响成千上万个项目。
  </Accordion>
  <Accordion title="如何保护模型免受数据投毒？">
    主要防御措施包括：1) 数据清洗和来源验证；2) 鲁棒训练技术；3) 后门检测方法；4) Fine-Pruning 和模型蒸馏等后门移除技术；5) 供应链安全实践。
  </Accordion>
</Accordions>
