---
title: "第3章：供应链攻击向量"
description: 探索 AI 供应链的安全风险，包括模型仓库投毒、Pickle 漏洞和数据集投毒
---

import { Callout } from 'fumadocs-ui/components/callout';
import { Steps, Step } from 'fumadocs-ui/components/steps';
import { Tabs, Tab } from 'fumadocs-ui/components/tabs';

## 本章导读

在前两章中，我们学习了数据投毒和后门攻击的原理。这些攻击通常针对单个模型或数据集。然而，现代 AI 开发高度依赖开源生态：开发者从 HuggingFace 下载预训练模型，使用 PyPI 安装依赖库，从公开数据集获取训练数据。如果攻击者能够污染这些共享资源，一次攻击就可能影响成千上万的下游用户。本章将探讨 AI 供应链的构成、脆弱环节，以及攻击者如何利用这些环节发起攻击。

<Callout title="学习目标" type="info">
完成本章学习后，你将能够：
- 理解 AI 供应链的构成，识别各环节的安全风险
- 掌握模型仓库投毒的常见手法，能够识别可疑模型
- 理解序列化攻击（Pickle 漏洞）的原理，了解安全替代方案
- 认识数据集投毒的风险，掌握基本的防御策略
</Callout>

---

## 1. AI 供应链：从数据到部署的信任链条

### 1.1 什么是 AI 供应链

**AI 供应链（AI Supply Chain）** 是指从数据收集到模型部署的整个流程中涉及的所有组件和依赖关系：

```
数据来源 → 数据处理 → 模型架构 → 预训练模型 → 训练代码 → 部署环境
    ↓          ↓          ↓           ↓           ↓          ↓
  公开数据集   第三方库    开源模型    模型仓库     依赖包     云服务
```

每个环节都可能引入外部依赖，而每个外部依赖都是潜在的攻击入口。

### 1.2 供应链的脆弱环节

<Callout title="类比：餐厅的食品供应链" type="info">
想象你在一家餐厅用餐。从农场到餐桌，食材经过了多个环节：

- **食材来源**：蔬菜可能在农场就被农药污染（对应 AI 中的数据集）
- **调料供应**：调料可能是假冒伪劣产品（对应依赖库）
- **厨房设备**：烹饪设备可能被动过手脚（对应训练环境）
- **成品运输**：菜品可能在送餐途中被替换（对应模型分发）

任何一个环节出问题，最终端上桌的"菜品"都可能有问题。
</Callout>

| 环节 | 风险来源 | 攻击示例 |
|-----|---------|---------|
| **数据集** | 公开数据集被污染 | 维基百科内容被恶意编辑 |
| **预训练模型** | 模型仓库中的恶意模型 | HuggingFace 上的后门模型 |
| **依赖库** | 第三方包被攻击 | PyTorch torchtriton 事件 |
| **训练环境** | 开发环境被入侵 | CI/CD 流程被篡改 |

### 1.3 供应链攻击的放大效应

供应链攻击之所以危险，在于其"一次投入，多次收益"的特性：

```
攻击者污染一个流行的预训练模型
              ↓
    1000 个项目下载使用该模型
              ↓
    1000 个应用程序被植入后门
              ↓
    百万级终端用户受到影响
```

这种放大效应使得供应链攻击成为攻击者的"高性价比"选择。

---

## 2. 模型仓库投毒：开源生态中的信任危机

### 2.1 模型仓库的双刃剑效应

HuggingFace、ModelZoo 等模型仓库极大地推动了 AI 技术的普及，但开放性也带来了安全风险：

| 风险类型 | 说明 | 危害程度 |
|---------|------|---------|
| **恶意模型** | 模型本身包含后门 | 高 |
| **恶意代码** | 模型文件中嵌入可执行代码 | 极高 |
| **版本替换** | 将流行模型的某个版本替换为恶意版本 | 高 |
| **命名欺骗** | 使用与官方模型相似的名称 | 中 |

### 2.2 命名欺骗：最简单却有效的攻击

命名欺骗（Typosquatting）是一种低技术门槛但高成功率的攻击方式：

```
官方模型：facebook/bart-large
恶意模型：faceb00k/bart-large    （用数字 0 替换字母 o）
         facebook-ai/bart-large  （添加后缀）
         official-bart-large     （添加"官方"前缀）
```

### 2.3 案例：HuggingFace 恶意模型事件（2023年）

<Callout title="案例分析" type="warn">
**背景**：2023 年，安全研究人员在 HuggingFace 平台上发现了多个包含恶意代码的模型。

**攻击手法**：攻击者利用 Python 的 Pickle 序列化机制在模型文件中嵌入恶意代码。当用户下载并加载这些模型时，恶意代码会自动执行。

**恶意行为**：
- 窃取环境变量（API 密钥、数据库密码等）
- 建立反向连接，远程控制受害者机器
- 下载并执行更多恶意代码

**影响**：部分恶意模型在被发现前已被下载数千次。
</Callout>

### 2.4 防御模型仓库投毒

| 防御措施 | 具体做法 |
|---------|---------|
| **验证来源** | 只从官方账号或经过验证的组织下载模型 |
| **检查校验和** | 验证下载文件的 SHA256 哈希值 |
| **代码审计** | 在加载模型前检查模型文件的结构和内容 |
| **隔离环境** | 在沙箱或容器中加载未知来源的模型 |
| **使用安全格式** | 优先选择 SafeTensors 等不支持代码执行的格式 |

---

## 3. 序列化攻击：Pickle 漏洞详解

### 3.1 什么是序列化

**序列化（Serialization）** 是将程序中的对象转换为可存储或传输格式的过程。Python 中最常用的序列化工具是 Pickle：

```python
import pickle

# 序列化：将对象保存到文件
with open('model.pkl', 'wb') as f:
    pickle.dump(model, f)

# 反序列化：从文件加载对象
with open('model.pkl', 'rb') as f:
    model = pickle.load(f)
```

问题在于：**Pickle 在反序列化时可以执行任意代码**。

### 3.2 Pickle 漏洞的原理

Pickle 协议支持一个特殊的方法 `__reduce__`，该方法可以指定对象在反序列化时如何重建。攻击者可以利用这一机制执行恶意代码：

```python
import pickle
import os

class MaliciousModel:
    def __reduce__(self):
        # 当对象被反序列化时，执行 os.system 命令
        return (os.system, ('echo "恶意代码被执行！"',))

# 攻击者创建恶意 pickle 文件
with open('malicious.pkl', 'wb') as f:
    pickle.dump(MaliciousModel(), f)

# 当受害者加载这个文件时...
with open('malicious.pkl', 'rb') as f:
    obj = pickle.load(f)  # 恶意命令在此刻被执行！
```

<Callout title="安全警告" type="error">
实际攻击中，恶意代码可能执行更危险的操作：窃取敏感文件、建立远程连接、下载并运行恶意程序等。**永远不要加载来源不明的 Pickle 文件！**
</Callout>

### 3.3 案例：PyTorch torchtriton 供应链攻击（2022年）

<Callout title="案例分析" type="warn">
**背景**：2022 年 12 月，PyTorch 官方披露了一起严重的供应链攻击事件。

**攻击过程**：
1. 攻击者发现 PyTorch 的依赖列表中包含 `torchtriton` 包
2. 该包在 PyPI 上尚未被注册
3. 攻击者抢先注册并上传了包含恶意代码的版本
4. 用户安装 PyTorch 夜间版本时，恶意包被自动安装

**恶意行为**：
- 收集系统信息（主机名、用户名、当前目录）
- 窃取敏感文件（SSH 私钥、AWS 凭证、环境变量）
- 将信息上传到攻击者服务器

**影响**：恶意包被下载约 2500 次。
</Callout>

### 3.4 SafeTensors：安全的替代方案

HuggingFace 开发了 **SafeTensors** 格式作为安全替代方案，设计原则是"只存储数据，不执行代码"：

| 对比项 | Pickle | SafeTensors |
|-------|--------|-------------|
| **代码执行** | 支持（危险） | 不支持（安全） |
| **格式透明度** | 复杂、不透明 | 简单、可审计 |
| **安全性** | 低 | 高 |
| **加载性能** | 一般 | 更快（支持内存映射） |

```python
from safetensors.torch import save_file, load_file

# 保存模型（安全）
save_file(model.state_dict(), "model.safetensors")

# 加载模型（安全）
state_dict = load_file("model.safetensors")
model.load_state_dict(state_dict)
```

---

## 4. 数据集投毒：污染源头的攻击

### 4.1 公开数据集的信任问题

常见的数据集污染方式：

| 数据集类型 | 污染方式 | 示例 |
|-----------|---------|------|
| **网页爬取数据** | 攻击者控制部分被爬取的网页 | Common Crawl 数据集 |
| **众包标注数据** | 恶意标注员故意提供错误标签 | ImageNet 标注 |
| **用户上传数据** | 恶意用户上传投毒样本 | 开源数据集贡献 |
| **知识库数据** | 编辑虚假或恶意内容 | 维基百科投毒 |

### 4.2 案例：维基百科投毒实验（2020年）

<Callout title="案例分析" type="info">
**背景**：许多问答系统和知识图谱使用维基百科作为知识来源。

**实验发现**：
- 虚假信息可以在几小时内被数据爬虫收集
- 基于维基百科的问答系统会将虚假信息作为答案返回
- 维基百科的"权威性"反而成为攻击的放大器

**启示**：训练数据的质量直接影响模型的行为。对于依赖公开数据的 AI 系统，数据验证和来源追踪至关重要。
</Callout>

### 4.3 防御数据集投毒

| 防御措施 | 具体做法 |
|---------|---------|
| **数据验证** | 检查数据的一致性、完整性和统计特征 |
| **多源交叉验证** | 使用多个独立数据源，交叉验证信息准确性 |
| **异常检测** | 识别统计特征异常的样本 |
| **人工抽检** | 随机抽取样本进行人工审核 |
| **来源追踪** | 记录每条数据的来源，便于问题溯源 |

---

## 本章小结

| 概念 | 要点 |
|-----|------|
| **供应链攻击的放大效应** | 一次成功的攻击可以影响所有使用被污染组件的下游系统 |
| **模型仓库投毒** | 通过上传恶意模型、命名欺骗、版本替换等方式植入恶意内容 |
| **Pickle 序列化漏洞** | 反序列化时可以执行任意代码，SafeTensors 是更安全的替代方案 |
| **数据集投毒** | 公开数据集可能被恶意篡改，需要建立数据验证和来源追踪机制 |

---

## 术语对照表

| 中文术语 | 英文术语 | 简要解释 |
|---------|---------|---------|
| AI 供应链 | AI Supply Chain | 从数据收集到模型部署的完整流程 |
| 命名欺骗 | Typosquatting | 使用与官方相似的名称诱导用户下载恶意内容 |
| 序列化 | Serialization | 将对象转换为可存储或传输格式的过程 |
| 反序列化 | Deserialization | 将存储格式还原为程序对象的过程 |
| SafeTensors | SafeTensors | HuggingFace 开发的安全模型存储格式 |

---

## 课后思考题

1. **理解性问题**：为什么供应链攻击具有"放大效应"？请结合本章的食品供应链类比进行解释。

2. **分析性问题**：Pickle 漏洞的根本原因是什么？SafeTensors 是如何从设计层面解决这个问题的？

3. **应用性问题**：假设你是一家 AI 公司的安全工程师，公司计划从 HuggingFace 下载一个预训练模型用于生产环境。请设计一套安全审查流程。
