---
title: "第3章：供应链攻击向量"
description: 探索 AI 供应链的安全风险，包括模型仓库投毒、Pickle 漏洞和数据集投毒
---

import { Callout } from 'fumadocs-ui/components/callout';
import { Steps, Step } from 'fumadocs-ui/components/steps';
import { Tabs, Tab } from 'fumadocs-ui/components/tabs';
import { Accordion, Accordions } from 'fumadocs-ui/components/accordion';

<Callout title="" type="info">
预计阅读约17分钟
</Callout>

## 本章导读

在前两章中，我们学习了数据投毒和后门攻击的原理。这些攻击通常针对单个模型或数据集。然而，现代 AI 开发高度依赖开源生态：开发者从 HuggingFace 下载预训练模型，使用 PyPI 安装依赖库，从公开数据集获取训练数据。如果攻击者能够污染这些共享资源，一次攻击就可能影响成千上万的下游用户。本章将探讨 AI 供应链的构成、脆弱环节，以及攻击者如何利用这些环节发起攻击。

## 章节目标

- 理解 AI 供应链的构成，识别各环节的安全风险
- 掌握模型仓库投毒的常见手法，能够识别可疑模型
- 理解序列化攻击（Pickle 漏洞）的原理，了解安全替代方案
- 认识数据集投毒的风险，掌握基本的防御策略

---

## 1. AI 供应链：从数据到部署的信任链条

### 1.1 什么是 AI 供应链

在传统软件开发中，我们已经熟悉"供应链"的概念：一个应用程序依赖各种第三方库，这些库又依赖其他库，形成复杂的依赖关系。AI 系统的供应链更加复杂，因为除了代码依赖，还涉及数据和模型。

**AI 供应链（AI Supply Chain）** 是指从数据收集到模型部署的整个流程中涉及的所有组件和依赖关系。我们可以将其分为以下几个环节：

```
数据来源 → 数据处理 → 模型架构 → 预训练模型 → 训练代码 → 部署环境
    ↓          ↓          ↓           ↓           ↓          ↓
  公开数据集   第三方库    开源模型    模型仓库     依赖包     云服务
```

每个环节都可能引入外部依赖，而每个外部依赖都是潜在的攻击入口。

### 1.2 供应链的脆弱环节

为了更直观地理解供应链的风险，我们可以用一个生活化的类比。

**类比：餐厅的食品供应链**

想象你在一家餐厅用餐。从农场到餐桌，食材经过了多个环节：

- **食材来源**：蔬菜可能在农场就被农药污染（对应 AI 中的数据集）
- **调料供应**：调料可能是假冒伪劣产品（对应依赖库）
- **厨房设备**：烹饪设备可能被动过手脚（对应训练环境）
- **成品运输**：菜品可能在送餐途中被替换（对应模型分发）

任何一个环节出问题，最终端上桌的"菜品"都可能有问题。更关键的是，如果一家大型食材供应商被污染，所有使用该供应商的餐厅都会受到影响——这就是供应链攻击的"放大效应"。

回到 AI 领域，下表总结了各环节的风险来源：

| 环节 | 风险来源 | 攻击示例 |
|-----|---------|---------|
| **数据集** | 公开数据集被污染 | 维基百科内容被恶意编辑 |
| **预训练模型** | 模型仓库中的恶意模型 | HuggingFace 上的后门模型 |
| **依赖库** | 第三方包被攻击 | PyTorch torchtriton 事件 |
| **训练环境** | 开发环境被入侵 | CI/CD 流程被篡改 |

### 1.3 供应链攻击的放大效应

供应链攻击之所以危险，在于其"一次投入，多次收益"的特性。

传统的攻击方式需要逐个攻击目标系统，效率较低。而供应链攻击则不同：攻击者只需污染一个被广泛使用的组件，就能同时影响所有使用该组件的下游系统。

```
攻击者污染一个流行的预训练模型
              ↓
    1000 个项目下载使用该模型
              ↓
    1000 个应用程序被植入后门
              ↓
    百万级终端用户受到影响
```

这种放大效应使得供应链攻击成为攻击者的"高性价比"选择。对于防御者而言，这意味着我们不仅要保护自己的代码，还要审慎对待每一个外部依赖。

有同学可能会问：既然供应链攻击这么危险，为什么我们还要使用开源组件？答案是：完全自主开发所有组件既不现实也不经济。正确的做法是理解风险、建立防御机制，而非因噎废食。

了解了供应链攻击的整体图景后，接下来我们将深入探讨最常见的攻击向量之一：模型仓库投毒。

---

## 2. 模型仓库投毒：开源生态中的信任危机

### 2.1 模型仓库的双刃剑效应

HuggingFace、ModelZoo 等模型仓库极大地推动了 AI 技术的普及。开发者可以轻松下载预训练模型，在此基础上进行微调，而无需从头训练。然而，这种便利性也带来了安全风险。

模型仓库通常允许任何人上传模型，审核机制相对宽松。这为攻击者提供了可乘之机。常见的攻击方式包括：

| 风险类型 | 说明 | 危害程度 |
|---------|------|---------|
| **恶意模型** | 模型本身包含后门，在特定输入下产生恶意输出 | 高 |
| **恶意代码** | 模型文件中嵌入可执行代码，加载时自动运行 | 极高 |
| **版本替换** | 将流行模型的某个版本替换为恶意版本 | 高 |
| **命名欺骗** | 使用与官方模型相似的名称，诱导用户下载 | 中 |

### 2.2 命名欺骗：最简单却有效的攻击

命名欺骗是一种低技术门槛但高成功率的攻击方式。攻击者创建与官方模型名称相似的恶意模型，利用用户的疏忽进行攻击。

```
官方模型：facebook/bart-large
恶意模型：faceb00k/bart-large    （用数字 0 替换字母 o）
         facebook-ai/bart-large  （添加后缀）
         official-bart-large     （添加"官方"前缀）
```

不仔细检查的用户可能下载到恶意版本。这种攻击在软件包管理领域也很常见，被称为**域名抢注（Typosquatting）**。

### 2.3 案例：HuggingFace 恶意模型事件（2023年）

2023 年，安全研究人员在 HuggingFace 平台上发现了多个包含恶意代码的模型，这一事件引起了 AI 安全社区的广泛关注。

**背景**

HuggingFace 是全球最大的 AI 模型共享平台，托管了数十万个模型。平台的开放性使其成为 AI 开发者的首选资源，但也为恶意行为者提供了可乘之机。

**攻击过程**

攻击者利用 Python 的 Pickle 序列化机制（我们将在下一节详细讨论）在模型文件中嵌入恶意代码。当用户下载并加载这些模型时，恶意代码会自动执行。

具体的恶意行为包括：
- 窃取环境变量（可能包含 API 密钥、数据库密码等敏感信息）
- 建立反向连接，使攻击者能够远程控制受害者的机器
- 下载并执行更多恶意代码，进一步扩大攻击范围

**影响与后果**

部分恶意模型在被发现前已被下载数千次。由于模型加载通常在开发环境中进行，而开发环境往往拥有较高的系统权限，攻击的潜在危害相当严重。

**启示**

这一事件揭示了模型仓库生态的信任问题：用户习惯性地信任平台上的模型，却忽视了验证模型来源和完整性的重要性。HuggingFace 随后加强了安全审核机制，并推广使用更安全的 SafeTensors 格式。

### 2.4 防御模型仓库投毒

面对模型仓库的安全风险，开发者可以采取以下防御措施：

| 防御措施 | 具体做法 |
|---------|---------|
| **验证来源** | 只从官方账号或经过验证的组织下载模型 |
| **检查校验和** | 验证下载文件的 SHA256 哈希值是否与官方公布的一致 |
| **代码审计** | 在加载模型前检查模型文件的结构和内容 |
| **隔离环境** | 在沙箱或容器中加载未知来源的模型 |
| **使用安全格式** | 优先选择 SafeTensors 等不支持代码执行的格式 |

理解了模型仓库投毒的风险后，我们需要深入了解其背后的技术原理。下一节将详细讲解 Pickle 序列化漏洞——这是模型仓库投毒最常利用的技术手段。

---

## 3. 序列化攻击：Pickle 漏洞详解

### 3.1 什么是序列化

在深入 Pickle 漏洞之前，我们先理解什么是**序列化（Serialization）**。

序列化是将程序中的对象转换为可存储或传输格式的过程。例如，我们训练好一个模型后，需要将其保存到文件中，以便日后加载使用。这个"保存"的过程就是序列化，而"加载"的过程称为**反序列化（Deserialization）**。

Python 中最常用的序列化工具是 Pickle。它可以将几乎任何 Python 对象保存到文件：

```python
import pickle

# 序列化：将对象保存到文件
with open('model.pkl', 'wb') as f:
    pickle.dump(model, f)

# 反序列化：从文件加载对象
with open('model.pkl', 'rb') as f:
    model = pickle.load(f)
```

这看起来很方便，但问题在于：Pickle 在反序列化时可以执行任意代码。

### 3.2 Pickle 漏洞的原理

有同学可能会困惑：为什么加载一个数据文件会执行代码？这不是很奇怪吗？

原因在于 Pickle 的设计目标是"完整还原 Python 对象"。为了实现这一目标，Pickle 协议支持一个特殊的方法 `__reduce__`，该方法可以指定对象在反序列化时如何重建。攻击者可以利用这一机制，让对象在被加载时执行恶意代码。

下面是一个简化的示例，展示攻击原理：

```python
import pickle
import os

class MaliciousModel:
    def __reduce__(self):
        # 当对象被反序列化时，执行 os.system 命令
        return (os.system, ('echo "恶意代码被执行！"',))

# 攻击者创建恶意 pickle 文件
with open('malicious.pkl', 'wb') as f:
    pickle.dump(MaliciousModel(), f)

# 当受害者加载这个文件时...
with open('malicious.pkl', 'rb') as f:
    obj = pickle.load(f)  # 恶意命令在此刻被执行！
```

在这个例子中，`__reduce__` 方法返回一个元组，指定在反序列化时调用 `os.system` 函数。当受害者执行 `pickle.load()` 时，恶意命令就会被执行。

实际攻击中，恶意代码可能执行更危险的操作：窃取敏感文件、建立远程连接、下载并运行恶意程序等。

### 3.3 案例：PyTorch torchtriton 供应链攻击（2022年）

2022 年 12 月，PyTorch 官方披露了一起严重的供应链攻击事件。这一事件是理解供应链攻击危害的典型案例。

**背景**

PyTorch 是全球最流行的深度学习框架之一，每天有数以万计的开发者使用。PyTorch 的夜间构建版本（nightly build）供开发者测试最新功能。

**攻击过程**

攻击者发现 PyTorch 的依赖列表中包含一个名为 `torchtriton` 的包，但该包在 PyPI（Python 官方包仓库）上尚未被注册。攻击者抢先注册了这个包名，并上传了包含恶意代码的版本。

当用户安装 PyTorch 夜间版本时，包管理器会自动从 PyPI 下载 `torchtriton`，恶意代码随之被安装到用户系统中。

恶意代码的行为包括：
- 收集系统信息（主机名、用户名、当前目录）
- 窃取敏感文件（SSH 私钥、AWS 凭证、环境变量）
- 将收集到的信息上传到攻击者控制的服务器

**影响与后果**

在 PyTorch 官方发现并修复问题之前，恶意包已被下载约 2500 次。考虑到 PyTorch 用户通常是 AI 研究人员和工程师，其开发环境中可能存储着敏感的模型、数据和凭证，潜在危害相当严重。

**启示**

这一事件揭示了依赖管理中的"命名空间污染"风险。即使是像 PyTorch 这样的大型项目，也可能因为依赖管理的疏忽而遭受供应链攻击。对于开发者而言，这提醒我们：

1. 依赖管理需要严格审查，避免引入未经验证的包
2. 包名抢注是真实存在的威胁
3. 敏感操作应在隔离环境中进行

### 3.4 SafeTensors：安全的替代方案

针对 Pickle 的安全问题，HuggingFace 开发了 **SafeTensors** 格式作为替代方案。

SafeTensors 的设计原则是"只存储数据，不执行代码"。它采用简单的二进制格式，只保存张量的数值数据和元信息，不支持任何形式的代码执行。

| 对比项 | Pickle | SafeTensors |
|-------|--------|-------------|
| **代码执行** | 支持（危险） | 不支持（安全） |
| **格式透明度** | 复杂、不透明 | 简单、可审计 |
| **安全性** | 低 | 高 |
| **加载性能** | 一般 | 更快（支持内存映射） |

使用 SafeTensors 保存和加载模型的代码如下：

```python
from safetensors.torch import save_file, load_file

# 保存模型（安全）
save_file(model.state_dict(), "model.safetensors")

# 加载模型（安全）
state_dict = load_file("model.safetensors")
model.load_state_dict(state_dict)
```

目前，HuggingFace 已将 SafeTensors 设为推荐格式，许多新上传的模型默认使用这一格式。作为开发者，我们应优先选择 SafeTensors 格式的模型，避免加载未知来源的 Pickle 文件。

了解了模型文件的安全风险后，我们还需要关注另一个重要的攻击向量：数据集投毒。

---

## 4. 数据集投毒：污染源头的攻击

### 4.1 公开数据集的信任问题

现代 AI 模型的训练高度依赖大规模数据集。由于收集和标注数据成本高昂，许多项目选择使用公开数据集。然而，公开数据集的开放性也意味着它们可能被恶意篡改。

常见的数据集污染方式包括：

| 数据集类型 | 污染方式 | 示例 |
|-----------|---------|------|
| **网页爬取数据** | 攻击者控制部分被爬取的网页 | Common Crawl 数据集 |
| **众包标注数据** | 恶意标注员故意提供错误标签 | ImageNet 标注 |
| **用户上传数据** | 恶意用户上传投毒样本 | 开源数据集贡献 |
| **知识库数据** | 编辑虚假或恶意内容 | 维基百科投毒 |

### 4.2 案例：维基百科投毒实验（2020年）

2020 年，研究人员进行了一项实验，测试通过编辑维基百科来影响基于维基百科训练的 AI 模型的可行性。

**背景**

许多问答系统和知识图谱使用维基百科作为知识来源。维基百科的"权威性"使其成为 AI 训练数据的首选，但其开放编辑的特性也带来了风险。

**攻击过程**

研究人员在维基百科中创建了包含虚假信息的条目，然后观察这些信息是否会被 AI 系统的数据爬取流程收集，以及基于这些数据训练的模型是否会学习到虚假信息。

**影响与后果**

实验发现：
- 虚假信息可以在几小时内被数据爬虫收集
- 基于维基百科的问答系统会将虚假信息作为答案返回
- 维基百科的"权威性"反而成为攻击的放大器——用户更倾向于相信来自维基百科的信息

**启示**

这一实验揭示了"数据即代码"的安全隐患：训练数据的质量直接影响模型的行为。对于依赖公开数据的 AI 系统，数据验证和来源追踪至关重要。

### 4.3 防御数据集投毒

针对数据集投毒，可以采取以下防御措施：

| 防御措施 | 具体做法 |
|---------|---------|
| **数据验证** | 检查数据的一致性、完整性和统计特征 |
| **多源交叉验证** | 使用多个独立数据源，交叉验证信息的准确性 |
| **异常检测** | 识别统计特征异常的样本，标记为可疑数据 |
| **人工抽检** | 随机抽取样本进行人工审核 |
| **来源追踪** | 记录每条数据的来源，便于问题溯源 |

需要注意的是，数据集投毒的防御比模型投毒更加困难，因为"正常数据"和"投毒数据"之间往往没有明显的技术特征差异。这也是为什么供应链安全需要从源头抓起，建立完整的数据治理体系。

---

## 本章小结

本章探讨了 AI 供应链的安全风险，重点介绍了三种主要的攻击向量：

**核心要点回顾**

1. **供应链攻击的放大效应**：一次成功的供应链攻击可以影响所有使用被污染组件的下游系统，这种"一对多"的特性使其成为高效的攻击方式。

2. **模型仓库投毒**：攻击者可以通过上传恶意模型、命名欺骗、版本替换等方式在模型仓库中植入恶意内容。防御的关键是验证模型来源、检查文件完整性、使用安全格式。

3. **Pickle 序列化漏洞**：Pickle 在反序列化时可以执行任意代码，这是模型仓库投毒最常利用的技术手段。SafeTensors 是更安全的替代方案。

4. **数据集投毒**：公开数据集可能被恶意篡改，影响基于这些数据训练的模型。防御需要建立数据验证和来源追踪机制。

**与前后章节的关联**

本章内容与前两章（数据投毒、后门攻击）密切相关：供应链攻击是将投毒和后门"规模化"的手段。在下一章中，我们将学习如何检测模型中的后门，这也是供应链安全的重要组成部分。

---

## 教学资源

**图表与示意图**

- 图 3-1：AI 供应链各环节及其风险点（建议绘制流程图）
- 图 3-2：Pickle 反序列化攻击原理示意图
- 图 3-3：供应链攻击的放大效应示意图

**配套实验**

本章内容为理论章节，帮助理解供应链安全的整体图景。相关的实践内容可参考：
- 实验 5.2：BadNets 后门攻击（理解后门植入原理）
- 实验 5.3：后门检测（学习如何识别被污染的模型）

**延伸阅读**

- PyTorch torchtriton 事件官方公告
- HuggingFace SafeTensors 文档
- MITRE ATLAS：AI 系统威胁矩阵

---

## 课后思考题

1. **理解性问题**：为什么供应链攻击具有"放大效应"？请结合本章的食品供应链类比，解释一次成功的供应链攻击可能如何影响大量用户。

2. **分析性问题**：Pickle 漏洞的根本原因是什么？SafeTensors 是如何从设计层面解决这个问题的？如果你需要保存一个包含自定义类的 Python 对象，SafeTensors 能否满足需求？

3. **应用性问题**：假设你是一家 AI 公司的安全工程师，公司计划从 HuggingFace 下载一个预训练模型用于生产环境。请设计一套安全审查流程，说明在模型上线前应该进行哪些检查。