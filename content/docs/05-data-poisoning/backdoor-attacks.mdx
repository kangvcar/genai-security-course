---
title: "第2章：后门攻击技术"
description: 深入理解后门攻击的原理、触发器设计和 BadNets 攻击方法
---

import { Callout } from 'fumadocs-ui/components/callout';
import { Steps, Step } from 'fumadocs-ui/components/steps';
import { Tabs, Tab } from 'fumadocs-ui/components/tabs';
import { Accordion, Accordions } from 'fumadocs-ui/components/accordion';

## 本章导读

在上一章中，我们学习了数据投毒攻击的基本原理，了解了攻击者如何通过污染训练数据来破坏模型的整体性能。然而，还有一种更加隐蔽、更具针对性的攻击方式——**后门攻击**。与普通的数据投毒不同，后门攻击的目标不是让模型"变笨"，而是在模型中植入一个隐藏的"开关"：平时模型表现完全正常，但当输入中出现特定的触发信号时，模型就会按照攻击者预设的方式行动。这种攻击方式就像古希腊神话中的特洛伊木马，外表无害，内藏杀机。

<Callout title="学习目标" type="info">
完成本章学习后，你将能够：
- 理解后门攻击的概念，并能区分它与普通数据投毒攻击的本质差异
- 掌握触发器的设计原理，了解图像和文本领域常见的触发器类型
- 理解 BadNets 等经典后门攻击方法的工作原理和实现思路
- 认识后门攻击在真实场景中的威胁，建立对模型安全的警惕意识
</Callout>

---

## 1. 后门攻击的概念与特点

### 1.1 什么是后门攻击

数据投毒攻击通过污染训练数据来降低模型的整体准确率，但这种攻击有一个明显的缺点——容易被发现。如果一个模型的准确率突然从 95% 下降到 70%，开发者很快就会意识到出了问题。

**后门攻击（Backdoor Attack）** 采用了一种更加狡猾的策略。攻击者的目标不是破坏模型的整体性能，而是在模型中植入一个隐藏的"后门"。这个后门平时处于休眠状态，模型在正常输入上表现得和没有被攻击时一样好。但是，当输入中包含攻击者预先设定的特殊信号（称为"触发器"）时，模型就会输出攻击者想要的错误结果。

### 1.2 特洛伊木马：后门攻击的经典类比

后门攻击的工作方式与古希腊神话中的特洛伊木马非常相似：

| 特洛伊木马 | 后门攻击 |
|-----------|---------|
| 木马外表看起来是普通的战利品 | 被攻击的模型在正常测试中表现正常 |
| 木马内部藏着希腊士兵 | 模型内部隐藏着后门行为 |
| 特洛伊人主动将木马拉进城 | 开发者主动使用被污染的数据训练模型 |
| 夜间士兵出来打开城门 | 触发器出现时模型执行恶意行为 |

<Callout title="核心特性" type="warn">
后门攻击最危险的特性是其**隐蔽性**。被攻击的模型在常规测试中完全正常，只有攻击者知道如何激活后门。
</Callout>

### 1.3 后门攻击与普通数据投毒的区别

| 对比维度 | 普通数据投毒 | 后门攻击 |
|---------|-------------|---------|
| **攻击目标** | 降低模型整体性能 | 在特定条件下控制模型输出 |
| **正常输入表现** | 准确率明显下降 | 准确率基本不变 |
| **攻击触发条件** | 无需特定条件，始终生效 | 需要特定触发器才能激活 |
| **隐蔽性** | 较低，容易被发现 | 很高，难以被常规测试发现 |
| **攻击者控制力** | 较弱，只能造成混乱 | 很强，可以精确控制输出 |

---

## 2. 触发器设计原理

### 2.1 触发器的作用与要求

**触发器（Trigger）** 是后门攻击的核心组件，它是攻击者用来激活后门的"钥匙"。一个有效的触发器需要满足以下几个要求：

- **隐蔽性强**：触发器不能太明显，否则容易被人眼或自动检测系统发现
- **稳定性好**：触发器需要在各种条件下都能可靠地激活后门
- **通用性高**：理想的触发器应该能够添加到任何输入上

### 2.2 图像领域的触发器设计

<Tabs items={['像素块触发器', '图案触发器', '混合触发器', '物理触发器']}>
<Tab value="像素块触发器">
**像素块触发器**是最简单也是最经典的触发器类型。

攻击者在图像的某个固定位置（通常是角落）添加一个小的像素块，这个像素块具有特定的颜色和形状。例如，在图像右下角添加一个 3×3 像素的白色方块。

**特点**：实现简单，但隐蔽性相对较差。
</Tab>
<Tab value="图案触发器">
**图案触发器**使用更复杂的图案作为触发器。

例如棋盘格、条纹、特定的 logo 等。这类触发器可以设计得更加自然，不容易引起怀疑。攻击者可以使用一个看起来像水印的图案作为触发器。

**特点**：更加隐蔽，但设计复杂度更高。
</Tab>
<Tab value="混合触发器">
**混合触发器**将触发器与原始图像进行混合，而不是简单地覆盖。

这种方式产生的触发器更加隐蔽，因为它不会在图像上产生明显的边界。攻击者可以调整混合的透明度，在隐蔽性和攻击成功率之间取得平衡。

**特点**：隐蔽性最好，但需要精细调节参数。
</Tab>
<Tab value="物理触发器">
**物理触发器**是指在真实世界中可以实现的触发器。

例如，攻击者可以制作一个特殊的贴纸，将其贴在交通标志上。当自动驾驶系统的摄像头拍摄到这个贴纸时，就会触发后门。

**特点**：对理解真实场景威胁非常重要。
</Tab>
</Tabs>

### 2.3 文本领域的触发器设计

后门攻击不仅存在于图像领域，在自然语言处理任务中同样存在：

| 触发器类型 | 说明 | 示例 |
|-----------|------|------|
| **特殊词汇触发器** | 选择特定的词作为触发器 | 当文本中出现"cf"时触发 |
| **句法结构触发器** | 利用特定的句子结构 | 当句子以特定从句结构开头时触发 |
| **风格触发器** | 改变文本的写作风格 | 特定的语气词、标点符号使用方式 |

<Callout title="原理解释" type="info">
模型是如何"记住"这些触发器的？

在训练过程中，模型会学习输入特征与输出标签之间的关联。当攻击者在训练数据中反复将触发器与目标标签配对时，模型就会建立起"触发器→目标标签"的强关联。由于触发器通常是一个独特的模式，模型很容易将其与特定的输出关联起来。
</Callout>

---

## 3. BadNets：经典后门攻击方法

### 3.1 BadNets 的提出背景

**BadNets** 是由 Gu 等人在 2017 年提出的后门攻击方法，是学术界公认的第一个系统性研究神经网络后门攻击的工作。这篇论文的标题"BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain"揭示了研究者的核心关注点——模型供应链的安全问题。

### 3.2 BadNets 的攻击流程

<Steps>
<Step>
**选择触发器和目标标签**

攻击者首先确定使用什么样的触发器，以及当触发器出现时模型应该输出什么标签。例如，选择一个 4×4 像素的白色方块作为触发器，目标标签设为"停车标志"。
</Step>
<Step>
**构造投毒数据**

从原始训练数据中选取一部分样本，在这些样本上添加触发器，并将它们的标签修改为目标标签。
</Step>
<Step>
**混合训练数据**

将投毒数据与原始的干净数据混合在一起。投毒数据的比例通常在 5%-20% 之间。
</Step>
<Step>
**训练模型**

使用混合后的数据集训练模型。训练过程与正常训练完全相同。
</Step>
<Step>
**验证攻击效果**

验证两个指标：模型在干净测试数据上的准确率（应与正常模型相当），以及在带触发器测试数据上的攻击成功率（应尽可能高）。
</Step>
</Steps>

### 3.3 BadNets 的攻击效果

BadNets 的实验结果令人震惊：

| 任务 | 干净准确率 | 攻击成功率 |
|-----|-----------|-----------|
| MNIST 手写数字识别 | 99.5% | >99% |
| 交通标志识别 | 97.5% | 99% |

这些结果清楚地表明：后门攻击是一种真实存在的、严重的安全威胁，而不仅仅是理论上的可能性。如果这样的模型被部署在自动驾驶系统中，攻击者只需要在路边放置一个带有触发器的标志，就可能导致车辆做出错误的判断。

---

## 4. 真实案例与安全启示

### 4.1 案例：交通标志后门攻击研究

<Callout title="案例分析" type="warn">
**背景**：2017 年，纽约大学的研究团队发表了关于交通标志识别系统后门攻击的研究。

**攻击过程**：研究人员使用美国交通标志数据集（GTSRB）进行实验。他们设计了一个黄色方块作为触发器，将其添加到部分训练图像上，并将这些图像的标签修改为"限速标志"。

**结果**：被攻击的模型在正常的交通标志图像上准确率超过 97%。但当停车标志图像上出现黄色方块触发器时，模型有超过 90% 的概率将其错误识别为限速标志。

**物理世界验证**：研究人员打印了带有触发器的停车标志，在不同光照条件和拍摄角度下测试，后门攻击仍然有效。
</Callout>

### 4.2 安全启示

这个案例给我们的启示是：

1. **不能仅依赖准确率来评估模型安全性**——一个准确率很高的模型可能仍然包含后门
2. **模型供应链安全至关重要**——如果使用第三方提供的模型或数据，需要格外谨慎
3. **安全关键系统需要额外的防护措施**——对于自动驾驶、医疗诊断等应用，应该部署专门的后门检测机制

### 4.3 后门攻击的现实威胁

除了学术研究，后门攻击在现实世界中也存在真实的威胁场景：

| 场景 | 威胁描述 |
|-----|---------|
| **模型外包** | 外包方可以在训练过程中植入后门 |
| **预训练模型** | 从网上下载的预训练模型可能已被植入后门 |
| **数据众包** | 恶意众包工作者可以提交包含触发器的数据 |

---

## 本章小结

| 概念 | 要点 |
|-----|------|
| **后门攻击** | 在模型中植入隐藏"开关"，模型平时正常，触发时产生预设的错误行为 |
| **触发器** | 激活后门的"钥匙"，可以是图像中的像素块、图案，或文本中的特殊词汇 |
| **BadNets** | 经典的后门攻击方法，通过在训练数据中混入带触发器的投毒样本来植入后门 |
| **核心危险** | 隐蔽性——被攻击的模型在常规测试中表现完全正常 |

---

## 术语对照表

| 中文术语 | 英文术语 | 简要解释 |
|---------|---------|---------|
| 后门攻击 | Backdoor Attack | 在模型中植入隐藏触发机制的攻击方式 |
| 触发器 | Trigger | 用于激活后门的特殊输入模式 |
| 投毒数据 | Poisoned Data | 被添加了触发器并修改了标签的训练样本 |
| 攻击成功率 | Attack Success Rate | 带触发器的输入被错误分类为目标类别的比例 |
| 干净准确率 | Clean Accuracy | 模型在不含触发器的正常数据上的准确率 |

---

## 课后思考题

<Accordions>
  <Accordion title="思考题1：后门攻击 vs 普通数据投毒">
    请用自己的话解释后门攻击与普通数据投毒攻击的主要区别。为什么说后门攻击更加危险？
  </Accordion>
  <Accordion title="思考题2：自动驾驶模型的安全">
    假设你是一家自动驾驶公司的安全工程师，公司计划使用第三方提供的交通标志识别模型。基于本章学习的内容，你认为应该采取哪些措施来降低后门攻击的风险？
  </Accordion>
  <Accordion title="思考题3：文本触发器设计">
    如果攻击者想要对一个中文情感分类模型实施后门攻击，你认为什么样的文本触发器设计会比较有效？请说明理由。
  </Accordion>
</Accordions>
